---
title: "ECCV 2020: Some Highlights"
excerpt: "The European Conference on Computer Vision (ECCV) 2020 ended last weed. This year's online conference contained 1360 papers, with 104 as orals, 160 as spotlights and the rest as posters. In addition to 45 workshops and 16 tutorials. In this blog post, I'll summarize some paper I've read and list that caught my attention."
date: 2020-08-30 00:00:00
published: false
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2020 European Conference on Computer Vision took place online, from 23 to 28 August, and consisted of
1360 papers, divided into 104 orals, 160 spotlights and the rest 1096 papers as posters.
In addition to 45 workshops and 16 tutorials. As it is the case in recent years with ML and CV conferences, the huge number of papers can be overwhelming at times. Similar to my [CVPR2020 post](https://yassouali.github.io/ml-blog/cvpr2020/), to get a grasp of the general trends of the conference this year, I will present in this blog post a sort of a snapshot of the conference by summarizing some papers (& listing some) that grabbed my attention.

- All of the papers can be found here: [ECCV Conference Papers](https://www.ecva.net/papers.php)
- A list of availble presentation on YT: [Crossminds ECCV](https://crossminds.ai/category/eccv%202020/). In addition to this [YT playlist](https://www.youtube.com/playlist?list=PL6liSIqFR4BXnfg7-HM5-f7LGEKL1EDYb).
- One sentence description of all ECCV-2020 papers: [ECCV Paper Digest](https://www.paperdigest.org/2020/08/eccv-2020-highlights/)
- ECCV virtual website: [ECCV papers and presentations](https://papers.eccv2020.eu/paper/160/)

*Disclaimer: This post is not a representation of the papers and subjects presented in ECCV 2020; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

# Some Statistics
The statistics presented in this section are taken from the official Opening & Awards presentation. Let's start
by some general statistics:

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/growth.png' | absolute_url }}">
</figure>

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/acceptance.png' | absolute_url }}">
</figure>

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/ECCV20/growth_review.png' | absolute_url }}">
</figure>

The trends of earlier years continued with more than 200% increase in submitted papers compared to the 2018 conference, and with a similar number of papers to CVPR 2020. As expected, this increase is joined by a corresponding increase in the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/areas.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to deep learning, recognition, detection, and understanding. Similar to CVPR 2020, we see an increasing interest in growing areas such as label-efficient methods (e.g., unsupervised learning) and low-level vision.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/institutions.png' | absolute_url }}">
</figure>

In terms of institutions; Similar to ICML this year, Google takes the lead with 180 authors, followed by The Chinese University of Hong Kong with 140 authors and Peking University with 110 authors.

# Recognition, Detection, Segmentation and Pose Estimation

#### End-to-End Object Detection with Transformers ([paper](https://arxiv.org/abs/2005.12872))

The task of object detection consists of localizing and classifying objects visible given an input image.
The popular framework for object detection consist of pre-defining a set of boxes (ie., a set of geometric priors like [anchors](https://arxiv.org/abs/1708.02002) or [region proposals](https://arxiv.org/abs/1506.01497)), which are then classified, followed by a regression step to find the adjust the predefined box, and then a post-processing step to remove duplicate predictions. However, this approach requires selecting a subset of candidate boxes to classify, and is not typically end-to-end differentiable.
In this paper, the authors propose [DETR](https://github.com/facebookresearch/detr) (**DE**tection **TR**ansformer) and end-to-end 
fully differentiable approach with no geometric priors. Bellow is a comparison of DETR and Faster R-CNN pipelines (image taken
from the authors presentation), highlighting the holistic nature of the approach.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/detr_compare.png' | absolute_url }}">
</figure>

DETR is based on the encoder-decoder transformer architecture. The model consists of three components: the CNN feature extractor,
the encoder, and the decoder. A given image is first passed through the feature extractor, to get image features. Then, positional encodings generated using sinusoids at different frequencies are added to the features to retain the 2D structure of the image. The resulting features are then passed through the transformer encoder to aggregate information across features and seperate the object instances. For decoding, a fixed
set of learned embeddings called object queries, ie. randomly initialized embeddings that are learned during training then fixed during evaluation, and their number defines an upper bound on the number of objects the model can detect. The queries are passed to the decoder with the encoded feature. Finally, the output feature vectors are fed through a (shared) fully connected layer to predict the class and bounding box for each query. To compute the loss and train the model, the outputs are matched with the ground truths with a one-to-one matching using the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/detr.png' | absolute_url }}">
</figure>

#### MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution ([paper](https://arxiv.org/abs/1909.12978))

Traditional neural network can only be used if a specific amount of compute if available, and if the resource constraints are not met, the model becomes unusable. However, this can greatly limit the usage of the models in real applications. For example,
if the model is used for in phone inference, the computational constrains are always changing depending on the load and the 
phone's battery charge. A simple solution is to keep several models of different sizes on the device, and use the 
one with the corresponding constrains each time, but this requires a large amount of memory and cannot be scaled to 
different constraints. Recent methods like [S-Net](https://arxiv.org/abs/1812.08928) and [US-Net]([Universally Slimmable Networks and Improved Training Techniques](https://arxiv.org/abs/1903.05134)) sample sub-networks during training so the model can be used at different width during deployment. But the performance drop dramatically with very low constraints.


<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/MutualNet.png' | absolute_url }}">
</figure>

This paper proposes to leverage both the network scale and the input scale to find a good trade-off between the accuracy and
the computational efficiency. As illustrated above, for a given training iteration, four sub-networks are sampled, a full one
and three sub-networks with varying widths. The full network is trained on the original size of the image with the ground-truth labels using the standard cross-entropy loss, while the rest of the sub-networks are trained with randomly down-scaled version of the input image using KL divergence loss between their outputs and the output of the full network (ie., a distillation loss).
This way, each sub-network will be able to learn multi-scale representations from both the input scale and the network scale. This way, during deployment, and given a specific resource constraint, the optimal combination of network scale and input scale can
be chosen for inference. 

#### Gradient Centralization: A New Optimization Technique for Deep Neural Networks ([paper](https://arxiv.org/abs/2004.01461))

Using second order statistics such as mean and variance during optimization to perform some form standardization of the activations or network's weight, using Batch norm or weight norm for instance, have become an important component of neural network training.
Instead of operating on the weights or the activations with additional normalization modules, Gradient Centralization (GC)
operates directly on gradients by centralizing the gradient vectors to have zero mean, which can smooth and accelerate the training process of neural networks and even improve the model generalization performance.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/GC.png' | absolute_url }}">
</figure>

The GC operator, given the computed gradients, first computes the mean of the gradient vectors as illustrated above, then removes the
mean of them. Formally, for a weight vector $$\mathbf{w}_i$$ whose gradient is $$\nabla_{\mathbf{w}_{i}} \mathcal{L}(i=1,2, \ldots, N)$$, the GC operator $$\Phi_{G C}$$ is defined as:

$$\Phi_{G C}\left(\nabla_{\mathbf{w}_{i}} \mathcal{L}\right)=\nabla_{\mathbf{w}_{i}} \mathcal{L}-\frac{1}{M} \sum_{j=1}^{M} \nabla_{w_{i, j}} \mathcal{L}$$

### Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval ([paper](https://arxiv.org/abs/2007.12163))

In image retrieval, the objective is to retrieve images of the same class as the query image from a large collection of
images. This tasks differs from classification where the classes encountered during test were seen during training, in image retrieval, we might get an image with a novel class and we need to fetch similar image, ie., an open set problem.
The general pipeline of image retrieval consists of extracting embeddings for the query image, and also all of image collection using a CNN feature extractor, compute the cosine similarity score between each pair, then rank the image in the collection.
The feature extractor is then trained to have a good ranking. The ranking performance is measured using [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) (AP), computing the sum of the rank of each positive over its rank on the whole image collection. However, computing the ranking of a given image consists of
consists of thresholding using a [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function), making it non-differentiable, so we cannot train end-to-end.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/smooth_ap.png' | absolute_url }}">
</figure>

To solve this, the authors proposed to replace the Heaviside step function with a smooth temperature controlled sigmoid, making
the ranking differentiable and useable as a loss function for end-to-end training. Compared to the triplet loss, the smooth-Ap loss optimizes a ranking loss, while is only a surrogate loss that indirectly optimizes for good ranking.

#### Hybrid Models for Open Set Recognition ([paper](https://arxiv.org/abs/2003.12506))

Existing image classification methods are often based on a closed-set assumption, ie., the training set covers
all possible classes that may appear in the testing phase. But hits assumption is clearly unrealistic, given that
even with large scale datasets such as ImageNet with 1K classes, it is impossible to cover all possible real-world classes.
This where open-set classification comes, and tries to solves this by assuming that the test set contains 
both known and unknown classes.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/hybrid_model.png' | absolute_url }}">
</figure>

In this paper, the authors use Flow-based model to tackle the problem of open-set classification. Flow-based are able fit a probability distribution to training samples in an unsupervised manner via maximum likelihood estimation. The flow models can then predict the probability density of each example. When the probability density of an input sample is large, it is likely to be part of the training distribution with a known class, and outliers with have a small density value. While previous model stacked a classifier
on top of the flow model, the authors propose to learn a joint embedding for both the flow model and the classifier since the embedding space learned from only flow-based model may not have sufficient discriminative expressiveness. As illustrated above, 
During training, images are mapped into a latent feature space by the encoder, then the encoded features are fed into both the classifier trained with a cross-entropy loss, and the flow model for density estimation. The whole architecture is trained in an end-to-end manner. For testing, the $$\log p(x)$$ of each image is computed and then compared with the lowest $$\log p(x)$$ taken over the training set. If it is greater than the threshold $$\tau$$, it is sent to the classifier to identify its specific known class, otherwise it is rejected as an unknown sample.

#### Conditional Convolutions for Instance Segmentation ([paper](https://arxiv.org/abs/2003.05664))

Instance segmentation remains as one of the challenging tasks in computer vision, requiring a per-pixel mask and the class label for each visible object in a given image. The dominant approach is [Mask R-CNN](https://arxiv.org/abs/1703.06870)  which consists of two steps, first, and the object detector Faster R-CNN predict a bounding box for each instance. For each instance, the regions of interest are cropped from the output feature maps using ROI Align, which are resized to the same resolution, and then a mask head which is a small fully convolutional network is used to predict the segmentation mask. However, the authors point out the following limitation with such an architecture; (1) the ROI Align might fetch irrelevant features belonging to the background or to other instances, in addition to the resizing operation that restricts the resolution of the instance segmentation, (2) the mask head requires a stack of 3x3 convolutions to induce a large enough receptive field to segmentation, considerably increases of the mask head.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/condinst.png' | absolute_url }}">
</figure>

In this paper, the authors propose to adapt FCNs used for semantic segmentation for instance segmentation. For effective instance segmentation, FCNs require two type of information, appearance information to categorize objects and location information to distinguish multiple objects belonging to the same category. The proposed network, called CondInst (conditional convolutions for instance segmentation), is a network based on [CondConv](https://arxiv.org/abs/1904.04971 and [HyperNetworks](https://arxiv.org/abs/1609.09106), where for each instance, a sub-network will generate the mask FCN network's weights conditioned on the center area of each instance, which are then used to predict the mask of the given instance. Specifically, as shown above, the network consists of heads applied at multiple scales of the feature map. Each head predict the class of a given instance at pre-defined positions, and the weight to be used by the mask FCN head. Then the mask prediction is done using the paramters produced by each head.


#### Multitask Learning Strengthens Adversarial Robustness ([paper](https://arxiv.org/abs/2007.07236))

One of the main limitations of deep neural networks is their vulnerability to adversarial attacks, where very small and invisible perturbations are injected into the input, resulting in the wrong outputs and fooling the network even if the appearance of the input remains the same. In recent years, the adversarial robustness of deep nets cas rigorously investigated at different stages of the pipeline, from the input data (eg., using unlabeled data and adversarial training) to the model its self using regularization (eg., (Parseval Networks)[https://arxiv.org/abs/1704.08847]), but the outputs of the model are still not utilized to
improve the robustness of the model. In this paper, the authors investigate the effect of having multiple outputs for multi-task
learning on the robustness of the learned model, since a growing number of machine learning applications call for
models capable of solving multiple tasks at once.


<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/multitask_robustness.png' | absolute_url }}">
</figure>

Using p-norm ball bounded attack, where the adversarial perturbation is found within a p-norm bounded ball with a given radius of a given input example, and the vulnerability is the total loss change. The authors showed an improved robustness when training on a pair of tasks (eg., two tasks are chosen from: segmentation, depth, normals, reshading, input reconstruction, 2d and 3d keypoints...).
The improved robustness is observed on single tasks attacks (ie., the perturbation is computed using one output) and multi tasks attacks (ie., the maximal perturbation of all the perturbations computed using all of outputs). The authors also theoretically show
that such a multi task robustness is only obtained if the tasks are correlated.

#### Dynamic Group Convolution for Accelerating Convolutional Neural Networks ([paper](https://arxiv.org/abs/2007.04242))

Group convolutions which were first introduced in AlexNet to accelerate training, and subsequently adapted for efficient 
CNNs such as [MobileNet](https://arxiv.org/abs/1704.04861) and [Shufflenet](https://arxiv.org/abs/1707.01083)
The standard group convolution consists of equally splitting the input and output channels in a convolution layer into mutually  exclusive sections or groups while performing a normal convolution operation within each individual groups. So for $$G$$
groups, the computation is reduced by $$G$$ times. However, the authors argue that they introduce two key limitations: 
(1) they weaken the representation capability of the normal  convolution  by  introducing  sparse  neuron  connections,
and (2) they have fixed channel division regardless of the properties of each input.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/DGC.png' | absolute_url }}">
</figure>

In order to adaptively select the most related input channels for each group while keeping the full structure of the original networks, the authors propose dynamic group convolution (DGC). 
DCG consists of two heads, in each heads, there is a saliency score generator that assigns an importance score to each
channel and the channels with low importance scores are pruned. Then, the normal convolution is conducted based on the selected subset of input channels generating the output channels in each head. Finally, the output channels from different heads are concatenated and shuffled.

#### Disentangled Non-local Neural Networks [paper](https://arxiv.org/abs/2006.06668]

The [non-local block](https://arxiv.org/abs/1711.07971) models long-range dependency between pixels using the attention mechanism, and has been widely used for numerous visual recognition tasks, such as object detection, semantic segmentation, and video action recognition.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/disentangledNL.png' | absolute_url }}">
</figure>
 
In this paper, the authors try to better understand the non-local block, find its limitations, and propose an improved version. First, they start by reformulating the similarity between from a pixel i (referred to as a key pixel) to pixel j (referred to as a query pixel) as the sum of two term: a pairwise term, which is whitened dot product term representing the pure pairwise relation between the and key pixels, and a unary term, representing where a given key pixel has the same impact on all query pixels. Then to understand the impact of each term, they train using either one, and find that pair-wise term is responsible of the category information, while the unary is responsible of boundary information. However, by analyzing the gradient of the non-local block, when the two are combined in the normal attention operator, their gradients are multiplied, so if one gradients of one of the two term is zero, the non-zero gradients wont have any constitution. To solve this, the authors proposed a disentangled version of the non-local, where each term is optimized separately.

#### Hard negative examples are hard, but useful ([paper](https://arxiv.org/abs/2007.12749))

Deep metric learning optimizes an embedding function that maps semantically similar images to relatively nearby locations and maps semantically dissimilar images to distant locations. A popular way way to learn the mapping is to define a loss function based on triplets of images: an anchor image, a positive image from the same class, and a negative image from a different class. The model is then penalized when the anchor is mapped closer to the negative image than it is to the positive image. However, during optimization, 
most triplet candidates already have the anchor much closer to the positive than the negative making them redundant. On the other hand, optimizing with the hardest negative examples leads to bad local minima in the early phase of the training, this is because 
in this cases, the anchor-negative similarity is larger than the anchor-positive similarity as measured by the cosine similarity, ie. dot product between normalized feature vectors.

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/ECCV20/hard_negatives.png' | absolute_url }}">
</figure>

The authors show some problems with the standard implementation of the triplet loss, (1) if the normalization is not considered during
the gradient computation, a large part of the gradient is lost, and (2) if two images of different classes are close by in the embedding space, the gradient of the loss might pull them closer instead of pushing them away. To solve this, instead of pulling the anchor-positive pair together to be tightly clustered as done in the standard triplet loss, the authors propose to avoid updating the anchor-positive pairs resulting in less tight clusters for a class of instances. This way the network can then focus on directly pushing apart the hard negative examples away from the anchor.

#### Volumetric Transformer Networks ([paper](https://arxiv.org/abs/2007.09433))

One of the keys behind the success CNNs comes from their ability to learn discriminative feature representations of semantic object parts, which are very useful for computer vision tasks. However, CNNs still lacks the ability to handle various spatial variations, such as scale, view point and intra-class variations. Recent methods, such as [spatial transformer networks](https://arxiv.org/abs/1506.02025) (STNs), try to suppress such variations by first wrapping the feature maps of spatially different images to a standard canonical configuration, then train classifiers on such standard features. However, such methods apply the same wrapping to all the feature channels, which does not take into consideration the fact that the individual feature channels can represent different semantic parts, which may require different spatial transformations with respect to the canonical configuration.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/VTN.png' | absolute_url }}">
</figure>

To solve this, the paper introduces Volumetric transformer network (VTN) shown above, a learnable module that predicts per channel and per spatial location wrapping transforms to reconfigure the intermediate CNN features into a spatially agnostic and standard representations. VTN is an encoder-decoder network with modules dedicated to letting the information flow across the feature channels to account for the dependencies between the semantic parts.

#### Faster AutoAugment: Learning Augmentation Strategies Using Backpropagation ([paper](https://arxiv.org/abs/1911.06987))

Data augmentations (DA) have become a important component and indispensable of deep learning methods, and recent works
(eg. [AutoAugment](https://arxiv.org/abs/1805.09501), [Fast AutoAugment](https://arxiv.org/abs/1905.00397) and [RandAugment](https://arxiv.org/abs/1909.13719)) showed that augmentation strategies found by search algorithms outperform standard augmentations.
With a pre-defined of possible transformations, such geometric transformations like rotation or color enhancing transformations like solarization, the objective with such methods is to find the optimal data augmentation parameters, ie., the magnitude of the augmentation, the probability of applying it, and the number of transformations to combine as illustrated in the left figure below.
The optimal strategy is learned with a double optimization loop, so that the validation error of a given CNN trained is such strategy is minimized. However, such an optimization method suffers from a large search space of possible policies requiring sophisticated search strategies, and a single iteration of policy optimization requires the full training of the CNN. To solve this, the authors
propose a density matching of original and augmented images with gradient based optimization.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/faster_aug.png' | absolute_url }}">
</figure>

By viewing DA as a way to fill missing points of original data, the objective then is to minimize the distance between the  distributions of augmented data and the original data using adversarial learning, and in order to learn the optimal
augmentation strategy, the policy needs to be differentiable with respect to the parameters of the transformations.
For the probability of applying a given augmentation, the authors use a stochastic binary variable sampled from
a Bernoulli distribution, and optimized using the [Gumbel trick](https://francisbach.com/the-gumbel-trick/), while
the magnitude is approximate with a straight-through estimator and the combination are learned as a combination
of one-hot vectors.

#### Other Papers
* [Metric learning: cross-entropy vs. pairwise losses](https://arxiv.org/abs/2003.08983)
* [Semantic Flow for Fast and Accurate Scene Parsing](https://arxiv.org/abs/2002.10120)
* [Object-Contextual Representations for Semantic Segmentation](https://arxiv.org/abs/1909.11065)
* [Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification](https://arxiv.org/abs/2001.01536)
* [Feature Normalized Knowledge Distillation for Image Classification](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf)
* [Mixup Networks for Sample Interpolation via Cooperative Barycenter Learning](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550630.pdf)
* [OnlineAugment: Online Data Augmentation with Less Domain Knowledge](https://arxiv.org/abs/2007.09271)
* [Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed Datasets](https://arxiv.org/abs/2007.09654)
* [DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning](https://arxiv.org/abs/2004.13458)
* [Estimating People Flows to Better Count Them in Crowded Scenes](https://arxiv.org/abs/1911.10782)
* [SoundSpaces: Audio-Visual Navigation in 3D Environments](https://arxiv.org/abs/1912.11474)
* [Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation](https://arxiv.org/abs/2003.08866)
* [DADA: Differentiable Automatic Data Augmentation](https://arxiv.org/abs/2003.03780)
* [URIE: Universal Image Enhancement for Visual Recognition in the Wild](https://arxiv.org/abs/2003.08979)
* [BorderDet: Border Feature for Dense Object Detection](https://arxiv.org/abs/2007.11056)
* [TIDE: A General Toolbox for Understanding Errors in Object Detection](https://arxiv.org/abs/2008.08115)
* [AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling](https://arxiv.org/abs/2007.09336)
* [PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments](https://arxiv.org/abs/2007.09584)
* [Learning Object Depth from Camera Motion and Video Object Segmentation](http://arxiv.org/abs/2007.05676)
* [Attentive Normalization](https://arxiv.org/abs/1908.01259)
* [Momentum Batch Normalization for Deep Learning with Small Batch Size ]([https://www4.comp.polyu.edu.hk/~cslzhang/paper/conf/ECCV20/ECCV_MBN.pdf)
* [A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions](https://arxiv.org/abs/2001.06057)

## Semi-Supervised, Unsupervised, Transfer, Representation & Few-Shot Learning

#### Big Transfer (BiT): General Visual Representation Learning ([paper](https://arxiv.org/abs/1912.11370))

In this paper, the authors revisit the simple paradigm of transfer learning, pre-train on a large amount of labeled source data (e.g., [JFT-300M](https://arxiv.org/abs/1707.02968) and [ImageNet-21k](https://github.com/dmlc/mxnet-model-gallery/blob/master/imagenet-21k-inception.md) datasets), then fine-tune the per-trained weights on the target tasks, reducing both the amount of data needed for target tasks and the fine-tuning time. The proposed framework is BiT (Big Transfer), and consists of a number of components that are necessary to build an effective network capable of leveraging large scale dataset and learning general and transferable representations.

On the (upstream) pre-training side, BiT consists of the following:
* For very large dataset, the fact that Batch Norm (BN) uses statistics from the training data during testing results in train/test discrepancy, where the training loss is correctly optimized while the validation loss is very unstable. In addition to the sensitivity of BN to the batch size. To solve this, BiT uses both [Group Norm](https://arxiv.org/abs/1803.08494) and [Weight Norm](https://arxiv.org/abs/1602.07868).
* A small model such as ResNet 50 does not benefit from large scale training data, so the size of the model needs to also be correspondingly scaled up.

For (down-stream) target tasks, BiT proposes the following:
* Uses standard SGD, with any layer freezing, dropout, L2-regularization or any adaptation gradients, in addition to initializing the last prediction layer to all 0's.
* Instead of resizing all input to a fixed size, eg., 224. During training, the image is resized to a square, cropped out a smaller random square, and randomly h-flipped. At test time, the image is resized to a fixed size,
* While [mixup](https://arxiv.org/abs/1710.09412) is not useful for large scale pre-training given the abundance of data, BiT finds that mixup regularization can be very beneficial for mid-sized dataset used for downstream tasks.

#### Learning Visual Representations with Caption Annotations ([paper](https://arxiv.org/abs/2008.01392))

Training deep models on large scale annotated dataset results in not only in a good performance on the task at hand, but also enable the model
to learn useful representation for downstream tasks. But can we obtain such useful features without such an expensive fine grained annotations?.
This paper investigates weakly-supervised pre-training using noisy labels, which are image caption in this case.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/ICMLM.png' | absolute_url }}">
</figure>

With the objective of using a limited set of image-caption pairs to learn visual representation, how can the training objective be formulated with an effective 
interaction between the images and theirs captions? Based on masked image modeling used in [BERT](https://arxiv.org/abs/1810.04805) which randomly masks 15% 
of the input tokens, and the model is then trained to reconstruct the masked input tokens using the encoder part of the [transformer](https://arxiv.org/abs/1706.03762) model.
The paper proposed image-conditioned masked language modeling (ICMLM), where the images are leveraged to reconstruct the masked tokens of their corresponding captions.
To solve this objective, the authors proposes two multi modal architectures, (1) ICMLM tfm, where the image is encoded using a CNN, the masked caption using 
BERT model, the caption and image features are then concatenated and passed through a transformer encoder resulting in multi-modal embedding used to predict the masked token.
And (2), ICMLM att+fc, similarity, the caption and image features are first produced, then passed through pair-wise attention to aggregate the information between the caption and the image, the resulting features are then pooled and passed through a fully connected layer for masked token prediction.

#### Memory-augmented Dense Predictive Coding for Video Representation Learning ([paper](https://arxiv.org/abs/2008.01065))
 
The recent progress in self-supervised representation learning for images resulted in impressive results on down-stream tasks. However,
although multi-model representation learning for videos saw similar gains, self-supervision using video stream only, without any other modalities
such as text or audio is still not as developed. Even if the temporal information of videos provide a free supervisory signal to train a model to predict the future states from the past in self-supervised manner. The task remains hard to solve since the exact future is not deterministic, and at a given time step, there many likely
and plausible hypotheses for future states (eg., when the action is "playing golf", a future frame could have the hands and golf club in many possible positions).

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/MemDPC.png' | absolute_url }}">
</figure>

This paper uses contrastive learning with a memory module to solve the issues with future prediction. To reduce the uncertainty, the model predicts the future
at the feature level, and is trained using a contrastive loss to avoid overstrict constrains. And to deal with multiple hypothesis, the memory modules is used to infer multiple futures simultaneously. Given a set of successive frame, a 2d-3d CNN encoder (ie., $$f$$) produces context features and an GPU (ie., $$g$$) to aggregate all the past information, which are then used to select slots from the shared memory module, and the predicted future state is a convex combination of the memory slots. The predicted future state is then compared with true features vectors of the future states using a contrastive loss. For down-straem tasks, the feature produced by $$g$$ are pooled and then fed to the classifier.

#### SCAN: Learning to Classify Images without Labels ([paper](https://arxiv.org/abs/2005.12320))

To group the unlabeled input images into semantically meaningful clusters, we need to find the solutions using the visual similarities alone. Prior work either, (1) learn
rich features with a self-supervised method, then applies k-means on the features to find the cluster, but this can lead to degeneracy quite easily. (2) end-to-end clustering approaches that either leverage CNNs features for deep clustering, or are based on mutual information maximization. However, the cluster depend on the initialization
and is likely to latch into low-level features.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'images/ECCV20/SCAN.png' | absolute_url }}">
</figure>

To solve the issues found in prior work, the paper proposed SCAN (semantic clustering by adopting nearest neighbors) consits of a two step procedure. In a first step, the feature representations are learned through a pretext task, then, to generate the initial cluster, SCAN mines the nearest neighbors of each image based on feature similarity instead of applying K-means. In a second step, he semantically meaningful nearest neighbors as are used as a prior into a to train the model to classify each image and its mined neighbors together using a loss function that maximizes their dot product after softmax, pushing the network to produce both consistent and discriminative (one-hot) predictions.
 
#### GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering ([paper](https://arxiv.org/abs/2002.11863))

Clustering consists of separating data into clusters according to sample similarity. Traditional methods use hand-crafted features and domain specific distance
function t measure the similarity, but such hand crafted feature are very limited in expressiveness. Subsequent work leveraged deep representations with clustering algorithms, but the performance of deep clustering still suffers when the input data is complex. For an effective clustering, in temrs of the features, they must contain both high-level
discriminative features, and capture object semantics. In terms of the clustering step, trivial solutions such as assigning all samples to a single or few clusters
must be avoided, and the clustering needs to be efficient to be applied to large-sized images.

The paper proposes GATCluster, which directly outputs semantic cluster labels without further post-processing, where the learned features are one-hot encoded vectors to guarantee the avoidance of trivial solutions are avoided. GATCluster is trained unsupervised manner with four self-learning tasks with the constraints of transformation invariance, separability maximization, entropy analysis, and attention mapping.

#### Associative Alignment for Few-shot Image Classification ([paper](https://arxiv.org/abs/1912.05094))

Few-shot learning image classification to obtain a model that can learn to recognize novel image classes when very few training examples are available. One of the popular approaches is Meta-learning that extracts common knowledge from a large amount of labeled data containing the base classes, and used to train a model. The model is then trained to classify images from novel concepts with only a few examples, the objective is to find a good initial weight that converge rapidly when trained on the new concepts. Interestingly, recent works demonstrated that standard transfer learning without meta learning where a feature extractor is first pre-trained on the base classes, then fine-tune the classifier on top of the pre-trained extractor on the new few examples performs on par with more sophisticated meta-learning strategies. However, the freezing of the extractor during fine-tuning that is necessary to avoid overfilling hinders the performances.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'images/ECCV20/AssociativeAlignment.png' | absolute_url }}">
</figure>

The paper proposes a two step approach to solve this, first the feature extractor is used to produce feature for the novel examples. The feature of each example is then
mapped to one of the base classes using a similarity metric in the embeddings space. The second step consist of associative alignment, where the feature extractor is fine-tuned so that the embeddings of the novel images a pushed closer to embeddings of their corresponding bases images. This is done fighter by centroid alignment where the distance between the center of each base class and the novel classes is reduced, or adversarial alignment where a discriminator pushed the feature extractor to align the base and novel examples in the embedding space.

#### Other Papers
* [Domain Adaptation through Task Distillation](https://arxiv.org/abs/2008.11911)
* [Are Labels Necessary for Neural Architecture Search?](https://arxiv.org/abs/2003.12056)
* [The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement](https://arxiv.org/abs/2008.10599)
* [Cross-Domain Cascaded Deep Translation](https://arxiv.org/abs/1906.01526)
* [Self-Challenging Improves Cross-Domain Generalization](https://arxiv.org/abs/2007.02454)
* [Label Propagation with Augmented Anchors for UDA](https://arxiv.org/abs/2007.07695)
* [Regularization with Latent Space Virtual Adversarial Training](https://arxiv.org/abs/2007.07695)
* [Transporting Labels via Hierarchical Optimal Transport for Semi-Supervised Learning](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490494.pdf)
* [Negative Margin Matters: Understanding Margin in Few-shot Classification](https://arxiv.org/abs/2003.12060)
* [Rethinking Few-Shot Image Classification: a Good Embedding Is All You Need?](https://arxiv.org/abs/2007.09584)
* [Prototype Rectification for Few-Shot Learning](https://arxiv.org/abs/1911.10713)

## 3D Computer Vision & Robotics

#### NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis ([paper](https://arxiv.org/abs/2003.08934))

3D view synthesis from 2D images is a challenging problem, especially if the the input 2D images are sparsely sampled.
The goal is to train a model that takes a set of 2D images of a 3D scene (with the optional camera pose and its intrinsics),
then, using the trained model, we can general novel views of the 3D scene that we are found in the input 2D images.
A successful approach is voxed-based representations to represent the 3D scene on a discredited grid where we predict a 3D [voxel](https://en.wikipedia.org/wiki/Voxel) of RGB-alpha grid using a 3D CNN. However, such methods are memory inefficient since they scale cubically with the space resolution, can be hard to optimize and are not able to parametrize scene surfaces smoothly.
A recent trend in the computer vision community is to represent a given 3D scene as a continuous function using a fully-connected neural network. So the neural network itself is a compressed representation of the 3D scene, trained using the set of 2D images and then and can be used to render novel views, but existing methods were not able to match existing voxed-based methods.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/NERF.png' | absolute_url }}">
</figure>

NeRF (neural radiance fields) represents a scene as a continuous 5D function using a fully-connected network of 9 layers nad 256 channels, whose input is a single continuous 5D coordinate, ie., 3D spatial locations ($$x$$, $$y$$, $$z$$) and the viewing directions ($$\theta$$, $$\phi$$)), and whose output is RGB color and opacity (output density).
To synthesize a given view, the rendering procedure consists of querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. NeRF is able to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance with a simple reconstruction loss between the rendered images and the ground truths, and demonstrate results that outperform prior work on neural rendering and view synthesis.

#### Convolutional Occupancy Networks ([paper](https://arxiv.org/abs/2003.04618))




#### Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation ([paper](https://cse.buffalo.edu/~jsyuan/papers/2020/4836.pdf))
#### Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild ([paper](https://arxiv.org/abs/2007.15649))
#### Teaching Cameras to Feel: Estimating Tactile Physical Properties of Surfaces From Images ([paper](https://arxiv.org/abs/2004.14487))

#### Other Papers
* [Tracking Emerges by Looking Around Static Scenes, with Neural 3D Mapping](http://arxiv.org/abs/2008.01295)
* [Privacy Preserving Structure-from-Motion](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460324.pdf)
* [Multiview Detection with Feature Perspective Transformation](http://arxiv.org/abs/2007.07247)
* [Motion Capture from Internet Videos](https://arxiv.org/abs/2008.07931)
* [Atlas: End-to-End 3D Scene Reconstruction from Posed Images](https://arxiv.org/abs/2003.10432)
* [Generative Sparse Detection Networks for 3D Single-shot Object Detection](https://arxiv.org/abs/2006.12356)
* [PointTriNet: Learned Triangulation of 3D Point Sets](https://arxiv.org/abs/2005.02138)
* [Points2Surf: Learning Implicit Surfaces from Point Cloud Patches](https://arxiv.org/abs/2007.10453)
* [Geometric Capsule Autoencoders for 3D Point Clouds](https://arxiv.org/abs/1912.03310)
* [Deep Feedback Inverse Problem Solver](https://arxiv.org/abs/1803.00092)
* [Single View Metrology in the Wild](https://arxiv.org/abs/2007.09529)
* [Shape and Viewpoint without Keypoints](https://arxiv.org/abs/2007.10982)
* [Hierarchical Kinematic Human Mesh Recovery](https://arxiv.org/abs/2003.04232)
* [3D Human Shape and Pose from a Single Low-Resolution Image with Self-Supervised Learning](https://arxiv.org/abs/2007.13666)
* [Few-Shot Single-View 3-D Object Reconstruction with Compositional Priors](https://arxiv.org/abs/2004.06302)
* [Towards Streaming Perception](https://arxiv.org/abs/2005.10420)
* [NASA: Neural Articulated Shape Approximation](https://arxiv.org/abs/1912.03207)



## Image and Video Synthesis

#### Transforming and Projecting Images into Class-conditional Generative Networks [paper](https://arxiv.org/abs/2005.01703)
#### Contrastive Learning for Unpaired Image-to-Image Translation [paper](https://arxiv.org/abs/2007.15651)
#### What makes fake images detectable? Understanding properties that generalize [paper](https://arxiv.org/abs/2008.10588)
#### In-Domain GAN Inversion for Real Image Editing [paper](https://arxiv.org/abs/2004.00049)
#### Learning Stereo from Single Images [paper](https://arxiv.org/abs/2008.01484)
#### Rewriting a Deep Generative Model [paper](https://arxiv.org/abs/2007.15646)

#### Other Papers
Free View Synthesis [paper](https://arxiv.org/abs/2008.05511)
Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild [paper](http://arxiv.org/abs/2007.15068)
World-Consistent Video-to-Video Synthesis [paper](https://arxiv.org/abs/2007.08509
RetrieveGAN: Image Synthesis via Differentiable Patch Retrieval [paper](http://arxiv.org/abs/2007.08513]
Generating Videos of Zero-Shot Compositions of Actions and Objects [paper](https://arxiv.org/abs/1912.02401]
Perceiving 3D Human-Object Spatial Arrangements from a Single Image in the Wild [paper](https://arxiv.org/abs/2007.15649]
Across Scales & Across Dimensions: Temporal Super-Resolution using Deep Internal Learning [paper](http://arxiv.org/abs/2003.08872]
Conditional Entropy Coding for Efficient Video Compression [paper](https://arxiv.org/abs/2008.09180]
Semantic View Synthesis [paper](https://arxiv.org/abs/2008.10598]
Learning Camera-Aware Noise Models [paper](https://arxiv.org/abs/2008.09370]


## Vision and Language
#### Connecting Vision and Language with Localized Narratives (paper)[https://arxiv.org/abs/1912.03098]
#### UNITER: UNiversal Image-TExt Representation Learning (paper)[https://arxiv.org/abs/1909.11740]
#### Learning to Learn Words from Visual Scenes (paper)[https://arxiv.org/abs/1911.11237]


Contrastive Learning for Weakly Supervised Phrase Grounding (paper)[https://arxiv.org/abs/2006.09920]
Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments (paper)[https://arxiv.org/abs/2004.02857]
Adaptive Text Recognition through Visual Matching (paper)[http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123610052.pdf]


## Deep Learning: Applications, Methodology, and Theory
#### A Generic Visualization Approach for Convolutional Neural Networks (paper)[https://arxiv.org/abs/2007.09748]
#### Making Sense of CNNs: Interpreting Deep Representations & Their Invariances with INNs (paper)[https://arxiv.org/abs/2008.01777]

Spike-FlowNet: Event-based Optical Flow Estimation with Energy-Efficient Hybrid Neural Networks (paper)[https://arxiv.org/abs/2003.06696]

A Metric Learning Reality Check (paper)[https://arxiv.org/abs/2003.08505]
Learning Predictive Models from Observation and Interaction (paper)[https://arxiv.org/abs/1912.12773]
Beyond Fixed Grid: Learning Geometric Image Representation with a Deformable Grid (paper)[https://arxiv.org/abs/2008.09269]
Event-based Asynchronous Sparse Convolutional Networks (paper)[https://arxiv.org/abs/2003.09148]
Stable Low-rank Tensor Decomposition for Compression of Convolutional Neural Network (paper)[https://arxiv.org/abs/2008.05441]
EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning (paper)[https://arxiv.org/abs/2007.02491]