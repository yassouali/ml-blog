---
title: "ECCV 2020: Some Highlights"
excerpt: "The European Conference on Computer Vision (ECCV) 2020 ended last weed. This year's online conference contained 1360 papers, with 104 as orals, 160 as spotlights and the rest as posters. In addition to 45 workshops and 16 tutorials. In this blog post, I'll summarize some paper I've read and list that caught my attention."
date: 2020-08-30 00:00:00
published: false
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2020 European Conference on Computer Vision took place online, from 23 to 28 August, and consisted of
1360 papers, divided into 104 orals, 160 spotlights and the rest 1096 papers as posters.
In addition to 45 workshops and 16 tutorials. As it is the case in recent years with ML and CV conferences, the huge number of papers can be overwhelming at times. Similar to my [CVPR2020 post](https://yassouali.github.io/ml-blog/cvpr2020/), to get a grasp of the general trends of the conference this year, I will present in this blog post a sort of a snapshot of the conference by summarizing some papers (& listing some) that grabbed my attention.

- All of the papers can be found here: [ECCV Conference Papers](https://www.ecva.net/papers.php)
- A list of availble presentation on YT: [Crossminds ECCV](https://crossminds.ai/category/eccv%202020/). In addition to this [YT playlist](https://www.youtube.com/playlist?list=PL6liSIqFR4BXnfg7-HM5-f7LGEKL1EDYb).
- One sentence description of all ECCV-2020 papers: [ECCV Paper Digest](https://www.paperdigest.org/2020/08/eccv-2020-highlights/)
- ECCV virtual website: [ECCV papers and presentations](https://papers.eccv2020.eu/paper/160/)

*Disclaimer: This post is not a representation of the papers and subjects presented in ECCV 2020; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

# Some Statistics
The statistics presented in this section are taken from the official Opening & Awards presentation. Let's start
by some general statistics:

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/growth.png' | absolute_url }}">
</figure>

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/acceptance.png' | absolute_url }}">
</figure>

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/ECCV20/growth_review.png' | absolute_url }}">
</figure>

The trends of earlier years continued with more than 200% increase in submitted papers compared to the 2018 conference, and with a similar number of papers to CVPR 2020. As expected, this increase is joined by a corresponding increase in the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/areas.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to deep learning, recognition, detection, and understanding. Similar to CVPR 2020, we see an increasing interest in growing areas such as label-efficient methods (e.g., unsupervised learning) and low-level vision.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/institutions.png' | absolute_url }}">
</figure>

In terms of institutions; Similar to ICML this year, Google takes the lead with 180 authors, followed by The Chinese University of Hong Kong with 140 authors and Peking University with 110 authors.

# Recognition, Detection, Segmentation and Pose Estimation

#### End-to-End Object Detection with Transformers ([paper](https://arxiv.org/abs/2005.12872))

The task of object detection consists of localizing and classifying objects visible given an input image.
The popular framework for object detection consist of pre-defining a set of boxes (ie., a set of geometric priors like [anchors](https://arxiv.org/abs/1708.02002) or [region proposals](https://arxiv.org/abs/1506.01497)), which are then classified, followed by a regression step to find the adjust the predefined box, and then a post-processing step to remove duplicate predictions. However, this approach requires selecting a subset of candidate boxes to classify, and is not typically end-to-end differentiable.
In this paper, the authors propose [DETR](https://github.com/facebookresearch/detr) (**DE**tection **TR**ansformer) and end-to-end 
fully differentiable approach with no geometric priors. Bellow is a comparison of DETR and Faster R-CNN pipelines (image taken
from the authors presentation), highlighting the holistic nature of the approach.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/detr_compare.png' | absolute_url }}">
</figure>

DETR is based on the encoder-decoder transformer architecture. The model consists of three components: the CNN feature extractor,
the encoder, and the decoder. A given image is first passed through the feature extractor, to get image features. Then, positional encodings generated using sinusoids at different frequencies are added to the features to retain the 2D structure of the image. The resulting features are then passed through the transformer encoder to aggregate information across features and seperate the object instances. For decoding, a fixed
set of learned embeddings called object queries, ie. randomly initialized embeddings that are learned during training then fixed during evaluation, and their number defines an upper bound on the number of objects the model can detect. The queries are passed to the decoder with the encoded feature. Finally, the output feature vectors are fed through a (shared) fully connected layer to predict the class and bounding box for each query. To compute the loss and train the model, the outputs are matched with the ground truths with a one-to-one matching using the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/detr.png' | absolute_url }}">
</figure>

#### MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution ([paper](https://arxiv.org/abs/1909.12978))

Traditional neural network can only be used if a specific amount of compute if available, and if the resource constraints are not met, the model becomes unusable. However, this can greatly limit the usage of the models in real applications. For example,
if the model is used for in phone inference, the computational constrains are always changing depending on the load and the 
phone's battery charge. A simple solution is to keep several models of different sizes on the device, and use the 
one with the corresponding constrains each time, but this requires a large amount of memory and cannot be scaled to 
different constraints. Recent methods like [S-Net](https://arxiv.org/abs/1812.08928) and [US-Net]([Universally Slimmable Networks and Improved Training Techniques](https://arxiv.org/abs/1903.05134)) sample sub-networks during training so the model can be used at different width during deployment. But the performance drop dramatically with very low constraints.


<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/MutualNet.png' | absolute_url }}">
</figure>

This paper proposes to leverage both the network scale and the input scale to find a good trade-off between the accuracy and
the computational efficiency. As illustrated above, for a given training iteration, four sub-networks are sampled, a full one
and three sub-networks with varying widths. The full network is trained on the original size of the image with the ground-truth labels using the standard cross-entropy loss, while the rest of the sub-networks are trained with randomly down-scaled version of the input image using KL divergence loss between their outputs and the output of the full network (ie., a distillation loss).
This way, each sub-network will be able to learn multi-scale representations from both the input scale and the network scale. This way, during deployment, and given a specific resource constraint, the optimal combination of network scale and input scale can
be chosen for inference. 

#### Gradient Centralization: A New Optimization Technique for Deep Neural Networks ([paper](https://arxiv.org/abs/2004.01461))

Using second order statistics such as mean and variance during optimization to perform some form standardization of the activations or network's weight, using Batch norm or weight norm for instance, have become an important component of neural network training.
Instead of operating on the weights or the activations with additional normalization modules, Gradient Centralization (GC)
operates directly on gradients by centralizing the gradient vectors to have zero mean, which can smooth and accelerate the training process of neural networks and even improve the model generalization performance.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/GC.png' | absolute_url }}">
</figure>

The GC operator, given the computed gradients, first computes the mean of the gradient vectors as illustrated above, then removes the
mean of them. Formally, for a weight vector $$\mathbf{w}_i$$ whose gradient is $$\nabla_{\mathbf{w}_{i}} \mathcal{L}(i=1,2, \ldots, N)$$, the GC operator $$\Phi_{G C}$$ is defined as:

$$\Phi_{G C}\left(\nabla_{\mathbf{w}_{i}} \mathcal{L}\right)=\nabla_{\mathbf{w}_{i}} \mathcal{L}-\frac{1}{M} \sum_{j=1}^{M} \nabla_{w_{i, j}} \mathcal{L}$$

### Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval ([paper](https://arxiv.org/abs/2007.12163))

In image retrieval, the objective is to retrieve images of the same class as the query image from a large collection of
images. This tasks differs from classification where the classes encountered during test were seen during training, in image retrieval, we might get an image with a novel class and we need to fetch similar image, ie., an open set problem.
The general pipeline of image retrieval consists of extracting embeddings for the query image, and also all of image collection using a CNN feature extractor, compute the cosine similarity score between each pair, then rank the image in the collection.
The feature extractor is then trained to have a good ranking. The ranking performance is measured using [Average Precision](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision) (AP), computing the sum of the rank of each positive over its rank on the whole image collection. However, computing the ranking of a given image consists of
consists of thresholding using a [Heaviside step function](https://en.wikipedia.org/wiki/Heaviside_step_function), making it non-differentiable, so we cannot train end-to-end.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/smooth_ap.png' | absolute_url }}">
</figure>

To solve this, the authors proposed to replace the Heaviside step function with a smooth temperature controlled sigmoid, making
the ranking differentiable and useable as a loss function for end-to-end training. Compared to the triplet loss, the smooth-Ap loss optimizes a ranking loss, while is only a surrogate loss that indirectly optimizes for good ranking.

#### Hybrid Models for Open Set Recognition ([paper](https://arxiv.org/abs/2003.12506))

Existing image classification methods are often based on a closed-set assumption, ie., the training set covers
all possible classes that may appear in the testing phase. But hits assumption is clearly unrealistic, given that
even with large scale datasets such as ImageNet with 1K classes, it is impossible to cover all possible real-world classes.
This where open-set classification comes, and tries to solves this by assuming that the test set contains 
both known and unknown classes.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/hybrid_model.png' | absolute_url }}">
</figure>

In this paper, the authors use Flow-based model to tackle the problem of open-set classification. Flow-based are able fit a probability distribution to training samples in an unsupervised manner via maximum likelihood estimation. The flow models can then predict the probability density of each example. When the probability density of an input sample is large, it is likely to be part of the training distribution with a known class, and outliers with have a small density value. While previous model stacked a classifier
on top of the flow model, the authors propose to learn a joint embedding for both the flow model and the classifier since the embedding space learned from only flow-based model may not have sufficient discriminative expressiveness. As illustrated above, 
During training, images are mapped into a latent feature space by the encoder, then the encoded features are fed into both the classifier trained with a cross-entropy loss, and the flow model for density estimation. The whole architecture is trained in an end-to-end manner. For testing, the $$\log p(x)$$ of each image is computed and then compared with the lowest $$\log p(x)$$ taken over the training set. If it is greater than the threshold $$\tau$$, it is sent to the classifier to identify its specific known class, otherwise it is rejected as an unknown sample.

#### Conditional Convolutions for Instance Segmentation ([paper](https://arxiv.org/abs/2003.05664))

Instance segmentation remains as one of the challenging tasks in computer vision, requiring a per-pixel mask and the class label for each visible object in a given image. The dominant approach is [Mask R-CNN](https://arxiv.org/abs/1703.06870)  which consists of two steps, first, and the object detector Faster R-CNN predict a bounding box for each instance. For each instance, the regions of interest are cropped from the output feature maps using ROI Align, which are resized to the same resolution, and then a mask head which is a small fully convolutional network is used to predict the segmentation mask. However, the authors point out the following limitation with such an architecture; (1) the ROI Align might fetch irrelevant features belonging to the background or to other instances, in addition to the resizing operation that restricts the resolution of the instance segmentation, (2) the mask head requires a stack of 3x3 convolutions to induce a large enough receptive field to segmentation, considerably increases of the mask head.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/condinst.png' | absolute_url }}">
</figure>

In this paper, the authors propose to adapt FCNs used for semantic segmentation for instance segmentation. For effective instance segmentation, FCNs require two type of information, appearance information to categorize objects and location information to distinguish multiple objects belonging to the same category. The proposed network, called CondInst (conditional convolutions for instance segmentation), is a network based on [CondConv](https://arxiv.org/abs/1904.04971 and [HyperNetworks](https://arxiv.org/abs/1609.09106), where for each instance, a sub-network will generate the mask FCN network's weights conditioned on the center area of each instance, which are then used to predict the mask of the given instance. Specifically, as shown above, the network consists of heads applied at multiple scales of the feature map. Each head predict the class of a given instance at pre-defined positions, and the weight to be used by the mask FCN head. Then the mask prediction is done using the paramters produced by each head.


#### Multitask Learning Strengthens Adversarial Robustness ([paper](https://arxiv.org/abs/2007.07236))

One of the main limitations of deep neural networks is their vulnerability to adversarial attacks, where very small and invisible perturbations are injected into the input, resulting in the wrong outputs and fooling the network even if the appearance of the input remains the same. In recent years, the adversarial robustness of deep nets cas rigorously investigated at different stages of the pipeline, from the input data (eg., using unlabeled data and adversarial training) to the model its self using regularization (eg., (Parseval Networks)[https://arxiv.org/abs/1704.08847]), but the outputs of the model are still not utilized to
improve the robustness of the model. In this paper, the authors investigate the effect of having multiple outputs for multi-task
learning on the robustness of the learned model, since a growing number of machine learning applications call for
models capable of solving multiple tasks at once.


<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/multitask_robustness.png' | absolute_url }}">
</figure>

Using p-norm ball bounded attack, where the adversarial perturbation is found within a p-norm bounded ball with a given radius of a given input example, and the vulnerability is the total loss change. The authors showed an improved robustness when training on a pair of tasks (eg., two tasks are chosen from: segmentation, depth, normals, reshading, input reconstruction, 2d and 3d keypoints...).
The improved robustness is observed on single tasks attacks (ie., the perturbation is computed using one output) and multi tasks attacks (ie., the maximal perturbation of all the perturbations computed using all of outputs). The authors also theoretically show
that such a multi task robustness is only obtained if the tasks are correlated.

#### Dynamic Group Convolution for Accelerating Convolutional Neural Networks ([paper](https://arxiv.org/abs/2007.04242))

Group convolutions which were first introduced in AlexNet to accelerate training, and subsequently adapted for efficient 
CNNs such as [MobileNet](https://arxiv.org/abs/1704.04861) and [Shufflenet](https://arxiv.org/abs/1707.01083)
The standard group convolution consists of equally splitting the input and output channels in a convolution layer into mutually  exclusive sections or groups while performing a normal convolution operation within each individual groups. So for $$G$$
groups, the computation is reduced by $$G$$ times. However, the authors argue that they introduce two key limitations: 
(1) they weaken the representation capability of the normal  convolution  by  introducing  sparse  neuron  connections,
and (2) they have fixed channel division regardless of the properties of each input.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/DGC.png' | absolute_url }}">
</figure>

In order to adaptively select the most related input channels for each group while keeping the full structure of the original networks, the authors propose dynamic group convolution (DGC). 
DCG consists of two heads, in each heads, there is a saliency score generator that assigns an importance score to each
channel and the channels with low importance scores are pruned. Then, the normal convolution is conducted based on the selected subset of input channels generating the output channels in each head. Finally, the output channels from different heads are concatenated and shuffled.

#### Hard negative examples are hard, but useful ([paper](https://arxiv.org/abs/2007.12749))

Deep metric learning optimizes an embedding function that maps semantically similar images to relatively nearby locations and maps semantically dissimilar images to distant locations. A popular way way to learn the mapping is to define a loss function based on triplets of images: an anchor image, a positive image from the same class, and a negative image from a different class. The model is then penalized when the anchor is mapped closer to the negative image than it is to the positive image. However, during optimization, 
most triplet candidates already have the anchor much closer to the positive than the negative making them redundant. On the other hand, optimizing with the hardest negative examples leads to bad local minima in the early phase of the training, this is because 
in this cases, the anchor-negative similarity is larger than the anchor-positive similarity as measured by the cosine similarity, ie. dot product between normalized feature vectors.

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/ECCV20/hard_negatives.png' | absolute_url }}">
</figure>

The authors show some problems with the standard implementation of the triplet loss, (1) if the normalization is not considered during
the gradient computation, a large part of the gradient is lost, and (2) if two images of different classes are close by in the embedding space, the gradient of the loss might pull them closer instead of pushing them away. To solve this, instead of pulling the anchor-positive pair together to be tightly clustered as done in the standard triplet loss, the authors propose to avoid updating the anchor-positive pairs resulting in less tight clusters for a class of instances. This way the network can then focus on directly pushing apart the hard negative examples away from the anchor.

#### Volumetric Transformer Networks ([paper](https://arxiv.org/abs/2007.09433))

One of the keys behind the success CNNs comes from their ability to learn discriminative feature representations of semantic object parts, which are very useful for computer vision tasks. However, CNNs still lacks the ability to handle various spatial variations, such as scale, view point and intra-class variations. Recent methods, such as [spatial transformer networks](https://arxiv.org/abs/1506.02025) (STNs), try to suppress such variations by first wrapping the feature maps of spatially different images to a standard canonical configuration, then train classifiers on such standard features. However, such methods apply the same wrapping to all the feature channels, which does not take into consideration the fact that the individual feature channels can represent different semantic parts, which may require different spatial transformations with respect to the canonical configuration.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/VTN.png' | absolute_url }}">
</figure>

To solve this, the paper introduces Volumetric transformer network (VTN) shown above, a learnable module that predicts per channel and per spatial location wrapping transforms to reconfigure the intermediate CNN features into a spatially agnostic and standard representations. VTN is an encoder-decoder network with modules dedicated to letting the information flow across the feature channels to account for the dependencies between the semantic parts.

#### Faster AutoAugment: Learning Augmentation Strategies Using Backpropagation ([paper](https://arxiv.org/abs/1911.06987))

Data augmentations (DA) have become a important component and indispensable of deep learning methods, and recent works
(eg. [AutoAugment](https://arxiv.org/abs/1805.09501), [Fast AutoAugment](https://arxiv.org/abs/1905.00397) and [RandAugment](https://arxiv.org/abs/1909.13719)) showed that augmentation strategies found by search algorithms outperform standard augmentations.
With a pre-defined of possible transformations, such geometric transformations like rotation or color enhancing transformations like solarization, the objective with such methods is to find the optimal data augmentation parameters, ie., the magnitude of the augmentation, the probability of applying it, and the number of transformations to combine as illustrated in the left figure below.
The optimal strategy is learned with a double optimization loop, so that the validation error of a given CNN trained is such strategy is minimized. However, such an optimization method suffers from a large search space of possible policies requiring sophisticated search strategies, and a single iteration of policy optimization requires the full training of the CNN. To solve this, the authors
propose a density matching of original and augmented images with gradient based optimization.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/faster_aug.png' | absolute_url }}">
</figure>

By viewing DA as a way to fill missing points of original data, the objective then is to minimize the distance between the  distributions of augmented data and the original data using adversarial learning, and in order to learn the optimal
augmentation strategy, the policy needs to be differentiable with respect to the parameters of the transformations.
For the probability of applying a given augmentation, the authors use a stochastic binary variable sampled from
a Bernoulli distribution, and optimized using the [Gumbel trick](https://francisbach.com/the-gumbel-trick/), while
the magnitude is approximate with a straight-through estimator and the combination are learned as a combination
of one-hot vectors.

#### Disentangled Non-local Neural Networks (paper)[https://arxiv.org/abs/2006.06668]

#### Momentum Batch Normalization for Deep Learning with Small Batch Size (paper)[https://www4.comp.polyu.edu.hk/~cslzhang/paper/conf/ECCV20/ECCV_MBN.pdf]


#### Other Papers
* [Mixup Networks for Sample Interpolation via Cooperative Barycenter Learning](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123550630.pdf)
* [OnlineAugment: Online Data Augmentation with Less Domain Knowledge](https://arxiv.org/abs/2007.09271)
* [Distribution-Balanced Loss for Multi-Label Classification in Long-Tailed Datasets](https://arxiv.org/abs/2007.09654)
* [DiVA: Diverse Visual Feature Aggregation for Deep Metric Learning](https://arxiv.org/abs/2004.13458)
* [Estimating People Flows to Better Count Them in Crowded Scenes](https://arxiv.org/abs/1911.10782)
* [SoundSpaces: Audio-Visual Navigation in 3D Environments](https://arxiv.org/abs/1912.11474)
* [Spatially Adaptive Inference with Stochastic Feature Sampling and Interpolation](https://arxiv.org/abs/2003.08866)
* [DADA: Differentiable Automatic Data Augmentation](https://arxiv.org/abs/2003.03780)
* [URIE: Universal Image Enhancement for Visual Recognition in the Wild](https://arxiv.org/abs/2003.08979)
* [BorderDet: Border Feature for Dense Object Detection](https://arxiv.org/abs/2007.11056)
* [TIDE: A General Toolbox for Understanding Errors in Object Detection](https://arxiv.org/abs/2008.08115)
* [AABO: Adaptive Anchor Box Optimization for Object Detection via Bayesian Sub-sampling](https://arxiv.org/abs/2007.09336)
* [PIoU Loss: Towards Accurate Oriented Object Detection in Complex Environments](https://arxiv.org/abs/2007.09584)
* [Learning Object Depth from Camera Motion and Video Object Segmentation](http://arxiv.org/abs/2007.05676)
* [Semantic Flow for Fast and Accurate Scene Parsing](https://arxiv.org/abs/2002.10120)
* [Object-Contextual Representations for Semantic Segmentation](https://arxiv.org/abs/1909.11065)
* [Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification](https://arxiv.org/abs/2001.01536)
* [Metric learning: cross-entropy vs. pairwise losses](https://arxiv.org/abs/2003.08983)
* [Feature Normalized Knowledge Distillation for Image Classification](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf)
* [Attentive Normalization](https://arxiv.org/abs/1908.01259)
