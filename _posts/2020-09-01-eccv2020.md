---
title: "ECCV 2020: Some Highlights"
excerpt: "The European Conference on Computer Vision (ECCV) 2020 ended last weed. This year's online conference contained 1360 papers, with 104 as orals, 160 as spotlights and the rest as posters. In addition to 45 workshops and 16 tutorials. In this blog post, I'll summarize some paper I've read and list that caught my attention."
date: 2020-08-30 00:00:00
published: false
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2020 European Conference on Computer Vision took place online, from 23 to 28 August, and consisted of
1360 papers, divided into 104 orals, 160 spotlights and the rest 1096 papers as posters.
In addition to 45 workshops and 16 tutorials. As it is the case in recent years with ML and CV conferences, the huge number of papers can be overwhelming at times. Similar to my [CVPR2020 post](https://yassouali.github.io/ml-blog/cvpr2020/), to get a grasp of the general trends of the conference this year, I will present in this blog post a sort of a snapshot of the conference by summarizing some papers (& listing some) that grabbed my attention.

- All of the papers can be found here: [ECCV Conference Papers](https://www.ecva.net/papers.php)
- A list of availble presentation on YT: [Crossminds ECCV](https://crossminds.ai/category/eccv%202020/). In addition to this [YT playlist](https://www.youtube.com/playlist?list=PL6liSIqFR4BXnfg7-HM5-f7LGEKL1EDYb).
- One sentence description of all ECCV-2020 papers: [ECCV Paper Digest](https://www.paperdigest.org/2020/08/eccv-2020-highlights/)
- ECCV virtual website: [ECCV papers and presentations](https://papers.eccv2020.eu/paper/160/)

*Disclaimer: This post is not a representation of the papers and subjects presented in ECCV 2020; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

# Some Statistics
The statistics presented in this section are taken from the official Opening & Awards presentation. Let's start
by some general statistics:

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/growth.png' | absolute_url }}">
</figure>

<figure style="width: 65%" class="align-center">
  <img src="{{ 'images/ECCV20/acceptance.png' | absolute_url }}">
</figure>

<figure style="width: 70%" class="align-center">
  <img src="{{ 'images/ECCV20/growth_review.png' | absolute_url }}">
</figure>

The trends of earlier years continued with more than 200% increase in submitted papers compared to the 2018 conference, and with a similar number of papers to CVPR 2020. As expected, this increase is joined by a corresponding increase in the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/areas.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to deep learning, recognition, detection, and understanding. Similar to CVPR 2020, we see an increasing interest in relatively new areas such as label-efficient methods (e.g., unsupervised learning) and low-level vision.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/institutions.png' | absolute_url }}">
</figure>

In terms of institutions; Similar to ICML this year, Google takes the lead with 180 authors, followed by The Chinese University of Hong Kong with 140 authors and Peking University with 110 authors.





# Recognition, Detection, Segmentation and Pose Estimation

#### End-to-End Object Detection with Transformers ([paper](https://arxiv.org/abs/2005.12872))

The task of object detection consists of localizing and classifying objects visible given an input image.
The popular framework for object detection consist of pre-defining a set of boxes (ie., a set of geometric priors like [anchors](https://arxiv.org/abs/1708.02002) or [region proposals](https://arxiv.org/abs/1506.01497)), which are then classified, followed by a regression step to find the adjust the predefined box, and then a post-processing step to remove duplicate predictions. However, this approach requires selecting a subset of candidate boxes to classify, and is not typically end-to-end differentiable.
In this paper, the authors propose [DETR](https://github.com/facebookresearch/detr) (**DE**tection **TR**ansformer) and end-to-end 
fully differentiable approach with no geometric priors. Bellow is a comparison of DETR and Faster R-CNN pipelines (image taken
from the authors presentation), highlighting the holistic nature of the approach.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'images/ECCV20/detr_compare.png' | absolute_url }}">
</figure>

DETR is based on the encoder-decoder transformer architecture. The model consists of three components: the CNN feature extractor,
the encoder, and the decoder. A given image is first passed through the feature extractor, to get image features. Then, positional encodings generated using sinusoids at different frequencies are added to the features to retain the 2D structure of the image. The resulting features are then passed through the transformer encoder to aggregate information across features and seperate the object instances. For decoding, a fixed
set of learned embeddings called object queries, ie. randomly initialized embeddings that are learned during training then fixed during evaluation, and their number defines an upper bound on the number of objects the model can detect. The queries are passed to the decoder with the encoded feature. Finally, the output feature vectors are fed through a (shared) fully connected layer to predict the class and bounding box for each query. To compute the loss and train the model, the outputs are matched with the ground truths with a one-to-one matching using the [Hungarian algorithm](https://en.wikipedia.org/wiki/Hungarian_algorithm).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/detr.png' | absolute_url }}">
</figure>



#### MutualNet: Adaptive ConvNet via Mutual Learning from Network Width and Resolution ([paper](https://arxiv.org/abs/1909.12978))

Traditional neural network can only be used if a specific amount of compute if available, and if the resource constraints are not met, the model becomes unusable. However, this can greatly limit the usage of the models in real applications. For example,
if the model is used for in phone inference, the computational constrains are always changing depending on the load and the 
phone's battery charge. A simple solution is to keep several models of different sizes on the device, and use the 
one with the corresponding constrains each time, but this requires a large amount of memory and cannot be scaled to 
different constraints. Recent methods like [S-Net](https://arxiv.org/abs/1812.08928) and [US-Net]([Universally Slimmable Networks and Improved Training Techniques](https://arxiv.org/abs/1903.05134)) sample sub-networks during training so the model can be used at different width during deployment. But the performance drop dramatically with very low constraints.


<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/MutualNet.png' | absolute_url }}">
</figure>

This paper proposes to leverage both the network scale and the input scale to find a good trade-off between the accuracy and
the computational efficiency. As illustrated above, for a given training iteration, four sub-networks are sampled, a full one
and three sub-networks with varying widths. The full network is trained on the original size of the image with the ground-truth labels using the standard cross-entropy loss, while the rest of the sub-networks are trained with randomly down-scaled version of the input image using KL divergence loss between their outputs and the output of the full network (ie., a distillation loss).
This way, each sub-network will be able to learn multi-scale representations from both the input scale and the network scale. This way, during deployment, and given a specific resource constraint, the optimal combination of network scale and input scale can
be chosen for inference. 


#### Dynamic Group Convolution for Accelerating Convolutional Neural Networks ([paper](https://arxiv.org/abs/2007.04242))

Group convolutions which were first introduced in AlexNet to accelerate training, and subsequently adapted for efficient 
CNNs such as [MobileNet](https://arxiv.org/abs/1704.04861) and [Shufflenet](https://arxiv.org/abs/1707.01083)
The standard group convolution consists of equally splitting the input and output channels in a convolution layer into mutually  exclusive sections or groups while performing a normal convolution operation within each individual groups. So for $$G$$
groups, the computation is reduced by $$G$$ times. However, the authors argue that they introduce two key limitations: 
(1) they weaken the representation capability of the normal  convolution  by  introducing  sparse  neuron  connections,
and (2) they have fixed channel division regardless of the properties of each input.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/DGC.png' | absolute_url }}">
</figure>

In order to adaptively select the most related input channels for each group while keeping the full structure of the original networks, the authors propose dynamic group convolution (DGC). 
DCG consists of two heads, in each heads, there is a saliency score generator that assigns an importance score to each
channel and the channels with low importance scores are pruned. Then, the normal convolution is conducted based on the selected subset of input channels generating the output channels in each head. Finally, the output channels from different heads are concatenated and shuffled.

#### Faster AutoAugment: Learning Augmentation Strategies Using Backpropagation ([paper](https://arxiv.org/abs/1911.06987))

Data augmentations (DA) have become a important component and indispensable of deep learning methods, and recent works
(eg. [AutoAugment](https://arxiv.org/abs/1805.09501), [Fast AutoAugment](https://arxiv.org/abs/1905.00397) and [RandAugment](https://arxiv.org/abs/1909.13719)) showed that augmentation strategies found by search algorithms outperform standard augmentations.
With a pre-defined of possible transformations, such geometric transformations like rotation or color enhancing transformations like solarization, the objective with such methods is to find the optimal data augmentation parameters, ie., the magnitude of the augmentation, the probability of applying it, and the number of transformations to combine as illustrated in the left figure below.
The optimal strategy is learned with a double optimization loop, so that the validation error of a given CNN trained is such strategy is minimized. However, such an optimization method suffers from a large search space of possible policies requiring sophisticated search strategies, and a single iteration of policy optimization requires the full training of the CNN. To solve this, the authors
propose a density matching of original and augmented images with gradient based optimization.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/ECCV20/faster_aug.png' | absolute_url }}">
</figure>

By viewing DA as a way to fill missing points of original data, the objective then is to minimize the distance between the  distributions of augmented data and the original data using adversarial learning, and in order to learn the optimal
augmentation strategy, the policy needs to be differentiable with respect to the parameters of the transformations.
For the probability of applying a given augmentation, the authors use a stochastic binary variable sampled from
a Bernoulli distribution, and optimized using the [Gumbel trick](https://francisbach.com/the-gumbel-trick/), while
the magnitude is approximate with a straight-through estimator and the combination are learned as a combination
of one-hot vectors.