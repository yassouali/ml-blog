---
title: "CVPR 2021: An Overview"
excerpt: "The 2021 CVPR conference concluded last week, with 1660 papers, 30 tutorials, 83 workshops. In this
blog post, we'll take a quick look into the emerging trends in the computer vision by going through some a small
portion of the accepted papers."
date: 2021-06-29 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2021 CVPR conference, one of the main computer vision and machine learning conference, concluded its second 100% virtual version
last week with a record of papers presented at the main conference. Of about 7500 submissions, 5900 made it to the decision making process
and 1660 papers (vs 1467 papers last year) were accepted with an acceptation rate of 23.7% (vs 22.1% last year).
Such a huge (and growing) number of papers can be a bit overwhelming, so to get a **feel** of the general trends of the conference this year, I will present in this blog post a quick look of the conference by summarizing some papers (& listing some) that seemed interesting to me.

*Note: This post is not a representation of the papers and subjects presented in CVPR; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

First, let's with some useful links:

- Papers: [CVPR2021 open access](https://openaccess.thecvf.com/CVPR2021?day=all)
- Workshops: [CVPR2021 workshops](http://cvpr2021.thecvf.com/workshops-schedule)
- Tutorials: [CVPR2021 tutorials](http://cvpr2021.thecvf.com/program)
- Presentations: [Crossminds](https://crossminds.ai/search/?keyword=CVPR%202021&filter=)
- Papers search interface: [blog.kitware.com](https://blog.kitware.com/demos/cvpr-2021-papers/?filter=authors&search=) & [public.tableau.com](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no)
- Awards: [CVPR2021 paper awards](http://cvpr2021.thecvf.com/node/329)
- Papers digest: [CVPR2021 Paper Digest](https://www.paperdigest.org/2021/06/cvpr-2021-highlights/)
- Papers & code: [CVPR2021 paper & code](https://github.com/amusi/CVPR2021-Papers-with-Code)




# CVPR 2021 in numbers
A portion of the statistics presented in this section are taken from [this](https://github.com/hoya012/CVPR-2021-Paper-Statistics) github repo & this [public tableau gallery](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no).

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/acceptance_rate.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a 20% increase in authors and a 29% increase in submitted papers, joined by rising the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/authors_by_country.png' | absolute_url }}">
</figure>

Similar to the last two years, China is the first contributor to CVPR in terms of accepted papers, followed by the USA, Korea, UK and Germany.


<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/subjects.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, the topic of the year is 3D computer vision with more than 200 papers focusing on this subject alone, followed by deep & representation
learning and image synthesis papers. There is also a notable increase in papers related to explainable AI and medical & biological imaging.

# Action & Behavior Recognition

MoViNets: Mobile Video Networks for Efficient Video Recognition

Task Programming: Learning Data Efficient Behavior Representations

Multimodal Motion Prediction With Stacked Transformers


# Recognition & Detection

Towards Open World Object Detection

You Only Look One-Level Feature

Benchmarking Representation Learning for Natural World Image Collections

Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels

Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks

Line Segment Detection Using Transformers Without Edges


# Motion and Tracking

SiamMOT: Siamese Multi-Object Tracking

# Explainable AI & Privacy

Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings

Transformer Interpretability Beyond Attention Visualization

Black-box Explanation of Object Detectors via Saliency Maps


# Medical and Biological Vision

Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling

# Video Analysis and Understanding

Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps

Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion

Omnimatte: Associating Objects and Their Effects in Video 

# 3D Vision & Robotics
Scan2Cap: Context-aware Dense Captioning in RGB-D Scans 

Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction 

Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts 

Learning Delaunay Surface Elements for Mesh Reconstruction

A Deep Emulator for Secondary Motion of 3D Characters

NeuTex: Neural Texture Mapping for Volumetric Neural Rendering

Learning to recover 3D Scene Shape from a single image

Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos

Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos

NeX: Real-Time View Synthesis With Neural Basis Expansion

Multi-Modal Fusion Transformer for End-to-End Autonomous Driving

Learned Initializations for Optimizing Coordinate-Based Neural Representations

Point2Skeleton: Learning Skeletal Representations from Point Clouds

Back to the Feature: Learning Robust Camera Localization From Pixels To Pose

Wide-Baseline Relative Camera Pose Estimation With Directional Learning

Neural Lumigraph Rendering

Pulsar: Efficient Sphere-Based Neural Rendering

SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data

Holistic 3D Scene Understanding From a Single Image With Implicit Representation

D-NeRF: Neural Radiance Fields for Dynamic Scenes

Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes

Deep Multi-Task Learning for Joint Localization, Perception, and Prediction

IBRNet: Learning Multi-View Image-Based Rendering

KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control

DeRF: Decomposed Radiance Fields

pixelNeRF: Neural Radiance Fields From One or Few Images

Shelf-Supervised Mesh Prediction in the Wild

Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans

AutoInt: Automatic Integration for Fast Neural Volume Rendering

LoFTR: Detector-Free Local Feature Matching With Transformers

Shape and Material Capture at Home

NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video

SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans

MP3: A Unified Model to Map, Perceive, Predict and Plan

Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty

NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis

Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images

MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments From a Single Moving Camera




# Biometrics, Face, Gesture and Body Pose 

SMPLicit: Topology-Aware Generative Model for Clothed People

On Self-Contact and Human Pose

Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics

PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation

OSTeC: One-Shot Texture Completion

SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks

HOTR: End-to-End Human-Object Interaction Detection with Transformers

Birds of a Feather: Capturing Avian Shape Models From Images

# Computational Photography

Event-Based Synthetic Aperture Imaging With a Hybrid Network

GAN Prior Embedded Network for Blind Face Restoration in the Wild

Passive Inter-Photon Imaging

Real-Time High-Resolution Background Matting 

Im2Vec: Synthesizing Vector Graphics without Vector Supervision


# Representation & Adversarial Learning

DECOR-GAN: 3D Shape Detailization by Conditional Refinement

Exploring Simple Siamese Representation Learning 

Audio-Visual Instance Discrimination with Cross-Modal Agreement

Where and What? Examining Interpretable Disentangled Representations

CutPaste: Self-Supervised Learning for Anomaly Detection and Localization

Spatiotemporal Contrastive Video Representation Learning

Taskology: Utilizing Task Relations at Scale

UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers

Self-Supervised Geometric Perception

Adversarially Adaptive Normalization for Single Domain Generalization

MOS: Towards Scaling Out-of-Distribution Detection for Large Semantic Space

Generative Hierarchical Features From Synthesizing Images

Natural Adversarial Examples

Training Generative Adversarial Networks in One Stage

Fast End-to-End Learning on Protein Surfaces



# Image and Video Synthesis

Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction 

Rethinking and Improving the Robustness of Image Style Transfer

Ensembling with Deep Generative Views

SSN: Soft Shadow Network for Image Compositing

Spatially-Adaptive Pixelwise Networks for Fast Image Translation

Learning Continuous Image Representation With Local Implicit Image Function

Motion Representations for Articulated Animation

GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields

Closed-Form Factorization of Latent Semantics in GANs

Stylized Neural Painting

DriveGAN: Towards a Controllable High-Quality Neural Simulation

Image Generators With Conditionally-Independent Pixel Synthesis

Cross-Modal Contrastive Learning for Text-to-Image Generation 

Dual Contradistinctive Generative Autoencoder

Repopulating Street Scenes

NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections

Space-Time Neural Irradiance Fields for Free-Viewpoint Video

SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation

Positional Encoding As Spatial Inductive Bias in GANs

Regularizing Generative Adversarial Networks Under Limited Data

Variational Transformer Networks for Layout Generation 

Deep Animation Video Interpolation in the Wild

Stable View Synthesis

Navigating the GAN Parameter Space for Semantic Image Editing

Skip-Convolutions for Efficient Video Processing

Stochastic Image-to-Video Synthesis Using cINNs

Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing

Playable Video Generation

Plan2Scene: Converting Floorplans to 3D Scenes

SceneGen: Learning to Generate Realistic Traffic Scenes

GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving

OCONet: Image Extrapolation by Object Completion

Anycost GANs for Interactive Image Synthesis and Editing

StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation

Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation

Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes

Animating Pictures With Eulerian Motion Fields

Taming Transformers for High-Resolution Image Synthesis

One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing



# Scene Analysis & Understanding

Exemplar-Based Open-Set Panoptic Segmentation Network

Binary TTC: A Temporal Geofence for Autonomous Navigation

Learning to Recover 3D Scene Shape from a Single Image

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Robust Consistent Video Depth Estimation

Scene Essence 

Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization

MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers 

Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Repurposing GANs for One-Shot Semantic Part Segmentation

Deep Occlusion-Aware Instance Segmentation With Overlapping BiLayers

VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation 

Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers

Information-Theoretic Segmentation by Inpainting Error Maximization

Single Image Depth Prediction With Wavelet Decomposition

Polygonal Building Extraction by Frame Field Learning

Mask Guided Matting via Progressive Refinement Network



# Model Architectures, Optimization & Learning

Pre-Trained Image Processing Transformer

Metadata Normalization

Decoupled Dynamic Filter Networks

On Feature Normalization and Data Augmentation

Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation

Scaling Local Self-Attention for Parameter Efficient Visual Backbones

Bottleneck Transformers for Visual Recognition

MIST: Multiple Instance Spatial Transformer

RepVGG: Making VGG-Style ConvNets Great Again

Involution: Inverting the Inherence of Convolution for Visual Recognition



# Vision & Language

Multimodal Contrastive Training for Visual Representation Learning

ArtEmis: Affective Language for Visual Art

VirTex: Learning Visual Representations From Textual Annotations

Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling

Learning by Planning: Language-Guided Global Image Editing


# Transfer, Low-shot, Semi & Unsupervised Learning
DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort

Learning Graph Embeddings for Compositional Zero-Shot Learning

MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking

Meta Pseudo Labels

Adaptive Prototype Learning and Allocation for Few-Shot Segmentation

Ranking Neural Checkpoints

Student-Teacher Learning From Clean Inputs to Noisy Inputs



# Optimization and Learning










# Datasets

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings

Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets





















































#### Other papers:
- [NAME](LINK)


# The rest
This post turned into a long one very quickly, so in order to avoid ending-up with a 1h long reading session, I will simply list some papers I came across in case the the reader is interested in the subjects.

<details>
  <summary>Click to expand</summary> <br>

<small> <div class="tip" markdown="1">

**Subject**:
- [Name](LINK)

</div> </small>
</details>




