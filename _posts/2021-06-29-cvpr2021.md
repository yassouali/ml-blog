---
title: "CVPR 2021: An Overview"
excerpt: "The 2021 CVPR conference concluded last week, with 1660 papers, 30 tutorials, 83 workshops. In this
blog post, we'll take a quick look into the emerging trends in the computer vision by going through some a small
portion of the accepted papers."
date: 2021-06-29 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2021 CVPR conference, one of the main computer vision and machine learning conference, concluded its second 100% virtual version
last week with a record of papers presented at the main conference. Of about 7500 submissions, 5900 made it to the decision making process
and 1660 papers (vs 1467 papers last year) were accepted with an acceptation rate of 23.7% (vs 22.1% last year).
Such a huge (and growing) number of papers can be a bit overwhelming, so to get a **feel** of the general trends of the conference this year, I will present in this blog post a quick look of the conference by summarizing some papers (& listing some) that seemed interesting to me.

*Note: This post is not a representation of the papers and subjects presented in CVPR; it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

First, let's with some useful links:

- Papers: [CVPR2021 open access](https://openaccess.thecvf.com/CVPR2021?day=all)
- Workshops: [CVPR2021 workshops](http://cvpr2021.thecvf.com/workshops-schedule)
- Tutorials: [CVPR2021 tutorials](http://cvpr2021.thecvf.com/program)
- Presentations: [Crossminds](https://crossminds.ai/search/?keyword=CVPR%202021&filter=)
- Papers search interface: [blog.kitware.com](https://blog.kitware.com/demos/cvpr-2021-papers/?filter=authors&search=) & [public.tableau.com](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no)
- Awards: [CVPR2021 paper awards](http://cvpr2021.thecvf.com/node/329)
- Papers digest: [CVPR2021 Paper Digest](https://www.paperdigest.org/2021/06/cvpr-2021-highlights/)
- Papers & code: [CVPR2021 paper & code](https://github.com/amusi/CVPR2021-Papers-with-Code)




# CVPR 2021 in numbers
A portion of the statistics presented in this section are taken from [this](https://github.com/hoya012/CVPR-2021-Paper-Statistics) github repo & this [public tableau gallery](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no).

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/acceptance_rate.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a 20% increase in authors and a 29% increase in submitted papers, joined by rising the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/authors_by_country.png' | absolute_url }}">
</figure>

Similar to the last two years, China is the first contributor to CVPR in terms of accepted papers, followed by the USA, Korea, UK and Germany.


<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/subjects.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, the topic of the year is 3D computer vision with more than 200 papers focusing on this subject alone, followed by deep & representation
learning and image synthesis papers. There is also a notable increase in papers related to explainable AI and medical & biological imaging.

# Action & Behavior Recognition
MoViNets: Mobile Video Networks for Efficient Video Recognition

Task Programming: Learning Data Efficient Behavior Representations

# Image Classification & Object Detection
Towards Open World Object Detection

You Only Look One-Level Feature

Benchmarking Representation Learning for Natural World Image Collections

Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels

# Motion and Tracking

SiamMOT: Siamese Multi-Object Tracking

# Explainable AI & Privacy
Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings

Transformer Interpretability Beyond Attention Visualization

# Medical and Biological Vision
Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling

# Video Analysis and Understanding
Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps

Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion


# 3D Vision & Robotics
Learned Initializations for Optimizing Coordinate-Based Neural Representations

Point2Skeleton: Learning Skeletal Representations from Point Clouds

Back to the Feature: Learning Robust Camera Localization From Pixels To Pose

Wide-Baseline Relative Camera Pose Estimation With Directional Learning

Neural Lumigraph Rendering

Pulsar: Efficient Sphere-Based Neural Rendering

SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data

Holistic 3D Scene Understanding From a Single Image With Implicit Representation

D-NeRF: Neural Radiance Fields for Dynamic Scenes

Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes

Deep Multi-Task Learning for Joint Localization, Perception, and Prediction

IBRNet: Learning Multi-View Image-Based Rendering

KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control

DeRF: Decomposed Radiance Fields

pixelNeRF: Neural Radiance Fields From One or Few Images

Shelf-Supervised Mesh Prediction in the Wild

Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans

AutoInt: Automatic Integration for Fast Neural Volume Rendering

LoFTR: Detector-Free Local Feature Matching With Transformers

Shape and Material Capture at Home

NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video

SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans




# Biometrics, Face, Gesture and Body Pose 
On Self-Contact and Human Pose

Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics

PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation

OSTeC: One-Shot Texture Completion

SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks

# Segmentation
Polygonal Building Extraction by Frame Field Learning

# Computational Photography
Event-Based Synthetic Aperture Imaging With a Hybrid Network

GAN Prior Embedded Network for Blind Face Restoration in the Wild

Passive Inter-Photon Imaging


# Representation Learning
Audio-Visual Instance Discrimination with Cross-Modal Agreement

Where and What? Examining Interpretable Disentangled Representations

CutPaste: Self-Supervised Learning for Anomaly Detection and Localization

Spatiotemporal Contrastive Video Representation Learning

Taskology: Utilizing Task Relations at Scale

UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers

Self-Supervised Geometric Perception

Adversarially Adaptive Normalization for Single Domain Generalization

MOS: Towards Scaling Out-of-Distribution Detection for Large Semantic Space

Generative Hierarchical Features From Synthesizing Images




# Vision for Graphis
Omnimatte: Associating Objects and Their Effects in Video 

# Image and Video Synthesis
GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields

Closed-Form Factorization of Latent Semantics in GANs

Stylized Neural Painting

DriveGAN: Towards a Controllable High-Quality Neural Simulation

Image Generators With Conditionally-Independent Pixel Synthesis

Cross-Modal Contrastive Learning for Text-to-Image Generation 

Dual Contradistinctive Generative Autoencoder

Repopulating Street Scenes

NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections

Space-Time Neural Irradiance Fields for Free-Viewpoint Video

SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation

Positional Encoding As Spatial Inductive Bias in GANs

Regularizing Generative Adversarial Networks Under Limited Data

Variational Transformer Networks for Layout Generation 

Deep Animation Video Interpolation in the Wild

Stable View Synthesis

Navigating the GAN Parameter Space for Semantic Image Editing

Skip-Convolutions for Efficient Video Processing

Stochastic Image-to-Video Synthesis Using cINNs

Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing

Playable Video Generation

Plan2Scene: Converting Floorplans to 3D Scenes

SceneGen: Learning to Generate Realistic Traffic Scenes


# Scene Analysis & Understanding
Learning to Recover 3D Scene Shape from a Single Image

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Robust Consistent Video Depth Estimation

Scene Essence 

Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization

MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers 

Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Repurposing GANs for One-Shot Semantic Part Segmentation

Deep Occlusion-Aware Instance Segmentation With Overlapping BiLayers




# Model Architectures, Optimization & Learning
Pre-Trained Image Processing Transformer

Metadata Normalization

Decoupled Dynamic Filter Networks

On Feature Normalization and Data Augmentation

Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation

Scaling Local Self-Attention for Parameter Efficient Visual Backbones

Bottleneck Transformers for Visual Recognition

MIST: Multiple Instance Spatial Transformer

# Vision & Language
ArtEmis: Affective Language for Visual Art

# Transfer, Low-shot, Semi & Unsupervised Learning
DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort

Learning Graph Embeddings for Compositional Zero-Shot Learning

MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking

Meta Pseudo Labels

Adaptive Prototype Learning and Allocation for Few-Shot Segmentation


# Optimization and Learning










# Datasets

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings






















































#### Other papers:
- [NAME](LINK)


# The rest
This post turned into a long one very quickly, so in order to avoid ending-up with a 1h long reading session, I will simply list some papers I came across in case the the reader is interested in the subjects.

<details>
  <summary>Click to expand</summary> <br>

<small> <div class="tip" markdown="1">

**Subject**:
- [Name](LINK)

</div> </small>
</details>




