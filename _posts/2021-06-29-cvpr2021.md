---
title: "CVPR 2021: An Overview"
excerpt: "The 2021 CVPR conference concluded last week, with 1660 papers, 30 tutorials, 83 workshops. In this
blog post, we'll take a quick look into the emerging trends in the computer vision by going through some a small
portion of the accepted papers."
date: 2021-07-02 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2021 CVPR conference, one of the main computer vision and machine learning conferences, concluded its second 100% virtual version
last week with a record of papers presented at the main conference. Of about 7500 submissions, 5900 made it to the decision making process
and 1660 papers (vs 1467 papers last year) were accepted with an acceptation rate of 23.7% (vs 22.1% last year).
Such a huge (and growing) number of papers can be a bit overwhelming, so to get a feel of the general trends of the conference this year, I will present in this blog post a quick look of the conference by summarizing some papers (& listing some) that seemed interesting to me.

First, let's start with some useful links:

- Papers: [CVPR2021 open access](https://openaccess.thecvf.com/CVPR2021?day=all)
- Workshops: [CVPR2021 workshops](http://cvpr2021.thecvf.com/workshops-schedule)
- Tutorials: [CVPR2021 tutorials](http://cvpr2021.thecvf.com/program)
- Presentations: [Crossminds](https://crossminds.ai/search/?keyword=CVPR%202021&filter=)
- Papers search interface: [blog.kitware.com](https://blog.kitware.com/demos/cvpr-2021-papers/?filter=authors&search=) & [public.tableau.com](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no)
- Awards: [CVPR2021 paper awards](http://cvpr2021.thecvf.com/node/329)
- Papers digest: [CVPR2021 Paper Digest](https://www.paperdigest.org/2021/06/cvpr-2021-highlights/)
- Papers & code: [CVPR2021 paper & code](https://github.com/amusi/CVPR2021-Papers-with-Code)

*Note: This post is not an objective representation of the papers and subjects presented in CVPR 2021, it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

## Table of Contents

- [CVPR 2021 in numbers](#cvpr-2021-in-numbers)
- [Recognition, Detection & Tracking](#recognition-detection--tracking)
- [Model Architectures & Learning Methods](#model-architectures--optimization)
- [3D Computer Vision](#3d-computer-vision)
- [Image and Video Synthesis](#image-and-video-synthesis)
- [Scene Analysis & Understanding](#scene-analysis--understanding)
- [Representation & Adversarial Learning](#representation--adversarial-learning)
- [Transfer, Low-shot, Semi & Unsupervised Learning](#transfer-low-shot-semi--unsupervised-learning)
- [Computational Photography](#computational-photography)
- [Other Subjects](#other)

# CVPR 2021 in numbers
A portion of the statistics presented in this section are taken from [this](https://github.com/hoya012/CVPR-2021-Paper-Statistics) github repo & this [public tableau gallery](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no).

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/acceptance_rate.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a notable increase in authors and number of submitted papers, joined by a rising the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/authors_by_country.png' | absolute_url }}">
</figure>

Similar to the last two years, China is the first contributor to CVPR in terms of accepted papers, followed by the USA, Korea, UK and Germany.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/subjects.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, the topic of the year is 3D computer vision with more than 200 papers focusing on this subject alone, followed by deep & representation
learning, image synthesis, and computation photography. There is also a notable increase in papers related to explainable AI and medical & biological imaging.

# Recognition, Detection & Tracking

#### Task Programming: Learning Data Efficient Behavior Representations ([paper](https://arxiv.org/abs/2011.13917))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/1.png' | absolute_url }}">
</figure>

In behavioral analysis, the location and pose of agents is first extracted from each frame of a behavior video, and then the labels are generated for a given behaviors of interest on a frame-by-frame basis based on the pose and movements of the agents as depicted in the figure above.
However, to predict the behaviors frame-by-frame in the second step, we need to train behavior detection models which are data intensive and require specialized domain knowledge and high-frequency temporal annotations. This paper studies two alternative ways to better use domain experts instead of a simple increase in the number of annotations: (1) self-supervised learning and (2) creating task programs by domain experts which are engineered decoder tasks for representation learning. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/2.png' | absolute_url }}">
</figure>

Trajectory Embedding for Behavior Analysis (TREBA) uses these two ideas to learn task-relevant low-dimensional representations of pose trajectories, this is done
by training a network to jointly reconstruct the input trajectory and predict the expert-programmed decoder tasks (see figure above). For the first task, given a set
of unlabelled trajectories, where each trajectory is a sequence of states (eg, location or pose of the agents), the history of the agent stages is
encoded using an RNN encoder, and the RNN the decoder (ie, a [trajectory variational autoencoder or TVAE](https://arxiv.org/abs/1806.02813)) then predicts the next states.
As for the second task, we first need to create decoder tasks for trajectory self-supervised learning by domain experts (which is called the process of Task Programing).
First, the experts find the attributes from the trajectory data that are useful to detect the agents behaviors of interest, then write a program
to compute these attributes (eg, distance or angle between two interacting mice) based on the trajectory data using systems like [MARS](https://www.biorxiv.org/content/10.1101/2020.07.26.222299v1) or [SimBA](https://www.biorxiv.org/content/10.1101/2020.04.19.049452v2). These programs are finally used to generate training
data for self-supervised multi-task learning.

#### Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks ([paper](https://arxiv.org/abs/2010.15703))

One of the main challenges of deploying deep nets on mobile and low-power computational  platforms for large scale usage is their large memory and computational requirements.
Luckily, such deep nets are often overparameterized, living some room for compression to reduce their memory and computational demands wit a minimal accuracy hit.
One way to compress deep nets is scalar quantization, which compresses each parameter individually, but the compression rates are still limited by the number of parameters.
Another approach is vector quantization which compresses multiple parameters into a single code, thus exploiting redundancies among groups of network parameters, but
finding the parameters to group can be challenging (eg, in fully connected layers for example, there is no notion of spatial dimensions to group the parameters by).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/3.png' | absolute_url }}">
</figure>

The paper proposes Permute, Quantize, and Fine-tune (PQF), a method that (1) searches for permutations of the network weights that yield functionally equivalent, yet an easier-to-quantize network, (2) quantizes the permuted weights, and finally, (3) fine tunes the permuted-and-quantized network to recover the accuracy of the original uncompressed network. Since the second step consists splitting each weight matrix $$W$$ into subvectors (say $$d$$ elements per subvector) which are then compressed individually, where each subvector is approximated by a smaller code or centroid, the objective of the first step is to find a permutation $$P$$ of the weight matrix such that the constructed subvectors are easier to quantize into codes (see figure above). The optimization of $$P$$ is done by minimizing the determinant of the covariance of the resulting subvectors (see the paper for why they do this). Finally, a fine tuning of the compressed network is conducted to remove the accumulated errors in the activations after the quantization which degrade the performances. 

#### Towards Open World Object Detection ([paper](https://arxiv.org/abs/2103.02603))

In this work, the authors propose  a novel computer vision problem called *Open World Object Detection*, where a model is trained to: 1) identify objects that have not been introduced to it as *unknown* without explicit supervision to do so (ie, open set learning), and 2) incrementally learn these identified unknown categories without forgetting previously learned classes when the corresponding labels are progressively received (ie, incremental and continual learning). In [open set](https://ieeexplore.ieee.org/document/6365193) recognition, the objective is to identify the new instances not seen during training as unknowns, while in [open world](https://arxiv.org/abs/1412.5687) recognition extends this framework by requiring the classifier recognize the newly identified unknown classes.
However, adapting the open set and open world methods from recognition to detection is not trivial since the object detector is explicitly trained to detect the unknown classes as background, making the task of detecting unknown classes harder.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/4.png' | absolute_url }}">
</figure>

To solve this problem, the paper proposes ORE, Open World Object Detector, that learns a clear discrimination between classes in the latent space. This way, (1) the task of detecting an unknown instance as a novelty can be reduced to comparing its representation with the representations of the known instances (ie, open set detection), and also (2), it facilitates learning feature representations for the new class instances without overlapping with the previous classes (ie, incremental and continual learning, thus extending open set to open world detection). To have such a learning behavior, a contrastive clustering objective is introduced in order to force instances of same class to remain close-by, while instances of dissimilar classes would be pushed far apart. This is done by minimizing the distances between the class prototypes and the class representations, where the instances of class unknown are instances with high objectness score, but do not overlap with a ground-truth objects.
The classification head of the trained detector is then transformed into an [energy function](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf) to label an instance as unknown or not.

#### Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling ([paper](https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.html))

For a standard vision task, it is a common practice to adopt the ground-truth labels obtained via either the majority-vote or by simply one annotation from a preferred rater
as the single source of the training data. However, in medical images, the typical practice consists of collecting multiple annotations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. In this case, using standard training procedure of other vision tasks will overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations availble in medical image analysis.
To take this into consideration, the paper proposes MR-Net to explicitly model the multi-rater (dis-)agreement. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/5.png' | absolute_url }}">
</figure>

MRNet contains a coarse to fine two-stage processing pipeline (figure above, right). The first stage consists of a U-Net encoder with a ResNet34 backbine pretrained on ImageNet. Together with an Expertise-aware Inferring Module (EIM), inserted at the bottleneck layer to embed the expertise information of individual raters, named expertness vector, into the extracted high-level semantic features of the network. The outputs are then passed to the U-Net decoder give a coarse prediction.
The second stage then refines the coarse predictions using two modules. The first one is Multi-rater Reconstruction Module (MRM) that reconstructs the raw multi-rater's grading, the reconstruction are then used to estimate the pixel-wise uncertainty map that represents the inter-observer variability across different regions. Finally,
a Multi-rater Perception Module (MPM) with a soft attention mechanism to utilizes the produced uncertainty map to refine the coarse prediction  of the first stage
and predict the final fine segmentation maps (see section 3 of the paper for details about each component).

#### Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized ([paper](https://arxiv.org/abs/2101.05022))

One of the flaws of ImageNet is the presence of a significant level of label noise, where many instances contain multiple classes while having a single-label ground-truth. This mismatch between the single label annotations of ImageNet and the multi-label nature of its images becomes even more problematic with random-crop training, where the input may contain an entirely different class than the ground-truth.
To aviate this problem, the paper proposes to generate the multi-labels using a strong image classifier trained on an extra source of data,
in addition to leveraging pixel-wise multi-label predictions before the final pooling layer as a complementary location-specific supervision signal.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/6.png' | absolute_url }}">
</figure>

Re-labeling ImageNet consists of using a machine annotator, which is a classifier trained on a super-ImageNet scale datasets (like [JFT-300M](https://arxiv.org/abs/1707.02968) and [InstagramNet-1B](https://arxiv.org/abs/1805.00932)) and fine-tunned on ImageNet. While being trained on single-label classification, such classifiers are still 
capable of multi-label predictions for images with multiple categories, making them suitable for relabeling ImageNet. In addition to generating the multi-label classes, Re-labeling also leverages the location-specific labels, which are the spatial features before the global pooling layer weighted by the weights of the last fully-connected layers. The model is then trained on both the original ImageNet labels and the generated location-specific labels. For the second part, the paper proposes LabelPooling which conducts a regional pooling ([RoI Align](https://arxiv.org/abs/1703.06870)) operation on the label map corresponding to the coordinates of the random crop
(see figure above, right), which are passed to the machine annotator (pretrained then finetuned classifier) to generate the multi-class labels used for the multi-label loss.

#### Other papers to check out
- [You Only Look One-Level Feature](https://arxiv.org/abs/2103.09460).
- [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483).
- [Line Segment Detection Using Transformers Without Edges](https://arxiv.org/abs/2101.01909).
- [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511).
- [Multimodal Motion Prediction With Stacked Transformer](https://arxiv.org/abs/2103.11624)s.
- [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595).

# Model Architectures & Learning Methods

#### Pre-Trained Image Processing Transformer ([paper](https://arxiv.org/abs/2012.00364))

This paper presents Image Processing Transformer (IPT), a transformer model for low-level computer vision tasks (mainly denoising, super-resolution and deraining).
IPT has 4 main blocks. First, multiple heads for extracting features from the inputs, which are the corrupted images such as images with noise and low-resolution images. Each head is a small convolutional net (3 layers) that outputs $$C$$ dimensional feature maps of the same spatial dimensions as the input. The standard encoder-decoder transformer blocks for feature refinement and information recovery, with the introduction of task embedding as input seeds into the decoder. Finally, multiple tails that map the transformer's output features back into the image space.  

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/32.png' | absolute_url }}">
</figure>

For Pre-training, IPT is trained on corrupted inputs ImageNet. For a given image, first, a corruption function is applied (such as bicubic degradation to generate low-resolution images for super-resolution, or adding Gaussian noise for denoising), the model is then train with an L1 loss between the reconstructed image from the output of IPT and the clean image. Additionally, in order to make IPT applicable to other low-level tasks outside of the introduced corruptions, they also train with a contrastive loss where the model is trained to maximize the similarity between the features of the patches coming from the same input image. After pre-training, then model can then be fine-tuned on the low level task of choice.

#### RepVGG: Making VGG-Style ConvNets Great Again ([paper](https://arxiv.org/abs/2101.03697))

The popular CNN architectures, while delivering good results, still have some drawbacks:
1) The complicated multi-branch designs, such as residual connections in ResNet and branch-concatenation in Inception, make the models difficult to implement/customize, slow down the inference and reduce the memory utilization. 2) Some components such as depthwise convs in MobileNets and channel shuffle in ShuffleNets increase the memory access cost and lack support in various devices.
This paper takes a step back in time, and proposes RepVGG, a network with VGG-like design, where at inference-time, the network is composed of nothing but a stack of 3x3 convolutions and ReLUs.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/33.png' | absolute_url }}">
</figure>

While plain CNNs have many strengths, they possess one fatal weakness: poor performance :). So in the design of RepVGG, there needs to be a multi-branch architecture introduced in training time, and at inference time, RepVGG can then fall back in the plain one branch architecture for efficiency. In order to do this, the design of RepVGG is based on what the paper calls *Structural Re-param*, which describes how to convert a trained multi-branch block into a single 3x3 conv layer for inference.
For a single block with three branches, a 3x3 conv, a 1x1 conv and identity (can be viewed as 1x1 conv with an identity kernel), each one followed by a batch norm, where the ouput of the block is the sum of the outputs of the three branches. At inference time, RepVGG first converts each conv & the following batch norm of each branch into a single conv with a bias vector. Then, the remaining 3 convs are combined into a single 3x3 conv layer by adding up the biases and adding up the kernels (where the center of 3x3 conv gets the single value in the 1x1 kernel).

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/34.png' | absolute_url }}">
</figure>

#### Bottleneck Transformers for Visual Recognition ([paper](https://arxiv.org/abs/2101.11605))

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/36.png' | absolute_url }}">
</figure>

BoTNet, or Bottleneck Transformer, consists of a simple adjustment of the ResNet architecture to incorporate self-attention.
This is done by just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes. With such an adjudgment, the resulting BoTNet improves upon the ResNet baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/35.png' | absolute_url }}">
</figure>

#### Scaling Local Self-Attention for Parameter Efficient Visual Backbones ([paper](https://arxiv.org/abs/2103.12731))

While self-attention models have recently been shown to provide notable improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50, they are still not on par with the high-performing convolutional models
such as [EfficientNet](https://arxiv.org/abs/1905.11946) ([V2](https://arxiv.org/pdf/2104.00298.pdf)).
In order to close this gap, this paper proposes a new self-attention model family called HaloNets, which is based 
on a more efficient implementation of self-attention, that improves the speed, memory usage, and accuracy of these models.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/37.png' | absolute_url }}">
</figure>

Global self-attention, in which all locations attend to each other, is too expensive for most image scales due to the quadratic computation cost with respect to $$k$$ (the kernel size). Thus, self-attention based models like [SASA](https://arxiv.org/abs/1906.05909) 
use a local form of self-attention that aggregates the information around each pixel similar to convolutions. However, to do this, 
we need to extract local 2D grids around each pixel, and such an operation can be quite expensive both computationally and memory wise, since for each pixel, we need to fetch $$k^2$$ pixels, and this operation contains a lot of duplicates since two neighboring pixel share
most of their neighbors ($$k \times (k-1)$$ out of $$k^2$$). To solve this, HaloNets use blocked local self-attention (see figure above), where the local neighborhood for a block of pixels is extracted once together, instead of extracting separate neighborhoods per pixel. This operation consist of first dividing the input tensor into non-overlapping blocks, where each block behaves as a group of query pixels, then a shared neighborhood is constructed around each block, which is used to compute the keys and values and finally the outputs. This way, we only compute one neighborhood per block instead of one neighborhood per pixel (see section 2.2 for more details). After defining such an operation, HaloNets is designed based on a similar architecture to ResNets with blocked local self-attention instead of convolutions.

#### Involution: Inverting the Inherence of Convolution for Visual Recognition ([paper](https://arxiv.org/abs/2103.06255))

Convolution kernels have two main properties, spatial-agnostic, where the same kernel is applied to all of the spatial location of the input volume, and channel-specific, where many kernels is applied over the same input, resuling in an output with multiple channles in order to collect diverse information. These two propoerties result in an enhacend efficiency and makes the covolution operation translation equivalence. However, with such a design comes some limitations, such as a contrained receptive field where a single conv operation can't capture long range spatial interactions and a possible inter-channel redundancy inside the convolution filters.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/38.png' | absolute_url }}">
</figure>

This paper takes a contrarian approach, and proposes an operation called *involution* that switches the two properties of convoutions,
resulting in a spatial-specific and channel-agnostic operation. To have a spatial-specific operation, 
the involution kernel belonging to a specific spatial location is to be generated solely conditioned on the incoming feature vector at the corresponding location itself. This is implemented in two steps, first a 1x1 conv, where the output channels correspond to the size of the kernel ($$C = k^2$$) is first applied, generating spatial location specific kernel weights (each spatial location will have its own specific conv kernel). The output is then reshaped from one vector per spatial location into a spatial kernel, and then applied over the input volume followed by an aggregation operation (average pooling) over $$k \times k$$ volume per spatial location.
As for channel-agnostic, it can be obtained by simply sharing the convolution over all channels. This operation is then used to design RedNet, a ResNet style model with involutions.

#### On Feature Normalization and Data Augmentation ([paper](https://arxiv.org/abs/2002.11102))

The usage of normalization techniques such as bach norm in recognition models have become a standard practice, where the moments  (ie, mean and standard deviation) of latent features are often removed as when training image recognition models, which helps increase their stability and reduce the training time. However, such moments can play a much more central role in some vision tasks like image generation, where they capture style and shape information of an image and can be instrumental in the generation process. In this context, this paper proposes *Moment Exchange* or MoEx, an implicit data augmentation method that encourages the recognition model to utilize the moment information for better performances and better robustness. 

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/39.png' | absolute_url }}">
</figure>

MoEx is an operation applied in the feature space with in order to systematically regulate how much attention a network pays to the signal in the feature moments. As illustrated above, given an input, the mean and variance across channels are first extracted after the first layer. Then, instead of removing them, their swapped with the moments of an other image that are extracted in the same manner. This results in a set of features that contains information about both images, and then model is then trained to predict an interpolation of the labels of the two inputs. This way, the model is pushed to focus on two different signals for classification, 
the normalized feature of the first image and the moments of the second.
  
- [MIST: Multiple Instance Spatial Transformer](https://arxiv.org/abs/1811.10725)
- [Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation](https://arxiv.org/abs/2012.07177)
- [Decoupled Dynamic Filter Networks](https://arxiv.org/abs/2104.14107)
- [Skip-Convolutions for Efficient Video Processing](https://arxiv.org/abs/2104.11487)
- [Metadata Normalization](https://arxiv.org/abs/2104.09052)

# 3D Computer Vision

#### MP3: A Unified Model to Map, Perceive, Predict and Plan ([paper](https://arxiv.org/abs/2101.06806))

Most modern self-driving stacks require up-to-date high-definition maps that contain rich semantic information necessary for driving, such as the topology and location of the lanes, crosswalks, traffic lights, intersections as well as the traffic rules for each lane. While such maps greatly facilitate the perception and motion forecasting tasks, as the online inference process has to mainly focus on dynamic objects (eg, vehicles, pedestrians, cyclists), scaling them is hard given their complexity and cost, and given that even very small errors in the mapping might result in fatal mistakes. This motivates the development of mapless technology, which can serve as the fail-safe in the case of localization failures or outdated maps, and potentially unlock self-driving at scale at a much lower cost.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/10.png' | absolute_url }}">
</figure>

However, with a mapless approach comes a number of challenges: (1) The sole source of training signal is the controls of an expert driver (such steering and acceleration), without providing intermediate interpretable representations that can help explain the self-driving vehicle decisions. (2) Without any mechanism to inject structure and prior knowledge, such an approach can very brittle to distributional shift such missing a lane. To address these issues, the paper presents MP3, an end-to-end approach to mapless driving that is interpretable, does not incur any information loss, and reasons about uncertainty in the intermediate representations.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/11.png' | absolute_url }}">
</figure>

The MP3 model takes as input a high-level goal, a history of LiDAR point clouds to extract rich geometric and semantic features from the scene over time (see [this](https://arxiv.org/abs/2012.12395) for more details), and odometry data to compensate for the vehicle's motion.
The inputs are then processed using a backbone network and fed into a set of probabilistic spatial layers to model the static and dynamic parts of the environment.
The static environment is represented by a planning-centric online map which captures information about which areas are drivable and which ones are reachable given the traffic rules. The dynamic actors are captured in a novel occupancy flow that provides occupancy and velocity estimates over time. The motion planning module then leverages these representations
to retrieve dynamically feasible trajectories, predicts a spatial mask over the map to estimate the route given an abstract goal, and leverages the online map and occupancy flow directly as cost functions for explainable, safe plans.

#### Multi-Modal Fusion Transformer for End-to-End Autonomous Driving ([paper](https://arxiv.org/abs/2104.09224))

In a standard driving situation such as the one depicted in the left image bellow, the vehicle must capture the global context of the scene involving the interaction between the traffic light (yellow) and the vehicles (red) for a safe navigation. To do this, the different modalities (ie, the LiDAR point cloud and the camera view) must be fused together in order to obtain such a global view. This raises the following questions: how to fuse such multi-modal representations? to what extent each modality should be processed independently before fusion? and how can such fusion be conduced?. The paper proposes a Modal Fusion Transformer (TransFuser), a transformer-based model designed to integrate both LiDAT and camera views with global attention, thus capturing the necessary 3D context for safe navigation.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/9.png' | absolute_url }}">
</figure>

The paper considers the task of point-to-point navigation in an urban setting, where the goal is to complete given route while safely reacting to other dynamic agents and following traffic rules. The TransFuser is trained using L1 loss between the predicted trajectories and the correct trajectories using a dataset consisting of high-dimensional observations of the environment, and the corresponding expert trajectory.
TransFuser takes as input RGB images and [LiDAR BEV](https://arxiv.org/abs/1905.01296) representations and uses several transformer modules to fuse the intermediate feature maps between both modalities. The fusion is applied at multiple resolutions and throughout the feature extractor resulting in a 512-dimensional feature vector output from both the image and LiDAR BEV stream, which is the desired compact representation of the environment that encodes the global context of the 3D scene. This compact representation can then be used as input to an auto-regressive (a GRU) prediction network that outputs the trajectories in the form of waypoints in vehicle's coordinate frame. 

#### Neural Lumigraph Rendering ([paper](https://arxiv.org/abs/2103.11571))

The recent [neural rendering](https://arxiv.org/abs/2004.03805) techniques are capable of generating photorealistic image quality for novel view synthesis and 3D shape estimation from 2D images. But they are either slow to train and/or require a considerable rendering time time for high image resolutions. [NeRF](https://arxiv.org/abs/2003.08934) for instance does not offer real-time rendering due to the use of neural scene representation and [volume rendering](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.3394).
To overcome this, the paper proposes the use SDF-based sinusoidal representation network ([SIREN](https://arxiv.org/abs/2006.09661)) to implicitly model the surface of objects (ie, implicitly defining an object or a scene using a neural network and training directly with 3D data), which can be extracted using the [marching cubes algorithm](https://dl.acm.org/doi/10.1145/37401.37422) and exported into traditional mesh-based representations for real-time rendering.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/7.png' | absolute_url }}">
</figure>

However, given the high capacity of SIRENs, they are prone to overfitting, making them incapable of rendering new views that are interpolations of views encountered during training. To solve this, the authors propose a novel smoothness loss function that maintains SIREN's high-capacity encoding in the image domain while constraining it in the angular domain to prevent overfitting on these views. The SIREN based network can then be trained using a sparse set of multi-view 2D images while providing a high-quality 3D surface that can be directly exported for real-time rendering algorithms at test time.

#### NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis ([paper](https://arxiv.org/abs/2012.03927))

NeRV extends [NeRF](https://arxiv.org/abs/2003.08934) to arbitrary lighting conditions, taking as input a set of images of a scene illuminated by unconstrained lighting, and producing a 3D representation that can be rendered from novel viewpoints under novel and unobserved lighting conditions.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/8.png' | absolute_url }}">
</figure>

Given that NeRF models just the amount of outgoing light from a location, the fact that this out going light is itself the result of interactions between incoming light and the material properties of an underlying surface is ignored, so rendering viewpoints
under novel lighting conditions is not possible. A naive way (figure above, right) to solve this is to query NeRF's MLP for the volume density at samples along the camera ray to determine the amount of light reflected at each location that reaches the camera. Then for each location, 
query the MLP for the volume density at points between the location and every light source. This procedure is clearly too computationally infeasible. The paper solve this by proposing NeRV, a method to train a NeRF-like model that can simulate realistic environment lighting and global illumination by using an additional MLP as a lookup table into a visibility field during rendering. As a result, the training consists of jointly optimizing the visibility MLP (or reflectance MLP) for estimating the light surface visibility at a given 3D position along, alongside the NeRF's MLP (also called the shape MLP) for volumetric representation. At test time, the rendering is then conducted along the ray by querying the shape and reflectance MLPs for the volume densities, surface normals, and [BRDF](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function) parameters at each point.

#### Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans ([paper](https://arxiv.org/abs/2012.15838))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/12.png' | absolute_url }}">
</figure>

Given a very sparse set of camera views (say 3 or 4 views as depicted above), learning implicit neural representations of 3D scenes becomes infeasible. To solve this, the paper proposes Neural Body, a method that leverages observations over video frames order to learn
a new human body representation that is consistent over the different frames, and shares the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/13.png' | absolute_url }}">
</figure>

First, a set of latent codes are defined and anchored to the vertices of the [SMLP](https://smpl.is.tue.mpg.de/) deformable human model, so that the spatial location of each vertex varies with the human pose. Then, the 3D representation at a given frame is estimated from sparse camera views, and then applied to the code location based on the obtained 3D human pose. Finally, the network is trained to regress the density and color for any 3D point based on these latent codes. Both the latent codes and the network are jointly learned from images of all video frames during the reconstruction process.

#### pixelNeRF: Neural Radiance Fields From One or Few Images ([paper](https://arxiv.org/abs/2012.02190))

The [NeRF](https://arxiv.org/abs/2003.08934) framework consists of optimizing the representation of every scene independently, requiring many views per scene and a significant compute time. pixelNeRF adapts NeRF in order to be trained across multiple scenes jointly to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). This is done by 
using spatial features of a given image produced by a CNN, which are aligned to each pixel as an input, and then passes through NeRF model for training. This simple image conditioning allows the model to be trained on a set of multi-view images, where it can learn scene priors to perform view synthesis.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/14.png' | absolute_url }}">
</figure>

pixelNeRF consists of two components: a fully-convolutional image encoder, which encodes the input image into a pixel-aligned feature grid, and a NeRF network which outputs color and density given a spatial location and its corresponding encoded feature. 
When multiple input views are available, each view is encoded into the feature grid, the multiple features are processed in parallel and then aggregated into the final color and opacity.

#### Other papers to check out

- [Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction](https://arxiv.org/abs/2012.03065)
- [NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268)
- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961)
- [Learning to recover 3D Scene Shape from a single image](https://arxiv.org/abs/2012.09365)
- [NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video](https://arxiv.org/abs/2104.00681)
- [Point2Skeleton: Learning Skeletal Representations from Point Clouds](https://arxiv.org/abs/2012.00230)
- [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)
- [NeX: Real-Time View Synthesis With Neural Basis Expansion](https://arxiv.org/abs/2103.05606)
- [Holistic 3D Scene Understanding From a Single Image With Implicit Representation](https://arxiv.org/abs/2103.06422)
- [Scan2Cap: Context-aware Dense Captioning in RGB-D Scans](https://arxiv.org/abs/2012.02206)
- [Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction](https://arxiv.org/abs/2012.01451)
- [Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)
- [Learning Delaunay Surface Elements for Mesh Reconstruction](https://arxiv.org/abs/2012.01203)
- [A Deep Emulator for Secondary Motion of 3D Characters](https://arxiv.org/abs/2103.01261)
- [NeuTex: Neural Texture Mapping for Volumetric Neural Rendering](https://arxiv.org/abs/2103.00762)
- [DeRF: Decomposed Radiance Fields](https://arxiv.org/abs/2011.12490)
- [IBRNet: Learning Multi-View Image-Based Rendering](https://arxiv.org/abs/2102.13090)
- [Learned Initializations for Optimizing Coordinate-Based Neural Representations](https://arxiv.org/abs/2012.02189)
- [AutoInt: Automatic Integration for Fast Neural Volume Rendering](https://arxiv.org/abs/2012.01714)
- [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)
- [Back to the Feature: Learning Robust Camera Localization From Pixels To Pose](https://arxiv.org/abs/2103.09213)
- [Wide-Baseline Relative Camera Pose Estimation With Directional Learning](https://arxiv.org/abs/2106.03336)
- [Pulsar: Efficient Sphere-Based Neural Rendering](https://arxiv.org/abs/2004.07484)
- [Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes](https://arxiv.org/abs/2101.10994)
- [Deep Multi-Task Learning for Joint Localization, Perception, and Prediction](https://arxiv.org/abs/2101.06720)
- [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://arxiv.org/abs/2104.11224)
- [Shelf-Supervised Mesh Prediction in the Wild](https://arxiv.org/abs/2102.06195)
- [LoFTR: Detector-Free Local Feature Matching With Transformers](https://arxiv.org/abs/2104.00680)
- [Shape and Material Capture at Home](https://arxiv.org/abs/2104.06397)
- [SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans](https://arxiv.org/abs/2006.14660)
- [Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty](https://arxiv.org/abs/2104.08278)
- [Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images](https://arxiv.org/abs/2105.02047)
- [MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments From a Single Moving Camera](https://arxiv.org/abs/2011.11814)
- [DECOR-GAN: 3D Shape Detailization by Conditional Refinement](https://arxiv.org/abs/2012.09159)

# Image and Video Synthesis

#### GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields ([paper](https://arxiv.org/abs/2011.12100))

While GaNs are capable of generating photorealistic and diverse high resolutions images, having a fine-grained control over the factors of variation in the data and the compositionality of the generated scenes is sill limited, operating only in 2D and ignoring the three-dimensional nature of the underlying scenes.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/15.png' | absolute_url }}">
</figure>

GIRAFFE proposes to represent scenes as compositional generative neural feature fields. During training, instead of having a single latent code as in the standard GaN setting, GIRAFFE randomly generated a set of *Shape* and *Appearance* codes for each object (& background) in the scene, which are used to generated Feature Fields. Then a second set of latent codes are generated, this time representing the pose transformation, which are applied to the generated Feature standard to obtain the Posed Feature Fields. Finally, the final Posed Feature Fields are aggregated into a single scene representation, and given a camera pose, are used as input to the neural rendering network to generate a 2D image. The generated and real 2D images are then passed to the discriminator to compute the adversarial loss. The whole model is trained end-to-end, and at test time, the composition of the generated images can be controlled using the Shape & Appearance and Pose latent codes.

#### GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving ([paper](https://arxiv.org/abs/2011.12100))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/16.png' | absolute_url }}">
</figure>

The ability to simulate/enhance various real world scenarios is an important yet challenging open problem, especially for safety-critical domains such as self-driving where producing visually appealing and realistic results requires a physics-based rendering, which is very costly. As a, alternative, GeoSim exploits the recent advances in image synthesis by combining data driven approaches (such as generative modeling) and computer graphics to insert dynamic objects into existing videos, while maintaining high visual quality through physically grounded simulation (see the example above). 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/17.png' | absolute_url }}">
</figure>

The first step of GeoSim is to create a large 3D assets (vehicles of different types and shapes) with accurate pose, shape and texture. Instead of using artists to create these assets, GeoSim leverages publicly available datasets to construct 3D assets of the objects. This is done using a learning-based, multi-view, multi-sensor reconstruction approach that leverages the 3D bounding boxes and trained in a self-supervised manner so that there is an agreement between the predicted 3D shape and that of the camera and LiDAR observations.
GeoSim then exploits the 3D scene layout from high-definition maps and LiDAR data to add these learned 3D assets in plausible locations and make them behave realistically by considering the full scene. Finally, using this new 3D scene, GeoSim performs image-based rendering to properly handle occlusions, and neural network-based image in-painting to ensure the inserted object seamlessly blends in by filling holes, adjusting color inconsistencies due to lighting changes, and removing sharp boundaries.
Check out [the paper's website](https://tmux.top/publication/geosim/) for some results.

#### Taming Transformers for High-Resolution Image Synthesis ([paper](https://arxiv.org/abs/2012.09841))

The recently introduced vision transformers (such as [ViT](https://arxiv.org/abs/2010.11929)) demonstrated that they can perform on par with
CNNs, and given enough training data, they tend to learn convolutional structures. This raises an obvious question, do we have to relearn such an inductive bias 
from scratch each time we train a vision model?. This paper proposes to merge both CNNs and transformers into a single framework for image synthesis. Thus leveraging the
efficiency of CNNs and their inductive image biases while still retaining the flexibility of transformers.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/18.png' | absolute_url }}">
</figure>

As depicted above, the proposes framework consists of CNN encoder-decoder network trained adversarially for [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937), and then a transformer that operates over the discrete representations in an autoregressive manner. More specifically, the training consist of two stages. First, the encoder, the decoder and the discriminator, all CNN-based, are trained using a reconstruction loss, [a perceptual loss](https://arxiv.org/abs/1801.03924), a commitment loss (ie, used to refine the codebook, see [VQVAE](https://arxiv.org/abs/1711.00937) for more details), and an adversarial loss. At the end of training, we end-up with a learned codebook where each spatial location in the input image can be represented by index in the codebook. By using such a formulation, the input image can be viewed as a sequence of codebook indices, and such a sequence can be used to train the auto-regressive transformer in the second step of the training process. Starting from the top right corner, at each time step, the transformer is tasked with predicting the next codebook index, and in order to reduce the computation, the input to the transformer is restricted into a sliding window without a significant loss in performance. Finally, at test time, we can use the trained transformer to generate large sequences without any restrictions (and with any type of conditioning, see section 3.2 of the paper), which correspond to very large images. The predicted indices are then used to fetch the discrete representations from the codebook, which are then passed to the decoder to synthesis an image.

#### Rethinking and Improving the Robustness of Image Style Transfer([Paper](https://arxiv.org/abs/2104.05623))

The objective of image style transfer is to map the content of a given image into the style of a different one, and in such task, VGG network has demonstrated 
remarkable ability to capture the visual style of an image. However, when such a network is replaced with a more modern, and a better performing network such as
ResNet, stylization performance degrades significantly as shown in the figure bellow.

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/19.png' | absolute_url }}">
</figure>

In this paper, the authors investigate the root cause of this behavior, and find that residual connections, which represent the main architectural difference between VGG and ResNet, produce peaky feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the ResNet architecture, they authors propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. This method, dubbed Stylization With Activation smoothinG (SWAG), consists of adding a softmax-based smoothing transformation to all of the activations in order to push the model to produce smoother activations, thus reducing large peaks and increasing small values, creating a more uniform distribution.

#### Learning Continuous Image Representation With Local Implicit Image Function ([Paper](https://arxiv.org/abs/2012.09161))

This paper proposes Local Implicit Image Function (LIIF) for representing natural and complex images in a continuous manner. With LIIF, an image is represented as a set of latent codes distributed in spatial dimensions. Given a coordinate, the decoding function takes the coordinate information and queries the local latent codes around it, and 
then predicts the RGB value at the given coordinate as an output. Since the LIIF representation is continuous, we can query an arbitrary high target resolution up to x30 higher than the training resolution encountered during training. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/20.png' | absolute_url }}">
</figure>

The proposed framework consists of an encoder that produces 2D feature maps given an input image, where the feature maps are evenly distributed in the 2D space of the continuous image domain, and each feature at a given spatial location is called a latent code. Then, the decoder takes as input a 2D coordinate in the image domain in addition to a weighted average of the 4 nearest latent codes oo the chosen 2D coordinate and outputs the RGB values. Now, in order to train both the encoder and the decoder jointly using self-supervision, this is done by taking a training image, randomly down-sampling it, the encoder then encodes the down-sampled image, while the decoder is queried to produced the RGB values of the original image, which is used as ground-truth.

#### Other papers to check out

- [Ensembling with Deep Generative Views](https://arxiv.org/abs/2104.14551)
- [SSN: Soft Shadow Network for Image Compositing](https://arxiv.org/abs/2007.08211)
- [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://arxiv.org/abs/2012.02992)
- [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060)
- [Motion Representations for Articulated Animation](https://arxiv.org/abs/2104.11280)
- [Playable Video Generation](https://arxiv.org/abs/2101.12195)
- [Repopulating Street Scenes](https://arxiv.org/abs/2103.16183)
- [Animating Pictures With Eulerian Motion Fields](https://arxiv.org/abs/2011.15128)
- [Closed-Form Factorization of Latent Semantics in GANs](https://arxiv.org/abs/2007.06600)
- [Stylized Neural Painting](https://arxiv.org/abs/2011.08114)
- [Image Generators With Conditionally-Independent Pixel Synthesis](https://arxiv.org/abs/2011.13775)
- [Cross-Modal Contrastive Learning for Text-to-Image Generation ](https://arxiv.org/abs/2101.04702)
- [Dual Contradistinctive Generative Autoencoder](https://arxiv.org/abs/2011.10063)
- [Space-Time Neural Irradiance Fields for Free-Viewpoint Video](https://arxiv.org/abs/2011.12950)
- [Positional Encoding As Spatial Inductive Bias in GANs](https://arxiv.org/abs/2012.05217)
- [Regularizing Generative Adversarial Networks Under Limited Data](https://arxiv.org/abs/2104.03310)
- [Variational Transformer Networks for Layout Generation](https://arxiv.org/abs/2104.02416)
- [Deep Animation Video Interpolation in the Wild](https://arxiv.org/abs/2104.02495)
- [Stable View Synthesis](https://arxiv.org/abs/2011.07233)
- [Navigating the GAN Parameter Space for Semantic Image Editing](https://arxiv.org/abs/2011.13786)
- [Stochastic Image-to-Video Synthesis Using cINNs](https://arxiv.org/abs/2105.04551)
- [Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing](https://arxiv.org/abs/2104.14754)
- [Plan2Scene: Converting Floorplans to 3D Scenes](https://arxiv.org/abs/2106.05375)
- [SceneGen: Learning to Generate Realistic Traffic Scenes](https://arxiv.org/abs/2101.06541)
- [OCONet: Image Extrapolation by Object Completion](https://openaccess.thecvf.com/content/CVPR2021/html/Bowen_OCONet_Image_Extrapolation_by_Object_Completion_CVPR_2021_paper.html)
- [Anycost GANs for Interactive Image Synthesis and Editing](https://arxiv.org/abs/2103.03243)
- [StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation](https://arxiv.org/abs/2011.12799)
- [Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951)
- [Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes](https://arxiv.org/abs/2103.17185)
- [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://arxiv.org/abs/2011.15126)
- [Training Generative Adversarial Networks in One Stage](https://arxiv.org/abs/2103.00430)
- [Generative Hierarchical Features From Synthesizing Images](https://arxiv.org/abs/2007.10379)

# Scene Analysis & Understanding

#### Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers ([Paper](https://arxiv.org/abs/2012.15840))

The proposed SEgmentation TRansformer (SETR) is based on an alternative formulation of semantic segmentation as a sequence-to-sequence task.
This way, instead of using the standard encoder-decoder architecture, such a formulation given us the possibility to employ a pure transformer without convolution and resolution reduction for pixel-level classification, since it is in line with the way transformers operated over
the inputs and how they produce their predictions. In addition to leveraging the capability of a transformer layer to model the global context which important for sematic segmentation in order to obtain coherent masks.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/21.png' | absolute_url }}">
</figure>

SETR (figure above, a) treats an input image as a sequence of image patches, where each image is first decomposed into a fixed-sized patches. Then, each path is flattened into a vector of pixel values and passed through a linear layer, outputting the patch embedding. These patch embedding are passed as a sequence to the transformer-encoder (ie, 24 transformer layers) with the global self-attention in order to discriminative features tailored for the segmentation task. The produced representations are then reshaped back into 2D shape (number of patches x embedding dimensionality) into the standard 3D features map shape (H x W x embedding dimensionality). The reshaped features are then passed to the decoder to predict the final per pixel classification at the original input size. Here, SETR proposes 3 types of decoders: (1) Naive upsampling: a 2-layer network followed by bilinear upsampling, (2) Progressive UPsampling: alternates between conv layers and a single 2x bilinear upsampling operation (figure above, b). (3) Multi-Level feature Aggregation: apply many levels of conv layers & 4x by bilinear upsampling over the encoder's output, merge them and apply a final upsampling (figure above, c).

#### MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers ([Paper](https://arxiv.org/abs/2012.00759))

MaX-DeepLab is an end-to-end model for panoptic segmentation without any hand-designed components such as box detection, non-maximum suppression, or thing-stuff merging. MaX-DeepLab directly predicts class-labeled masks with a mask transformer, and is trained with a panoptic quality inspired loss via bipartite matching.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/22.png' | absolute_url }}">
</figure>

Building upon the recent transformer-based end-to-end object detectors such as [DERT](https://arxiv.org/abs/2005.12872) or [Deformable DETR](https://arxiv.org/abs/2010.04159), MaX-DeepLab directly outputs non-overlapping masks and their corresponding semantic labels with a mask transformer. The model is then trained with a novel panoptic quality style loss (see section 3.2). This loss measures the similarity between ground-truth and predicted class-labeled masks as the multiplication of their mask similarity and their class similarity, and since the MaX-DeepLab outputs a larger number of masks then the ground-truths, a one-to-one matching is applied (as in DERT) before computing the loss.

As for the architecture, MaX-DeepLab integrates transformer blocks (called dual-path transformer) along a given CNN backbone in a dual-path fashion, with a bidirectional communication blocks between the two paths. Each 2D pixel-based CNN is augmented with 1D global memory with the same size as the number of predictions with different types of attention. Specifically, a dual-path transformer takes as input the 2D CNN features and 1D memory, and applies 4 types of attention: 1) pixel-to-pixel over the CNN features, and since the attention over spatial dimensions is expensive, they use [axial-attention](https://arxiv.org/abs/2003.07853). 
1) memory-to-memory updating the memory features with global context, and then the cross-attention, 3) pixel-to-memory and 4) memory-to-pixel attention, where each time the query of one is applied to the keys and values of the other to update either the pixel or memory features conditioned on the other.

#### Binary TTC: A Temporal Geofence for Autonomous Navigation ([Paper](https://arxiv.org/abs/2101.04777))

Path planning, whether for robotics or automotive applications, requires accurate perception, and one of main task used to acquire such an accurate perception is depth information. To infer depth, the most popular strategy is to use LiDAR which is capable of estimating depth, but only at sparse locations and can be quite expensive. Another alternative is to only use monocular cameras (this is the Tesla approach)
to construct the optical flow between consecutive frames, which carries information on the scene's depth, while greatly reducing the acquisition and maintenance costs. But this approach also has its drawback since it can only be estimated reliably in constrained and simple scenes. In this context, and since the objective behind the perception is to inform decisions, Binary TTC proposes to replace learning to infer depth with a new and simpler task that can be directly used to inform planning decisions.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/23.png' | absolute_url }}">
</figure>

The proposed Binary TTC task is based on the concept of [time-to-contact (TTC)](https://pubmed.ncbi.nlm.nih.gov/1834797/), which is the time for an object to collide with the camera plane under the current velocity conditions, and consists of predicting a binary classification map, where the objects that will collide with the camera plane within a given time are assigned a labels of 1.
More specifically, a binary classification network is trained to detect the objects that will collide with the camera plane for a given time interval. To train the network, the labels are generated using two images of a given dynamic scene, then, the sizes of each moving object in the two images is compared (after a scaling is applied to take into account the chosen time of collision). If the size becomes larger from the first to the second image, this means that the object is getting closer to the camera plane and can be labeled as 1.

#### Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging ([Paper](https://arxiv.org/abs/2105.14021))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/24.png' | absolute_url }}">
</figure>

The paper starts with a analysis of the behavior of standard monocular depth estimation models (they use [MiDaS](https://arxiv.org/abs/1907.01341)) when fed images at different resolutions. With small resolutions, the estimations lack many high-frequency details while generating a consistent overall structure of the scene. As the input resolution gets higher, more details are generated in the result but with inconsistencies in the scene structure characterized by gradual shifts in depth between image regions.

Based on the above observation, the paper proposes to merge depth estimates at different resolution with an image-to-image translation network that merges these estimates into a final prediction.
In order to determine the input resolutions which to be merged later, the authors propose to base this selection on the number of pixels that do not have any contextual cues nearby. These regions with the lowest contextual cue density will dictate the maximum resolution
that can be used for an image. Additionally, in order to also benefit from higher-resolution estimations to generate more high-frequency details, a patch based selection method is used to fin regions with higher contextual cue density that requires more high-frequency details, which are then merged together for the final results. 

Check out the authors [video](https://www.youtube.com/watch?v=lDeI17pHlqo) for a  brief but great explanation of the work.

#### Polygonal Building Extraction by Frame Field Learning ([Paper](https://arxiv.org/abs/2004.14875))

For the task of building segmentation where the objective is to output a polygons for each building in a given aerial photo, the existing approach are either based on vector representations, directly predicting vector polygons, or two step approach that first produce probability a map, which is then followed by polygon simplification. However, both approach are either hard to train or contain many steps before the final output.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/25.png' | absolute_url }}">
</figure>

For an end-to-end method that is easy to optimize, the authors propose to build on the semantic segmentation methods, where in addition to predicting a segmentation maps corresponding to the buildings in the image, they also task the model with predicting a frame field as a geometric prior to constrain the segmentation maps to have sharpe corners.

#### Other papers to check out

- [VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation ](https://arxiv.org/abs/2012.05258)
- [Exemplar-Based Open-Set Panoptic Segmentation Network](https://arxiv.org/abs/2105.08336)
- [The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth](https://arxiv.org/abs/2104.14540)
- [Robust Consistent Video Depth Estimation](https://arxiv.org/abs/2012.05901)
- [Scene Essence ](https://openaccess.thecvf.com/content/CVPR2021/html/Qiu_Scene_Essence_CVPR_2021_paper.html)
- [Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization](https://arxiv.org/abs/2104.05833)
- [Repurposing GANs for One-Shot Semantic Part Segmentation](https://arxiv.org/abs/2103.04379)
- [Deep Occlusion-Aware Instance Segmentation With Overlapping BiLayers](https://arxiv.org/abs/2103.12340)
- [Information-Theoretic Segmentation by Inpainting Error Maximization](https://arxiv.org/abs/2012.07287)
- [Single Image Depth Prediction With Wavelet Decomposition](https://arxiv.org/abs/2106.02022)
- [Learning to Recover 3D Scene Shape from a Single Image](https://arxiv.org/abs/2012.09365)

# Representation & Adversarial Learning

#### Exploring Simple Siamese Representation Learning ([Paper](https://arxiv.org/abs/2011.10566))

In the recent contrastive learning methods used for learning useful visual representation in an unsupervised manner, a model
is trained to map similar input images close by in the embedding space. Such methods, such [MoCo](https://arxiv.org/abs/1911.05722), [SimCLR](https://arxiv.org/abs/2002.05709), or [BYOL](https://arxiv.org/abs/2006.07733), add some additional conditions to the similarity maximization objective to avoid collapsing solutions, such as negative sample pairs, large batches, or momentum encoders.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/26.png' | absolute_url }}">
</figure>

This paper investigates the usage of SimSiam, a simple Siamese based setup where such conditions are not necessary, such an architecture consists of an encoder and a projector/prediction, which is then train to maximize the similarity between the two features (one after projection and one without) corresponding to two augmented version of an input images, with a stop gradient operator applied to the second projected output. The obtained results show that this simple approach gives similar performances to other more elaborate approaches, indicating that the Siamese architecture may be an essential reason for the common success of the other related contrastive methods, see section 5 for more details.

#### Where and What? Examining Interpretable Disentangled Representations ([Paper](https://arxiv.org/abs/2104.05622))

In disentangled representation learning, the objective is produce a representation of an input, where each dimension captures variations with
a semantic meaning. One of the main limitations of existing work is their inability to differentiate between entangled and disentangled representations in the solution pool.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/27.png' | absolute_url }}">
</figure>

To solve this non-uniqueness problem, the paper defines disentanglement from three perspectives: informativeness, independence, and *interpretability*. By adding interpretability condition to the produced representations, we force the model to only produce disentangled representations that do correspond to human-defined concepts. Now, the question becomes how can we interpretability in the representations without supervision?.
To solve this, the authors propose to exploit two hypotheses about interpretability to learn disentangled representations. The first one is Spatial Constriction (SC): a representation is usually interpretable if we can consistently tell where the controlled variations are in an image. The second hypothesis is Perceptual Simplicity: an interpretable code usually corresponds to a concept consisting of perceptually simple variations.

Based on these two hypothesis, a new model is introduced, where the Spatial Constriction is enforced with a SC module that restricts the impact of each latent code to specific areas on feature maps during generation. As for Perceptual Simplicity, the model is trained with a loss that  encourages the model to embed simple data variations along each latent dimension.

#### Audio-Visual Instance Discrimination with Cross-Modal Agreement ([Paper](https://arxiv.org/abs/2004.12943))

The paper presents a for cross-modal discrimination for self-supervised learning, in order to learn audio-visual representations from video and audio. The proposed contrastive learning frame-work consists of contrasting video representations against multiple audios representations at once (and vice versa), thus the cross-modal nature of the method.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/28.png' | absolute_url }}">
</figure>

The proposed approach learns both a cross-modal similarity metric by grouping video and audio instances that that co-occur over multiple instances, in addition to optimizing for visual similarity rather than just cross-modal similarity.

#### UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers ([Paper](https://arxiv.org/abs/2011.09094))

While [DETR](https://arxiv.org/abs/2005.12872) proposed a simple end-to-end object detector, thus removing all hand-designed components, it still comes with some training and optimization challenges, requiring large-scale training data and long training times (up to 500 epochs). UP-DETR proposes a new unsupervised pre-training tasks in order to reduce the amount of training time and data required, where 
DETR is first pre-trained on a pretext task designed specifically for object detection as a desired down-stream task.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/29.png' | absolute_url }}">
</figure>

Unsupervised Pre-training DETR (UP-DETR) defines random query patch detection as a pretext task to pre-train DETR in a self-supervised manner.
For this task, a set of patches are first cropped from the input image at random, the model is then trained to predict the bounding boxes of these patches.
The objective of this pre-training stage is to equip the model with better localization while maintaining its classification features. So to avoid suppressing the learned
classification features of the pre-trained backbone, UP-DETR freezes the backbone and adds a patch feature reconstruction loss.
Additionally, in order to specify the query patch the model needs to detect, the CNN features of the patch itself is added to the object queries before feeding them
to the decoder. For multi-patch detection, the patch are grouped into sets of patches and each set is assigned into a given object query, and the attention aggregation is applied with a masking operation so that each prediction is not dependent on the rest (see section 3.2).


#### Fast End-to-End Learning on Protein Surfaces ([Paper](https://www.biorxiv.org/content/10.1101/2020.12.28.424589v1))

Chemically, proteins are composed of a sequence of amino acids which determines the structure (called fold) of the protein, and this structure in turn 
determines the function the protein will have. The task of either structure prediction from a sequence of amino acids (what AlphaFold does) or the protein design from its structure, are both major unsolved problems in structural biology. Another challenging problem, the task of interest in this paper, is the study of the interactions of a given molecule with other molecules given their composition in order to identify interaction patterns on protein surfaces.

<figure style="width: 65%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/30.png' | absolute_url }}">
</figure>

In this context, the paper proposes dMaSIF (differentiable molecular surface interaction fingerprinting), a new and computationally efficient deep learning approach
that operates directly on the large set of atoms that compose the protein. It first generates a point cloud representation for the protein surface, learns task-specific geometric and chemical features on the surface point cloud, and finally, applies a new convolutional operator that approximates geodesic coordinates in the tangent space (please see the paper since I'm way over my head here).

#### Natural Adversarial Examples ([Paper](https://arxiv.org/abs/1907.07174))

ImageNet test examples tend to be simple, clear, close-up images, so the current test set may be too easy and may not represent harder images encountered in the real world, and a large capacity model can leverage spurious cues or shortcuts to solve the ImageNet classification problem.
To counteract this, the paper proposes two hard ImageNet test sets: *ImageNet-A* and *ImageNet-O* of natural adversarial examples with adversarial filtration.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/31.png' | absolute_url }}">
</figure>

*ImageNet-A* consists of real-world adversarially filtered images that fool current ImageNet classifiers.
Starting from a large set of images related to an ImageNet class, adversarially filtered examples are found by removing the samples that are correctly classified by ResNet-50 trained on ImageNet. *ImageNet-O* on the other hand in is a dataset of adversarially filtered examples for ImageNet out-of-distribution detectors.
Starting from ImageNet-22K, first, all of the examples that belong to ImageNet-1K classes are removed. Then, using a ResNet-50, the only maintained examples are the ones that were classified by the model into ImageNet-1K classes with high confidence.

- [Spatiotemporal Contrastive Video Representation Learning](https://arxiv.org/abs/2008.03800)
- [Adversarially Adaptive Normalization for Single Domain Generalization](https://arxiv.org/abs/2106.01899)
- [Self-Supervised Geometric Perception](https://arxiv.org/abs/2103.03114)
- [CutPaste: Self-Supervised Learning for Anomaly Detection and Localization](https://arxiv.org/abs/2104.04015)
- [Taskology: Utilizing Task Relations at Scale](https://arxiv.org/abs/2005.07289)
- [MOS: Towards Scaling Out-of-Distribution Detection for Large Semantic Space](https://arxiv.org/abs/2105.01879)

# Transfer, Low-shot, Semi & Unsupervised Learning

#### DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort

DatasetGAN is a method that generates massive datasets of high-quality semantically segmented images with minimal human effort. 
Based on the observation that GaNs are capable of acquiring rich semantic knowledge in order to render diverse and realistic examples of objects, DatasetGAN exploits the feature space of a trained GAN and train a shallow decoder to produce a pixel-level labeling, where such a decoder is trained on a very small number of labeled examples, and can then be used to labeled an infinite amount of synthetic images. The generated dataset can be used to train a model in an semi-supervised manner on the synthetic dataset, which 
can then be tested on real world images.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/40.png' | absolute_url }}">
</figure>

The architecture of DatasetGAN consist of two models, a StyleGAN that generated synthetic image, in addition to Style-Interpreter in the form of an ensemble of three-layer MLP classifiers, where each classifier takes as inputs a feature maps from StyleGAN (outputs of AdaIN layers), upsamples them to the image resolutions and predict pixel-level labels. The final prediction is the aggregation of the predictions of all of the MLP classifiers, which are then trained with a small number of finely annotated examples, and used for labeling the synthetic images.

#### Ranking Neural Checkpoints

During a given deep learning experiment, it is common practice to collect many checkpoint, which are different version of the final model at different training iteratins. This paper is concerned with ranking such checkpoints and find the models that transfers the best to the downstream task of interest.

More specifically, given a number of pretrained neural nets, called checkpoints $$\mathcal{C}$$. The objective is to find the best checkpoint over a distribution of downstream tasks $$\mathcal{T}$$. Each task consists of training and testing sets, and an evaluation procedure $$\mathbf{G}$$ that adjust the pretrained model by adding the task specific head, finetunes the model on the training set with a hyperparameter sweep under a given computation contraint, and retuns a perforamnce measure $$\mathbf{R}$$.
The objective of the paper is to find the best measure $$\mathcal{M}$$ to correctly rank the checkpoints.

$$\mathbf{R}^{*} \leftarrow \underset{\mathbf{R} \in \mathcal{R}}{\arg \max } \mathbb{E}_{t \sim \mathcal{T}} \mathcal{M}\left(\mathbf{R}_{t}, \mathbf{G}_{t}\right)$$

The paper proposes a measure called NLEEP, an extention of [LEEP](https://arxiv.org/abs/2002.12462) that evaluates the degree of transferability of the learned representations from source to target data without training. LEEP consists of computing the empirical conditional distribution of target labels given dummy source labels to measure the degree of transferability. NLEEP simply replaces the softmax classifier used for generating the dummy source labels with a Gaussian mixture model for more reliable than the class assignment. This way, we can evaluate the checkpoints on downstrem tasks while reducing cost of the evaluation procedure since LEEP does not require fine-tuning on the target data.

#### Other papers to check out

- [MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking](https://arxiv.org/abs/2104.00303)
- [Meta Pseudo Labels](https://arxiv.org/abs/2003.10580)
- [Adaptive Prototype Learning and Allocation for Few-Shot Segmentation](https://arxiv.org/abs/2104.01893)
- [Student-Teacher Learning From Clean Inputs to Noisy Inputs](https://arxiv.org/abs/2103.07600)
- [Learning Graph Embeddings for Compositional Zero-Shot Learning](https://arxiv.org/abs/2102.01987)

# Computational Photography

#### Real-Time High-Resolution Background Matting

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/42.png' | absolute_url }}">
</figure>

While many tools now provide background replacement functionality, they often yield artifacts at the boundaries, particularly in areas where there is fine detail like hair or glasses. On the other hand, the traditional image matting methods
provide much higher quality results, but do not run in real-time, at high resolutions, and frequently require manual input.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/41.png' | absolute_url }}">
</figure>

This paper proposes a real-time and high-resolution background matting method capable of processing 4K (3840x2160) images at 30fps and HD (1920x1080) images at 60fps. To acheive this, the model needs to be trained on large volumes of images with high-quality alpha mattes to generalize. To this end, the paper introduces two datasets with high-resolution alpha mattes and foreground layers extracted with chroma-key software. The model is then trained on these datases to learn strong priors, then fine-tunned on public dataset to learn fine-grained details. As for the network design, the model contains two processing paths; a base network that predicts the alpha matte and foreground layer at lower resolution, along with an error prediction map which specifies areas that may need high-resolution refinement. Based on this maps, a refinement network then takes the low-resolution result and the original image to generate high-resolution output, but only at select regions for efficency.

#### Im2Vec: Synthesizing Vector Graphics without Vector Supervision

Despite the large amount of generative methods for images, there a limited amount of approaches that operate directly on 
vector graphics and requires direct supervison. To solve this, the paper proposes Im2Vec,  
a new neural network that can generate complex vector graphics with varying topologies, and only requires indirect supervision from readily-available training imges with no vector counterparts.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/43.png' | absolute_url }}">
</figure>

Im2Vec consist of a standard encode-decoder architecture. Given an input image, the encoder maps it into a latent variable, which is then decoded into a vector graphic structure. The decoder on the other hand is designed so that it can generate complex graphics (see section 3 of the paper for more detials.)

#### Other papers to check out

- [Passive Inter-Photon Imaging](https://arxiv.org/abs/2104.00059)
- [Event-Based Synthetic Aperture Imaging With a Hybrid Network](https://arxiv.org/abs/2103.02376)
- [GAN Prior Embedded Network for Blind Face Restoration in the Wild](https://arxiv.org/abs/2105.06070)
- [Mask Guided Matting via Progressive Refinement Network](https://arxiv.org/abs/2012.06722)

# Other

#### Biometrics, Face, Gesture and Body Pose 

- [SMPLicit: Topology-Aware Generative Model for Clothed People](https://arxiv.org/abs/2103.06871)
- [On Self-Contact and Human Pose](https://arxiv.org/abs/2104.03176)
- [Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics](https://arxiv.org/abs/2007.12287)
- [PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2105.02465)
- [OSTeC: One-Shot Texture Completion](https://arxiv.org/abs/2012.15370)
- [SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks](https://arxiv.org/abs/2104.03313)
- [HOTR: End-to-End Human-Object Interaction Detection with Transformers](https://arxiv.org/abs/2104.13682)
- [Birds of a Feather: Capturing Avian Shape Models From Images](https://arxiv.org/abs/2105.09396)

#### Vision & Language

- [Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling](https://arxiv.org/abs/2102.06183)
- [Multimodal Contrastive Training for Visual Representation Learning](https://arxiv.org/abs/2104.12836)
- [ArtEmis: Affective Language for Visual Art](https://arxiv.org/abs/2101.07396)
- [VirTex: Learning Visual Representations From Textual Annotations](https://arxiv.org/abs/2006.06666)
- [Learning by Planning: Language-Guided Global Image Editing](https://arxiv.org/abs/2106.13156)

#### Datasets

- [Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts](https://arxiv.org/abs/2102.08981)
- [Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings](https://arxiv.org/abs/2011.11015)
- [Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets](https://arxiv.org/abs/2104.12690)
- [SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data](https://arxiv.org/abs/2105.08612)

#### Explainable AI & Privacy

- [Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings](https://arxiv.org/abs/2006.06634)
- [Transformer Interpretability Beyond Attention Visualization](https://arxiv.org/abs/2012.09838)
- [Black-box Explanation of Object Detectors via Saliency Maps](https://arxiv.org/abs/2006.03204)

#### Video Analysis and Understanding

- [Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps](https://arxiv.org/abs/2104.10386)
- [Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion](https://arxiv.org/abs/2103.07941)
- [Omnimatte: Associating Objects and Their Effects in Video ](https://arxiv.org/abs/2105.06993)
- [SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation](https://arxiv.org/abs/2101.08833)










