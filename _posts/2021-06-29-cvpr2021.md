---
title: "CVPR 2021: An Overview"
excerpt: "The 2021 CVPR conference concluded last week, with 1660 papers, 30 tutorials, 83 workshops. In this
blog post, we'll take a quick look into the emerging trends in the computer vision by going through some a small
portion of the accepted papers."
date: 2021-06-29 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2021 CVPR conference, one of the main computer vision and machine learning conference, concluded its second 100% virtual version
last week with a record of papers presented at the main conference. Of about 7500 submissions, 5900 made it to the decision making process
and 1660 papers (vs 1467 papers last year) were accepted with an acceptation rate of 23.7% (vs 22.1% last year).
Such a huge (and growing) number of papers can be a bit overwhelming, so to get a feel of the general trends of the conference this year, I will present in this blog post a quick look of the conference by summarizing some papers (& listing some) that seemed interesting to me.

*Note: This post is not an objective representation of the papers and subjects presented in CVPR 2021, it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

First, let's with some useful links:

- Papers: [CVPR2021 open access](https://openaccess.thecvf.com/CVPR2021?day=all)
- Workshops: [CVPR2021 workshops](http://cvpr2021.thecvf.com/workshops-schedule)
- Tutorials: [CVPR2021 tutorials](http://cvpr2021.thecvf.com/program)
- Presentations: [Crossminds](https://crossminds.ai/search/?keyword=CVPR%202021&filter=)
- Papers search interface: [blog.kitware.com](https://blog.kitware.com/demos/cvpr-2021-papers/?filter=authors&search=) & [public.tableau.com](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no)
- Awards: [CVPR2021 paper awards](http://cvpr2021.thecvf.com/node/329)
- Papers digest: [CVPR2021 Paper Digest](https://www.paperdigest.org/2021/06/cvpr-2021-highlights/)
- Papers & code: [CVPR2021 paper & code](https://github.com/amusi/CVPR2021-Papers-with-Code)

# CVPR 2021 in numbers
A portion of the statistics presented in this section are taken from [this](https://github.com/hoya012/CVPR-2021-Paper-Statistics) github repo & this [public tableau gallery](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no).

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/acceptance_rate.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a 20% increase in authors and a 29% increase in submitted papers, joined by rising the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/authors_by_country.png' | absolute_url }}">
</figure>

Similar to the last two years, China is the first contributor to CVPR in terms of accepted papers, followed by the USA, Korea, UK and Germany.


<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/subjects.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, the topic of the year is 3D computer vision with more than 200 papers focusing on this subject alone, followed by deep & representation
learning, image synthesis, and computation photography. There is also a notable increase in papers related to explainable AI and medical & biological imaging.

# Recognition, Detection & Tracking

#### Task Programming: Learning Data Efficient Behavior Representations ([paper](https://arxiv.org/abs/2011.13917))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/1.png' | absolute_url }}">
</figure>

In behavioral analysis, the location and pose of agents is first extracted from each frame of a behavior video, and then the labels are generated for a given behaviors of interest on a frame-by-frame basis based on the pose and movements of the agents as depicted in the figure above.
However, to predict the behaviors frame-by-frame in the second step, we need to train behavior detection models which are data intensive and require specialized domain knowledge and high-frequency temporal annotations. This paper studies two alternative ways to better use domain experts instead of a simple increase in the number of annotations: (1) self-supervised learning and (2) creating task programs by domain experts which are engineered decoder tasks for representation learning. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/2.png' | absolute_url }}">
</figure>

Trajectory Embedding for Behavior Analysis (TREBA) uses these two ideas to learn task-relevant low-dimensional representations of pose trajectories
by training a network to jointly reconstruct the input trajectory and predict the expert-programmed decoder tasks (see figure above). For the first task, given a set
of unlabelled trajectories, where each trajectory is a sequence of states (eg, location or pose of the agents), the history of the agent stages is
encoded using an RNN encoder, and the RNN the decoder (ie, a [trajectory variational autoencoder or TVAE](https://arxiv.org/abs/1806.02813)) then predicts the next states.
As for the second task, we first need to create decoder tasks for trajectory self-supervised learning by domain experts (which is called the process of Task Programing).
First, the experts find the attributes from the trajectory data that are useful to detect the agents behaviors of interest, then write a program
to compute these attributes (eg, distance or angle between two interacting mice) based on the trajectory data using systems like [MARS](https://www.biorxiv.org/content/10.1101/2020.07.26.222299v1) or [SimBA](https://www.biorxiv.org/content/10.1101/2020.04.19.049452v2). These programs are finally used to generate training
data for self-supervised multi-task learning.

#### Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks ([paper](https://arxiv.org/abs/2010.15703))

One of the main challenges of deploying deep nets on mobile and low-power computational  platforms for large scale usage is their large memory and computational requirements.
Luckily, such deep netw are often overparameterized living some room for compression to reduce their memory and computational demands without a minimal accuracy hit.
One way to compress deep nets is scalar quantization, compressing each parameter individually, but the compression rates are still limited by the number of paramters.
Another approach is vector  quantization which compresses multiple parameters into a single code thus exploiting redundancies among groups of network parameters, but
finding the parameters to group can be challenging (eg, in fully connected layers where there is no notion of spatial dimensions).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/3.png' | absolute_url }}">
</figure>

The paper proposes Permute, Quantize, and Fine-tune (PQF), a method that (1) searches for permutations of the network weights that yield functionally equivalent, yet an easier-to-quantize network, (2) quantizes the permuted network, and finally (3) fine tunes the permuted-and-quantized network to recover the accuracy of the original uncompressed network. Since the second step consists splitting each weight matrix $$W$$ into subvectors (say $$d$$ elements per subvector) which are then compressed individually, where each subvector is approximated by a smaller code or centroid, the objective of the first step is to find a permutation $$P$$ of the weight matrix such that the construced subvectors are easier to quantize into codes (see figure above). The optimization of $$P$$ is done by minimizing the determinant of the covariance of the resulting subvectors (see the paper for why they do this). Finally, a fine tuning of the compressed network is conducted to remove the accumulated errors in the activations which degrade the performances. 

#### Towards Open World Object Detection ([paper](https://arxiv.org/abs/2103.02603))

In this work, the authors propose  a novel computer vision problem called *Open World Object Detection*, where a model is trained to:  1) identify objects that have not been introduced to it as ‘unknown’ without explicit supervision to do so (ie, open set learning), and 2) incrementally learn these identified unknown categories without forgetting previously learned classes, when the corresponding labels are progressively received (ie, incremental and continual learning). In [open set](https://ieeexplore.ieee.org/document/6365193) recognition, the objective is to identify the  new instances not seen during training as unknowns, while in [open world](https://arxiv.org/abs/1412.5687) recognition extends this framework by requiring the classifier recognize the newly identified unknown classes.
However, adapting the open set and open world methods from recognition to detection is not trivial since the object detector is explicitly trained to detect the unknown classes as background, making the task of detecting unknown classes harder.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/4.png' | absolute_url }}">
</figure>

To solve this problem, the paper proposes ORE, Open World Object Detector, that learns a clear discrimination between classes in the latent space. This way, (1) the task of detecting an unknown instance as a novelty can be reduced to comparing its representation with the representations of the known instances (ie, open set detection), and also (2), it facilitates learning  feature  representations  for  the new class instances without overlapping with the previous classes (ie, incremental and continual learning, thus extending open set to open world detection). To have such a learning behavior, a contrastive clustering objective is introduced in order to force instances of same class to remain close-by, while instances of dissimilar classes would be pushed far apart. This is done by minimizing the distances between the class prototypes and the class representations, where the instances of class unknown are instances with high objectness score, but do not overlap with a ground-truth objects.
The classification head of the trained detector is then transformed into an [energy function](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf) to label an instance as unknown or not.

#### Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling ([paper](https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.html))

For a standard vision task, it is a common practice to adopt the ground-truth labels obtained via either the majority-vote or by simply one annotation from a preferred rater
as the single our of the training data. However, in medical images, the typical practice consists of collecting multiplean notations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. In this case, using standard training procedure of other vision tasks will overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations availble in medical image analysis.
To take this into consideration, the paper proposes MR-Net to explicitly model the multi-rater (dis-)agreement. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/5.png' | absolute_url }}">
</figure>

MRNet contains a coarse to fine two-stage processing pipeline (figure above, right). The first stage consists of a U-Net encoder with a ResNet34 pretrained on ImageNet. Together with an Expertise-aware Inferring Module (EIM) is inserted at the bottleneck layer to embed the expertise information of individual raters, named expertness vector, into the extracted high-level semantic features of the network, which are then passed to the U-Net decoder give a coarse prediction.
The second stage then refines the coarse prediction using two modules. Multi-rater Reconstruction Module (MRM) that reconstructs the raw multi-rater's grading, the reconstruction are then used to estimate the pixel-wise uncertainty map that represents the inter-observer variability across different regions. Finally,
a Multi-rater Perception Module (MPM) with a soft attention mechanism to utilizes the produced uncertainty map to refine the coarse prediction  of the first stage
and predict the final fine segmentation maps (see section 3 of the paper for details about each component).

#### Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized ([paper](https://arxiv.org/abs/2101.05022))

#### You Only Look One-Level Feature ([paper](https://arxiv.org/abs/2103.09460))

#### Other papers to check out
- [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483).
- [Line Segment Detection Using Transformers Without Edges](https://arxiv.org/abs/2101.01909).
- [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511).
- [Multimodal Motion Prediction With Stacked Transformer](https://arxiv.org/abs/2103.11624)s.
- [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595).



# 3D Vision & Robotics

Scan2Cap: Context-aware Dense Captioning in RGB-D Scans 

Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction 

Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts 

Learning Delaunay Surface Elements for Mesh Reconstruction

A Deep Emulator for Secondary Motion of 3D Characters

NeuTex: Neural Texture Mapping for Volumetric Neural Rendering

Learning to recover 3D Scene Shape from a single image

Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos

Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos

NeX: Real-Time View Synthesis With Neural Basis Expansion

Multi-Modal Fusion Transformer for End-to-End Autonomous Driving

Learned Initializations for Optimizing Coordinate-Based Neural Representations

Point2Skeleton: Learning Skeletal Representations from Point Clouds

Back to the Feature: Learning Robust Camera Localization From Pixels To Pose

Wide-Baseline Relative Camera Pose Estimation With Directional Learning

Neural Lumigraph Rendering

Pulsar: Efficient Sphere-Based Neural Rendering

SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data

Holistic 3D Scene Understanding From a Single Image With Implicit Representation

D-NeRF: Neural Radiance Fields for Dynamic Scenes

Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes

Deep Multi-Task Learning for Joint Localization, Perception, and Prediction

IBRNet: Learning Multi-View Image-Based Rendering

KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control

DeRF: Decomposed Radiance Fields

pixelNeRF: Neural Radiance Fields From One or Few Images

Shelf-Supervised Mesh Prediction in the Wild

Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans

AutoInt: Automatic Integration for Fast Neural Volume Rendering

LoFTR: Detector-Free Local Feature Matching With Transformers

Shape and Material Capture at Home

NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video

SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans

MP3: A Unified Model to Map, Perceive, Predict and Plan

Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty

NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis

Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images

MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments From a Single Moving Camera




# Image and Video Synthesis

Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction 

Rethinking and Improving the Robustness of Image Style Transfer

Ensembling with Deep Generative Views

SSN: Soft Shadow Network for Image Compositing

Spatially-Adaptive Pixelwise Networks for Fast Image Translation

Learning Continuous Image Representation With Local Implicit Image Function

Motion Representations for Articulated Animation

GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields

Closed-Form Factorization of Latent Semantics in GANs

Stylized Neural Painting

DriveGAN: Towards a Controllable High-Quality Neural Simulation

Image Generators With Conditionally-Independent Pixel Synthesis

Cross-Modal Contrastive Learning for Text-to-Image Generation 

Dual Contradistinctive Generative Autoencoder

Repopulating Street Scenes

NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections

Space-Time Neural Irradiance Fields for Free-Viewpoint Video

SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation

Positional Encoding As Spatial Inductive Bias in GANs

Regularizing Generative Adversarial Networks Under Limited Data

Variational Transformer Networks for Layout Generation 

Deep Animation Video Interpolation in the Wild

Stable View Synthesis

Navigating the GAN Parameter Space for Semantic Image Editing

Skip-Convolutions for Efficient Video Processing

Stochastic Image-to-Video Synthesis Using cINNs

Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing

Playable Video Generation

Plan2Scene: Converting Floorplans to 3D Scenes

SceneGen: Learning to Generate Realistic Traffic Scenes

GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving

OCONet: Image Extrapolation by Object Completion

Anycost GANs for Interactive Image Synthesis and Editing

StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation

Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation

Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes

Animating Pictures With Eulerian Motion Fields

Taming Transformers for High-Resolution Image Synthesis

One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing


# Scene Analysis & Understanding

Exemplar-Based Open-Set Panoptic Segmentation Network

Binary TTC: A Temporal Geofence for Autonomous Navigation

Learning to Recover 3D Scene Shape from a Single Image

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Robust Consistent Video Depth Estimation

Scene Essence 

Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization

MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers 

Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Repurposing GANs for One-Shot Semantic Part Segmentation

Deep Occlusion-Aware Instance Segmentation With Overlapping BiLayers

VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation 

Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers

Information-Theoretic Segmentation by Inpainting Error Maximization

Single Image Depth Prediction With Wavelet Decomposition

Polygonal Building Extraction by Frame Field Learning

Mask Guided Matting via Progressive Refinement Network

# Representation & Adversarial Learning

DECOR-GAN: 3D Shape Detailization by Conditional Refinement

Exploring Simple Siamese Representation Learning 

Audio-Visual Instance Discrimination with Cross-Modal Agreement

Where and What? Examining Interpretable Disentangled Representations

CutPaste: Self-Supervised Learning for Anomaly Detection and Localization

Spatiotemporal Contrastive Video Representation Learning

Taskology: Utilizing Task Relations at Scale

UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers

Self-Supervised Geometric Perception

Adversarially Adaptive Normalization for Single Domain Generalization

MOS: Towards Scaling Out-of-Distribution Detection for Large Semantic Space

Generative Hierarchical Features From Synthesizing Images

Natural Adversarial Examples

Training Generative Adversarial Networks in One Stage

Fast End-to-End Learning on Protein Surfaces

# Model Architectures, Optimization & Learning

Pre-Trained Image Processing Transformer

Metadata Normalization

Decoupled Dynamic Filter Networks

On Feature Normalization and Data Augmentation

Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation

Scaling Local Self-Attention for Parameter Efficient Visual Backbones

Bottleneck Transformers for Visual Recognition

MIST: Multiple Instance Spatial Transformer

RepVGG: Making VGG-Style ConvNets Great Again

Involution: Inverting the Inherence of Convolution for Visual Recognition

# Transfer, Low-shot, Semi & Unsupervised Learning

DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort

Learning Graph Embeddings for Compositional Zero-Shot Learning

MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking

Meta Pseudo Labels

Adaptive Prototype Learning and Allocation for Few-Shot Segmentation

Ranking Neural Checkpoints

Student-Teacher Learning From Clean Inputs to Noisy Inputs

# Biometrics, Face, Gesture and Body Pose 

SMPLicit: Topology-Aware Generative Model for Clothed People

On Self-Contact and Human Pose

Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics

PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation

OSTeC: One-Shot Texture Completion

SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks

HOTR: End-to-End Human-Object Interaction Detection with Transformers

Birds of a Feather: Capturing Avian Shape Models From Images

# Computational Photography

Event-Based Synthetic Aperture Imaging With a Hybrid Network

GAN Prior Embedded Network for Blind Face Restoration in the Wild

Passive Inter-Photon Imaging

Real-Time High-Resolution Background Matting 

Im2Vec: Synthesizing Vector Graphics without Vector Supervision


# Vision & Language

Multimodal Contrastive Training for Visual Representation Learning

ArtEmis: Affective Language for Visual Art

VirTex: Learning Visual Representations From Textual Annotations

Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling

Learning by Planning: Language-Guided Global Image Editing

# Datasets

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings

Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets

# Explainable AI & Privacy

Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings

Transformer Interpretability Beyond Attention Visualization

Black-box Explanation of Object Detectors via Saliency Maps

# Video Analysis and Understanding

Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps

Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion

Omnimatte: Associating Objects and Their Effects in Video 

















































#### Other papers:
- [NAME](LINK)


# The rest
This post turned into a long one very quickly, so in order to avoid ending-up with a 1h long reading session, I will simply list some papers I came across in case the the reader is interested in the subjects.

<details>
  <summary>Click to expand</summary> <br>

<small> <div class="tip" markdown="1">

**Subject**:
- [Name](LINK)

</div> </small>
</details>




