---
title: "CVPR 2021: An Overview"
excerpt: "The 2021 CVPR conference concluded last week, with 1660 papers, 30 tutorials, 83 workshops. In this
blog post, we'll take a quick look into the emerging trends in the computer vision by going through some a small
portion of the accepted papers."
date: 2021-06-29 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - conference
---

The 2021 CVPR conference, one of the main computer vision and machine learning conference, concluded its second 100% virtual version
last week with a record of papers presented at the main conference. Of about 7500 submissions, 5900 made it to the decision making process
and 1660 papers (vs 1467 papers last year) were accepted with an acceptation rate of 23.7% (vs 22.1% last year).
Such a huge (and growing) number of papers can be a bit overwhelming, so to get a feel of the general trends of the conference this year, I will present in this blog post a quick look of the conference by summarizing some papers (& listing some) that seemed interesting to me.

*Note: This post is not an objective representation of the papers and subjects presented in CVPR 2021, it is just a personnel overview of what I found interesting. Any feedback is welcomed!*

First, let's start with some useful links:

- Papers: [CVPR2021 open access](https://openaccess.thecvf.com/CVPR2021?day=all)
- Workshops: [CVPR2021 workshops](http://cvpr2021.thecvf.com/workshops-schedule)
- Tutorials: [CVPR2021 tutorials](http://cvpr2021.thecvf.com/program)
- Presentations: [Crossminds](https://crossminds.ai/search/?keyword=CVPR%202021&filter=)
- Papers search interface: [blog.kitware.com](https://blog.kitware.com/demos/cvpr-2021-papers/?filter=authors&search=) & [public.tableau.com](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no)
- Awards: [CVPR2021 paper awards](http://cvpr2021.thecvf.com/node/329)
- Papers digest: [CVPR2021 Paper Digest](https://www.paperdigest.org/2021/06/cvpr-2021-highlights/)
- Papers & code: [CVPR2021 paper & code](https://github.com/amusi/CVPR2021-Papers-with-Code)

# CVPR 2021 in numbers
A portion of the statistics presented in this section are taken from [this](https://github.com/hoya012/CVPR-2021-Paper-Statistics) github repo & this [public tableau gallery](https://public.tableau.com/views/CVPR2021/Dashboard1?:language=en-US&:display_count=n&:origin=viz_share_link:showVizHome=no).

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/acceptance_rate.png' | absolute_url }}">
</figure>

The trends of earlier years continued with a 20% increase in authors and a 29% increase in submitted papers, joined by rising the number of reviewers and area chairs to accommodate this expansion.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/authors_by_country.png' | absolute_url }}">
</figure>

Similar to the last two years, China is the first contributor to CVPR in terms of accepted papers, followed by the USA, Korea, UK and Germany.


<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/subjects.png' | absolute_url }}">
</figure>

As expected, the majority of the accepted papers focus on topics related to learning, recognition, detection, and understanding. However, the topic of the year is 3D computer vision with more than 200 papers focusing on this subject alone, followed by deep & representation
learning, image synthesis, and computation photography. There is also a notable increase in papers related to explainable AI and medical & biological imaging.

# Recognition, Detection & Tracking

#### Task Programming: Learning Data Efficient Behavior Representations ([paper](https://arxiv.org/abs/2011.13917))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/1.png' | absolute_url }}">
</figure>

In behavioral analysis, the location and pose of agents is first extracted from each frame of a behavior video, and then the labels are generated for a given behaviors of interest on a frame-by-frame basis based on the pose and movements of the agents as depicted in the figure above.
However, to predict the behaviors frame-by-frame in the second step, we need to train behavior detection models which are data intensive and require specialized domain knowledge and high-frequency temporal annotations. This paper studies two alternative ways to better use domain experts instead of a simple increase in the number of annotations: (1) self-supervised learning and (2) creating task programs by domain experts which are engineered decoder tasks for representation learning. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/2.png' | absolute_url }}">
</figure>

Trajectory Embedding for Behavior Analysis (TREBA) uses these two ideas to learn task-relevant low-dimensional representations of pose trajectories
by training a network to jointly reconstruct the input trajectory and predict the expert-programmed decoder tasks (see figure above). For the first task, given a set
of unlabelled trajectories, where each trajectory is a sequence of states (eg, location or pose of the agents), the history of the agent stages is
encoded using an RNN encoder, and the RNN the decoder (ie, a [trajectory variational autoencoder or TVAE](https://arxiv.org/abs/1806.02813)) then predicts the next states.
As for the second task, we first need to create decoder tasks for trajectory self-supervised learning by domain experts (which is called the process of Task Programing).
First, the experts find the attributes from the trajectory data that are useful to detect the agents behaviors of interest, then write a program
to compute these attributes (eg, distance or angle between two interacting mice) based on the trajectory data using systems like [MARS](https://www.biorxiv.org/content/10.1101/2020.07.26.222299v1) or [SimBA](https://www.biorxiv.org/content/10.1101/2020.04.19.049452v2). These programs are finally used to generate training
data for self-supervised multi-task learning.

#### Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks ([paper](https://arxiv.org/abs/2010.15703))

One of the main challenges of deploying deep nets on mobile and low-power computational  platforms for large scale usage is their large memory and computational requirements.
Luckily, such deep netw are often overparameterized living some room for compression to reduce their memory and computational demands without a minimal accuracy hit.
One way to compress deep nets is scalar quantization, compressing each parameter individually, but the compression rates are still limited by the number of paramters.
Another approach is vector  quantization which compresses multiple parameters into a single code thus exploiting redundancies among groups of network parameters, but
finding the parameters to group can be challenging (eg, in fully connected layers where there is no notion of spatial dimensions).

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/3.png' | absolute_url }}">
</figure>

The paper proposes Permute, Quantize, and Fine-tune (PQF), a method that (1) searches for permutations of the network weights that yield functionally equivalent, yet an easier-to-quantize network, (2) quantizes the permuted network, and finally (3) fine tunes the permuted-and-quantized network to recover the accuracy of the original uncompressed network. Since the second step consists splitting each weight matrix $$W$$ into subvectors (say $$d$$ elements per subvector) which are then compressed individually, where each subvector is approximated by a smaller code or centroid, the objective of the first step is to find a permutation $$P$$ of the weight matrix such that the construced subvectors are easier to quantize into codes (see figure above). The optimization of $$P$$ is done by minimizing the determinant of the covariance of the resulting subvectors (see the paper for why they do this). Finally, a fine tuning of the compressed network is conducted to remove the accumulated errors in the activations which degrade the performances. 

#### Towards Open World Object Detection ([paper](https://arxiv.org/abs/2103.02603))

In this work, the authors propose  a novel computer vision problem called *Open World Object Detection*, where a model is trained to:  1) identify objects that have not been introduced to it as ‘unknown’ without explicit supervision to do so (ie, open set learning), and 2) incrementally learn these identified unknown categories without forgetting previously learned classes, when the corresponding labels are progressively received (ie, incremental and continual learning). In [open set](https://ieeexplore.ieee.org/document/6365193) recognition, the objective is to identify the  new instances not seen during training as unknowns, while in [open world](https://arxiv.org/abs/1412.5687) recognition extends this framework by requiring the classifier recognize the newly identified unknown classes.
However, adapting the open set and open world methods from recognition to detection is not trivial since the object detector is explicitly trained to detect the unknown classes as background, making the task of detecting unknown classes harder.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/4.png' | absolute_url }}">
</figure>

To solve this problem, the paper proposes ORE, Open World Object Detector, that learns a clear discrimination between classes in the latent space. This way, (1) the task of detecting an unknown instance as a novelty can be reduced to comparing its representation with the representations of the known instances (ie, open set detection), and also (2), it facilitates learning  feature  representations  for  the new class instances without overlapping with the previous classes (ie, incremental and continual learning, thus extending open set to open world detection). To have such a learning behavior, a contrastive clustering objective is introduced in order to force instances of same class to remain close-by, while instances of dissimilar classes would be pushed far apart. This is done by minimizing the distances between the class prototypes and the class representations, where the instances of class unknown are instances with high objectness score, but do not overlap with a ground-truth objects.
The classification head of the trained detector is then transformed into an [energy function](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf) to label an instance as unknown or not.

#### Learning Calibrated Medical Image Segmentation via Multi-Rater Agreement Modeling ([paper](https://openaccess.thecvf.com/content/CVPR2021/html/Ji_Learning_Calibrated_Medical_Image_Segmentation_via_Multi-Rater_Agreement_Modeling_CVPR_2021_paper.html))

For a standard vision task, it is a common practice to adopt the ground-truth labels obtained via either the majority-vote or by simply one annotation from a preferred rater
as the single our of the training data. However, in medical images, the typical practice consists of collecting multiplean notations, each from a different clinical expert or rater, in the expectation that possible diagnostic errors could be mitigated. In this case, using standard training procedure of other vision tasks will overlook the rich information of agreement or disagreement ingrained in the raw multi-rater annotations availble in medical image analysis.
To take this into consideration, the paper proposes MR-Net to explicitly model the multi-rater (dis-)agreement. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/5.png' | absolute_url }}">
</figure>

MRNet contains a coarse to fine two-stage processing pipeline (figure above, right). The first stage consists of a U-Net encoder with a ResNet34 pretrained on ImageNet. Together with an Expertise-aware Inferring Module (EIM) is inserted at the bottleneck layer to embed the expertise information of individual raters, named expertness vector, into the extracted high-level semantic features of the network, which are then passed to the U-Net decoder give a coarse prediction.
The second stage then refines the coarse prediction using two modules. Multi-rater Reconstruction Module (MRM) that reconstructs the raw multi-rater's grading, the reconstruction are then used to estimate the pixel-wise uncertainty map that represents the inter-observer variability across different regions. Finally,
a Multi-rater Perception Module (MPM) with a soft attention mechanism to utilizes the produced uncertainty map to refine the coarse prediction  of the first stage
and predict the final fine segmentation maps (see section 3 of the paper for details about each component).

#### Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized ([paper](https://arxiv.org/abs/2101.05022))

One of the flaws of ImageNet is the presence of a significant level of label noise, where many instances contain multiple classes while having  a single-label ground-truth. This mismatch between the single label annotations of ImageNet and the multi-label nature of its images becomes even more problematic with random-crop training, where the input may contain an entirely different class than the ground-truth.
To aviate this problem, the paper proposes to generate the multi-labels using a strong image classifier trained on an extra source of data,
in addition to leveraging pixel-wise multi-label predictions before the final pooling layer as a complementary location-specific supervision signal.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/6.png' | absolute_url }}">
</figure>

Re-labeling ImageNet consists of using a machine annotator, which is a classifier trained on a super-ImageNet scale datasets (like [JFT-300M](https://arxiv.org/abs/1707.02968) and [InstagramNet-1B](https://arxiv.org/abs/1805.00932)) and fine-tunned on ImageNet. While being train on single-label classification, such classifier still multi-label predictions for images with multiple categories, making them suitable for relabeling ImageNet. In addition to generating the multi-label classes, Re-labeling also the location-specific labels, which are spatial features before the global pooling layer weighted by the weight of the last fully-connected layers. The model is then trained on both the original ImageNet labels and the generated location-specific labels. For the second part, the paper proposes LabelPooling which conducts a regional pooling ([RoI Align](https://arxiv.org/abs/1703.06870)) operation on the label map corresponding to the coordinates of the random crop
(see figure above, right), which are passed to the machine annotator to generate the multi-class labels used for the multi-label los.

#### Other papers to check out
- [You Only Look One-Level Feature](https://arxiv.org/abs/2103.09460).
- [Benchmarking Representation Learning for Natural World Image Collections](https://arxiv.org/abs/2103.16483).
- [Line Segment Detection Using Transformers Without Edges](https://arxiv.org/abs/2101.01909).
- [MoViNets: Mobile Video Networks for Efficient Video Recognition](https://arxiv.org/abs/2103.11511).
- [Multimodal Motion Prediction With Stacked Transformer](https://arxiv.org/abs/2103.11624)s.
- [SiamMOT: Siamese Multi-Object Tracking](https://arxiv.org/abs/2105.11595).

# 3D Computer Vision

#### MP3: A Unified Model to Map, Perceive, Predict and Plan ([paper](https://arxiv.org/abs/2101.06806))

Most modern self-driving stacks require up-to-date high-definition maps that contain rich semantic information necessary for driving such as the topology and location of the lanes, crosswalks, traffic lights, intersections as well as the traffic rules for each lane. While such maps greatly facilitate the perception and motion forecasting tasks, as the online inference process has to mainly focus on dynamic objects (eg, vehicles, pedestrians, cyclists), scaling them is hard given their complexity and cost, and given that even very small errors in the mapping might result in fatal mistakes. This motivates the development of mapless technology, which can serve as the fail-safe in the case of localization failures or outdated maps, and potentially unlock self-driving at scale at a much lower cost.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/10.png' | absolute_url }}">
</figure>

However, with a mapless approach comes a number of challenges: (1) The sole source of training signal is the controls of an expert driver (such steering and acceleration) without any without providing intermediate interpretable representations that can help explain the self-driving vehicle decisions. (2) Without any mechanism to inject structure and prior knowledge, such an approach can very brittle to distributional shift such missing a lane. To address these issues, MP3, an end-to-end approach to mapless driving that is interpretable, does not incur any information loss, and reasons about uncertainty in the intermediate representations.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/11.png' | absolute_url }}">
</figure>

The MP3 model takes as input, a high-level goal, a history of LiDAR point clouds to extract rich geometric and semantic features from the scene over time (see [this](https://arxiv.org/abs/2012.12395) for more details), and odometry data to compensate for the vehicle's motion.
The inputs are then processed using a backbone network and fed into a set of probabilistic spatial layers to model the static and dynamic parts of the environment.
The static environment is represented by a planning-centric online map which captures information about which areas are drivable and which ones are reachable given traffic rules. The dynamic actors are captured in a novel occupancy flow that provides occupancy andvelocity estimates over time. The motion planning module then leverages these representations
to retrieve dynamically feasible trajectories, predicts a spatial mask over the map to estimate the route given an abstract goal, and leverages the online map and occupancy flow directly as cost functions for explainable, safe plans.

#### Multi-Modal Fusion Transformer for End-to-End Autonomous Driving ([paper](https://arxiv.org/abs/2104.09224))

In a standard driving situation such as the one depicted in the left image bellow, the vehicle must capture the global context of the scene involving the interaction between the traffic light (yellow) and the vehicles (red) for a safe navigation. To do this, the different modalities (ie, the LiDAR point cloud and the camera view) must be fused together in order to obtain such a global view. This raises the following questions: how to fuse such multi-modal representations? to what extent each modality should be processed independently before fusion? and how can such fusion be conduced?. The paper proposes a Modal Fusion Transformer (TransFuser), a transformer-based model designed to integrate both LiDAT and camera views with global attention, thus capturing the necessary 3D context for safe navigation.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/9.png' | absolute_url }}">
</figure>

The paper consider the task of point-to-point navigation in an urban setting, where the goal is to complete a given route while safely reacting to other dynamic agents and following traffic rules. The TransFuser is then train using L1 loss to predicted trajectories and the correct trajectories using a dataset  consisting of high-dimensional observations of the environment, and the corresponding expert trajectory.
TransFuser takes as input RGB image and [LiDAR BEV](https://arxiv.org/abs/1905.01296) representations and uses several transformer modules to fuse the intermediate feature maps between both modalities. The fusion is applied at multiple resolutions and throughout the feature extractor resulting in a 512-dimensional feature vector output from both the image and LiDAR BEV stream, which is the desired compact representation of the environment that encodes the global context of the 3D scence. This compact representation can then be used as input to the auto-regressive (a GRU) prediction network that outputs the trajectories in the form of waypoints in vehicle's coordinate frame. 


#### Neural Lumigraph Rendering ([paper](https://arxiv.org/abs/2103.11571))

The recent [neural rendering](https://arxiv.org/abs/2004.03805) techniques are capable of generating photorealistic image quality for novel view synthesis and 3D shape estimation from 2D images, however, they are either slow to train and/or require a considerable rendering time time for high image resolutions. [NeRF](https://arxiv.org/abs/2003.08934) for instance does not offer real-time rendering due to the use of neural scene representation and [volume rendering](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.3394).
To overcome this, the paper proposes the use SDF-based sinusoidal representation network ([SIREN](https://arxiv.org/abs/2006.09661)) to implicitly model the surface of objects (ie, implicitly defining an object or a scene using a neural network and training directly with 3D data), which can be extracted using the [marching cubes algorithm](https://dl.acm.org/doi/10.1145/37401.37422) and exported into traditional mesh-based representations for real-time rendering.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/7.png' | absolute_url }}">
</figure>

However, given the high capacity of SIRENs, they are prone to overfitting, making them incapable of rendering new views that are interpolation of views encountered during training. To solve this, the authors propose a novel smoothness loss function that maintains SIREN's high-capacity encoding for the supervised images while constraining it in the angular domain to prevent overfitting on these views. SIREN based network can then be trained using a sparse set of multi-view 2D images while providing while providing a high-quality 3D surface that can be directly exported for real-time rendering algorithms at test time.

#### NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis ([paper](https://arxiv.org/abs/2012.03927))

NeRV extends [NeRF](https://arxiv.org/abs/2003.08934) to arbitrary lighting conditions, taking as input a set of images of a scene illuminated by unconstrained known lighting, and producing a 3D representation that can be rendered from novel viewpoints under novel and unobserved lighting conditions.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/8.png' | absolute_url }}">
</figure>

Given that NeRF models just the amount of outgoing light from a location, the fact that this out going light is itself the result of interactions between incoming light and the material properties of an underlying surface is ignored, so rendering viewpoints
under novel lighting conditions is not possible. A naive way (figure above, right) to solve this is to query NeRF's MLP for the volume density at samples along the camera ray to determine the amount of light reflected at each location that reaches the camera, and then for each location, 
query the MLP for the volume density at points between the location and every light source. This procedure is clearly too computationally infeasible. The paper solve this by proposing NeRV, a method to train NeRF-like model that can simulate realistic environment lighting and global illumination by using an additional MLP as a lookup table into a visibility field during rendering. As a result, the training consists of jointly optimizing the visibility MLP (or reflectance MLP) for estimating the light surface visibility at a given 3D position along, alongside the NeRF's MLP (also called the shape MLP) for volumetric representation. At test time, the rendering is then conducted along the ray by querying the shape and reflectance MLPs for the volume densities, surface normals, and [BRDF](https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function) parameters at each point.

#### Neural Body: Implicit Neural Representations With Structured Latent Codes for Novel View Synthesis of Dynamic Humans ([paper](https://arxiv.org/abs/2012.15838))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/12.png' | absolute_url }}">
</figure>

Given a very sparse set of camera views (say 3 or 4 views as depicted above), learning implicit neural representations of 3D scenes achieves  becomes infeasible. To solve this, the paper proposes Neural Body, a method that leverages observations over video frames order to learn
a new human body representation that is consistent over the different frames and share the same set of latent codes anchored to a deformable mesh, so that the observations across frames can be naturally integrated.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/13.png' | absolute_url }}">
</figure>

First, a set of latent codes are defined and anchored to the vertices of the [SMLP](https://smpl.is.tue.mpg.de/) deformable human model so that the spatial location of each vertex varies with the human pose. Then, the 3D representation at a given frame is estimated from sparse camera views, and then applied to the code location based on the obtained 3D human pose. Finally, the network is trained designed to regress the density and color for any 3D point based on these latent codes. Both the latent codes and the network are jointly learned from images of all video frames during the reconstruction process.

#### pixelNeRF: Neural Radiance Fields From One or Few Images ([paper](https://arxiv.org/abs/2012.02190))

The [NeRF](https://arxiv.org/abs/2003.08934) framework consists of optimizing the representation to every scene independently, requiring many views per scene and significant compute time. pixelNeRF adapts NeRF in order to be trained across multiple scenes jointly to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). This is done by 
using spatial features of a given image produced by a CNN, which are aligned to each pixel as an input, and then passes through NeRF model for training. This simple image conditioning allows the model to be trained on a set of multi-view images, where it can learn scene priors to perform view synthesis.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/14.png' | absolute_url }}">
</figure>

pixelNeRF consists of two components: a fully-convolutional image encoder, which encodes the input image into a pixel-aligned feature grid, and a NeRF network which outputs color and density given a spatial location and its corresponding encoded feature. 
When multiple input views are available, each view is encoded  into the feature grid, the multiple features are processed in parallel and then aggregated into the final color and opacity.

#### Other papers to check out

- [Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction](https://arxiv.org/abs/2012.03065)
- [NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections](https://arxiv.org/abs/2008.02268)
- [D-NeRF: Neural Radiance Fields for Dynamic Scenes](https://arxiv.org/abs/2011.13961)
- [Learning to recover 3D Scene Shape from a single image](https://arxiv.org/abs/2012.09365)
- [NeuralRecon: Real-Time Coherent 3D Reconstruction From Monocular Video](https://arxiv.org/abs/2104.00681)
- [Point2Skeleton: Learning Skeletal Representations from Point Clouds](https://arxiv.org/abs/2012.00230)
- [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)
- [NeX: Real-Time View Synthesis With Neural Basis Expansion](https://arxiv.org/abs/2103.05606)
- [Holistic 3D Scene Understanding From a Single Image With Implicit Representation](https://arxiv.org/abs/2103.06422)
- [Scan2Cap: Context-aware Dense Captioning in RGB-D Scans](https://arxiv.org/abs/2012.02206)
- [Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction](https://arxiv.org/abs/2012.01451)
- [Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts](https://arxiv.org/abs/2012.09165)
- [Learning Delaunay Surface Elements for Mesh Reconstruction](https://arxiv.org/abs/2012.01203)
- [A Deep Emulator for Secondary Motion of 3D Characters](https://arxiv.org/abs/2103.01261)
- [NeuTex: Neural Texture Mapping for Volumetric Neural Rendering](https://arxiv.org/abs/2103.00762)
- [DeRF: Decomposed Radiance Fields](https://arxiv.org/abs/2011.12490)
- [IBRNet: Learning Multi-View Image-Based Rendering](https://arxiv.org/abs/2102.13090)
- [Learned Initializations for Optimizing Coordinate-Based Neural Representations](https://arxiv.org/abs/2012.02189)
- [AutoInt: Automatic Integration for Fast Neural Volume Rendering](https://arxiv.org/abs/2012.01714)
- [Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos](https://arxiv.org/abs/2103.03319)
- [Back to the Feature: Learning Robust Camera Localization From Pixels To Pose](https://arxiv.org/abs/2103.09213)
- [Wide-Baseline Relative Camera Pose Estimation With Directional Learning](https://arxiv.org/abs/2106.03336)
- [Pulsar: Efficient Sphere-Based Neural Rendering](https://arxiv.org/abs/2004.07484)
- [Neural Geometric Level of Detail: Real-Time Rendering With Implicit 3D Shapes](https://arxiv.org/abs/2101.10994)
- [Deep Multi-Task Learning for Joint Localization, Perception, and Prediction](https://arxiv.org/abs/2101.06720)
- [KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control](https://arxiv.org/abs/2104.11224)
- [Shelf-Supervised Mesh Prediction in the Wild](https://arxiv.org/abs/2102.06195)
- [LoFTR: Detector-Free Local Feature Matching With Transformers](https://arxiv.org/abs/2104.00680)
- [Shape and Material Capture at Home](https://arxiv.org/abs/2104.06397)
- [SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans](https://arxiv.org/abs/2006.14660)
- [Fusing the Old with the New: Learning Relative Camera Pose with Geometry-Guided Uncertainty](https://arxiv.org/abs/2104.08278)
- [Cuboids Revisited: Learning Robust 3D Shape Fitting to Single RGB Images](https://arxiv.org/abs/2105.02047)
- [MonoRec: Semi-Supervised Dense Reconstruction in Dynamic Environments From a Single Moving Camera](https://arxiv.org/abs/2011.11814)

# Image and Video Synthesis

#### GIRAFFE: Representing Scenes As Compositional Generative Neural Feature Fields ([paper](https://arxiv.org/abs/2011.12100))

While GaNs are capable of generating photorealistic and diverse high resolutions images, having a fine-grained control over the factors of variation in the data and the compositionality of the generated scenes is sill limited, operating only in 2D and ignoring the three-dimensional of the underlying scenes.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/15.png' | absolute_url }}">
</figure>

GIRAFFE proposes to represent scenes as compositional generative neural feature fields. During training, instead of having a single latent code as in the standard GaN setting, GIRAFFE randomly generated a set Shape and Appearance codes for each object (& background) in the scene, which are used to generated Feature Fields. Then a second set of latent codes are generated, this time representing the pose transformation, which are applied to the generated Feature Field to obtain the Posed Feature Fields. Finally, the final Posed Feature Fields are aggregated into a single scene representation, and given a camera pose, are used as input the neural rendering network to generate a 2D image. The generated and real 2D images are then passed to the discriminator to compute the adversarial loss. The whole model is trained end-to-end and at test time, the composition of the generated images can be controlled using the Shape & Appearance and Pose latent codes.

#### GeoSim: Realistic Video Simulation via Geometry-Aware Composition for Self-Driving ([paper](https://arxiv.org/abs/2011.12100))

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/16.png' | absolute_url }}">
</figure>

The ability to simulate/enhance various real world scenarios is an important yet challenging open problem, especially for safety-critical domains such as self-driving where producing visually appealing and realistic results requires a physics-based rendering, which very costly. As alternative, GeoSim exploits the recent advances in image synthesis by combining data-triven approaches (such as generative modeling) and computer graphics to insert dynamic objects into existing videos while maintaining high visual quality through physically grounded simulation (see the example above). 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/17.png' | absolute_url }}">
</figure>

The first step of GeoSim is to create a large 3D assets (vehicles of different types and shapes) with accurate pose, shape and texture. Instead of using artists to create these assets GeoSim leverages publically availble datasets to construct 3D assets of the objects. This is done using a learning-based, multi-view, multi-sensor reconstruction approach that leverages the 3D bounding boxes, and trained in a self-supervised manner so that there is an agreement between the predicted 3D shape and that of the camera and LiDAR observations.
GeoSim then exploits the 3D scene layout from high-definition maps and LiDAR data to add these learned 3D asserts in plausible locations and make them behave realistically by considering the full scene. Finally, using thi snew 3D scene, GeoSim performs image-based rendering to properly handle occlusions, and neural network-based image in-painting to ensure the inserted object seamlessly blendsin by filling holes, adjusting color inconsistencies due to lighting changes, and removing sharp boundaries.
Check out [the paper's website](https://tmux.top/publication/geosim/) for some results.

#### Taming Transformers for High-Resolution Image Synthesis ([paper](https://arxiv.org/abs/2012.09841))

The reticently introduced vision transformers (such as [ViT](https://arxiv.org/abs/2010.11929)) demonstrated that they can perform on par with
CNNs, and given enough training data, they tend to learn convolutional structures. This raises an obvious question, do we have to relearn such an inductive bias 
from scratch each time we train a vision model?. This paper proposes to merge both CNNs and transformers into a single framework for image synthesis. Thus leveraging the
efficiency of CNNs and their inductive image biases while still retaining the flexibility of transformers.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/18.png' | absolute_url }}">
</figure>

As depicted above, the proposes framework consists of CNN encoder-decoder network trained adversarially for [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937), and then a transformer that operates over the discrete representations in an autoregressive manner. More speficically, the training consist of two stages. First the encoder, decoder and discriminator, all CNN-based, are trained using a reconstruction loss, [a perceptual loss](https://arxiv.org/abs/1801.03924), a commitment loss (ie, used to trained the codebook, see [VQVAE](https://arxiv.org/abs/1711.00937) for more details), and an adversarial loss. At the end of training, we end-up with a learned codebook where each spatial location in the input image can be represented by index in the codebook. By using such a formulation, the input image can be views as a sequence of codebook indices, and such a sequence can be used to train the auto-regressive transformer in the second step of the training process. Starting from the top righ corner, at each time step, the transformer is tasked with predicting the next codebook index and in order to reduce the computation, the input to the transformer is restricted into a sliding window without a singnificant loss in performance. Finally, at test time, we can use the trained transformer to generate large sequences without any restrictions (and with any type of conditioning, see section 3.2 of the paper), which correspond to very large images, the predicted indices are then used to fetch the discrete representation from the codebook, which are then passed to the decoder to synthesis an image.

#### Rethinking and Improving the Robustness of Image Style Transfer([Paper](https://arxiv.org/abs/2104.05623))

The objective of image style transfer is to map the content of a given image into the style of a different one, and in such task, VGG network has demonstrated 
remarkable ability to capture the visual style of an image. However, when such a network is replaced with a more modern, and a better performing network such as
ResNet, stylization performance degrades significantly as shown in the figure bellow.

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/19.png' | absolute_url }}">
</figure>

In this paper, the authors investigate the root cause of this behavior, and find that residual connections, which represent the main architectural difference between VGG and ResNet, produce peaky feature maps of small entropy, which are not suitable for style transfer. To improve the robustness of the ResNet architecture, they authors propose a simple yet effective solution based on a softmax transformation of the feature activations that enhances their entropy. This method, dubbed Stylization With Activation smoothinG (SWAG), consists of adding a softmax-based smoothing transformation to all of the activations in order to push the model to produce smoother activations, thus reducing large peaks and increasing small values, creating a more uniform distribution.

#### Learning Continuous Image Representation With Local Implicit Image Function ([Paper](https://arxiv.org/abs/2012.09161))

This paper proposes Local Implicit Image Function (LIIF) for representing natural and complex images in a continuous manner. With LIIF, an image is represented as a set of latent codes distributed in spatial dimensions. Given a coordinate, the decoding function takes the coordinate in-formation and queries the local latent codes around it, and 
then predicts the RGB value at the given coordinate as an output. Since the LIIF representation is continuous, we can query an arbitrary high target resolution up to x30 higher than the training resolution encountered during training. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/CVPR21/20.png' | absolute_url }}">
</figure>

The proposed framework consists of an encoder that produces 2D feature maps given an input image, where the feature maps are evenly distributed in the 2D space of the continuous image domain, and each feature at a given spatial location is called a latent code. Then, the decoder takes as input a 2D coordinate in the image domain in addition to a weighted average of the 4 nearest latent code to the chosen 2D coordinate and outputs the RGB values. Now, in order to train both the encoder and the decoder jointly using self-supervision. This is done by taking a training image, randomly down-sampling it, the encoder then encoder the down-sampled image, while the decoder is queried to produced the RGB values of the original image, which is used as ground-truth.

#### Other papers to check out

- [Ensembling with Deep Generative Views](https://arxiv.org/abs/2104.14551)
- [SSN: Soft Shadow Network for Image Compositing](https://arxiv.org/abs/2007.08211)
- [Spatially-Adaptive Pixelwise Networks for Fast Image Translation](https://arxiv.org/abs/2012.02992)
- [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060)
- [Motion Representations for Articulated Animation](https://arxiv.org/abs/2104.11280)
- [Playable Video Generation](https://arxiv.org/abs/2101.12195)
- [Repopulating Street Scenes](https://arxiv.org/abs/2103.16183)
- [Animating Pictures With Eulerian Motion Fields](https://arxiv.org/abs/2011.15128)
- [Closed-Form Factorization of Latent Semantics in GANs](https://arxiv.org/abs/2007.06600)
- [Stylized Neural Painting](https://arxiv.org/abs/2011.08114)
- [Image Generators With Conditionally-Independent Pixel Synthesis](https://arxiv.org/abs/2011.13775)
- [Cross-Modal Contrastive Learning for Text-to-Image Generation ](https://arxiv.org/abs/2101.04702)
- [Dual Contradistinctive Generative Autoencoder](https://arxiv.org/abs/2011.10063)
- [Space-Time Neural Irradiance Fields for Free-Viewpoint Video](https://arxiv.org/abs/2011.12950)
- [Positional Encoding As Spatial Inductive Bias in GANs](https://arxiv.org/abs/2012.05217)
- [Regularizing Generative Adversarial Networks Under Limited Data](https://arxiv.org/abs/2104.03310)
- [Variational Transformer Networks for Layout Generation](https://arxiv.org/abs/2104.02416)
- [Deep Animation Video Interpolation in the Wild](https://arxiv.org/abs/2104.02495)
- [Stable View Synthesis](https://arxiv.org/abs/2011.07233)
- [Navigating the GAN Parameter Space for Semantic Image Editing](https://arxiv.org/abs/2011.13786)
- [Stochastic Image-to-Video Synthesis Using cINNs](https://arxiv.org/abs/2105.04551)
- [Exploiting Spatial Dimensions of Latent in GAN for Real-Time Image Editing](https://arxiv.org/abs/2104.14754)
- [Plan2Scene: Converting Floorplans to 3D Scenes](https://arxiv.org/abs/2106.05375)
- [SceneGen: Learning to Generate Realistic Traffic Scenes](https://arxiv.org/abs/2101.06541)
- [OCONet: Image Extrapolation by Object Completion](https://openaccess.thecvf.com/content/CVPR2021/html/Bowen_OCONet_Image_Extrapolation_by_Object_Completion_CVPR_2021_paper.html)
- [Anycost GANs for Interactive Image Synthesis and Editing](https://arxiv.org/abs/2103.03243)
- [StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation](https://arxiv.org/abs/2011.12799)
- [Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation](https://arxiv.org/abs/2008.00951)
- [Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes](https://arxiv.org/abs/2103.17185)
- [One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing](https://arxiv.org/abs/2011.15126)


# Scene Analysis & Understanding

Polygonal Building Extraction by Frame Field Learning

Exemplar-Based Open-Set Panoptic Segmentation Network

Binary TTC: A Temporal Geofence for Autonomous Navigation

Learning to Recover 3D Scene Shape from a Single Image

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Robust Consistent Video Depth Estimation

Scene Essence 

Semantic Segmentation With Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization

MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers 

Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging

The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth

Repurposing GANs for One-Shot Semantic Part Segmentation

Deep Occlusion-Aware Instance Segmentation With Overlapping BiLayers

VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation 

Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers

Information-Theoretic Segmentation by Inpainting Error Maximization

Single Image Depth Prediction With Wavelet Decomposition

Mask Guided Matting via Progressive Refinement Network



# Representation & Adversarial Learning

DECOR-GAN: 3D Shape Detailization by Conditional Refinement

Exploring Simple Siamese Representation Learning 

Audio-Visual Instance Discrimination with Cross-Modal Agreement

Where and What? Examining Interpretable Disentangled Representations

CutPaste: Self-Supervised Learning for Anomaly Detection and Localization

Spatiotemporal Contrastive Video Representation Learning

Taskology: Utilizing Task Relations at Scale

UP-DETR: Unsupervised Pre-Training for Object Detection With Transformers

Self-Supervised Geometric Perception

Adversarially Adaptive Normalization for Single Domain Generalization

MOS: Towards Scaling Out-of-Distribution Detection for Large Semantic Space

Generative Hierarchical Features From Synthesizing Images

Natural Adversarial Examples

Training Generative Adversarial Networks in One Stage

Fast End-to-End Learning on Protein Surfaces

# Model Architectures, Optimization & Learning

Pre-Trained Image Processing Transformer

Metadata Normalization

Decoupled Dynamic Filter Networks

On Feature Normalization and Data Augmentation

Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation

Scaling Local Self-Attention for Parameter Efficient Visual Backbones

Bottleneck Transformers for Visual Recognition

MIST: Multiple Instance Spatial Transformer

RepVGG: Making VGG-Style ConvNets Great Again

Involution: Inverting the Inherence of Convolution for Visual Recognition

Skip-Convolutions for Efficient Video Processing


# Transfer, Low-shot, Semi & Unsupervised Learning

DatasetGAN: Efficient Labeled Data Factory With Minimal Human Effort

Learning Graph Embeddings for Compositional Zero-Shot Learning

MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking

Meta Pseudo Labels

Adaptive Prototype Learning and Allocation for Few-Shot Segmentation

Ranking Neural Checkpoints

Student-Teacher Learning From Clean Inputs to Noisy Inputs

# Biometrics, Face, Gesture and Body Pose 

SMPLicit: Topology-Aware Generative Model for Clothed People

On Self-Contact and Human Pose

Body2Hands: Learning To Infer 3D Hands From Conversational Gesture Body Dynamics

PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation

OSTeC: One-Shot Texture Completion

SCANimate: Weakly Supervised Learning of Skinned Clothed Avatar Networks

HOTR: End-to-End Human-Object Interaction Detection with Transformers

Birds of a Feather: Capturing Avian Shape Models From Images

# Computational Photography

Event-Based Synthetic Aperture Imaging With a Hybrid Network

GAN Prior Embedded Network for Blind Face Restoration in the Wild

Passive Inter-Photon Imaging

Real-Time High-Resolution Background Matting 

Im2Vec: Synthesizing Vector Graphics without Vector Supervision


# Vision & Language

Multimodal Contrastive Training for Visual Representation Learning

ArtEmis: Affective Language for Visual Art

VirTex: Learning Visual Representations From Textual Annotations

Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling

Learning by Planning: Language-Guided Global Image Editing

# Datasets

Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts

Enriching ImageNet With Human Similarity Judgments and Psychological Embeddings

Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets

SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and 3D Mesh Reconstruction From Video Data

# Explainable AI & Privacy

Privacy-Preserving Image Features via Adversarial Affine Subspace Embeddings

Transformer Interpretability Beyond Attention Visualization

Black-box Explanation of Object Detectors via Saliency Maps

# Video Analysis and Understanding

Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps

Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion

Omnimatte: Associating Objects and Their Effects in Video 

SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation
















































#### Other papers:
- [NAME](LINK)


# The rest
This post turned into a long one very quickly, so in order to avoid ending-up with a 1h long reading session, I will simply list some papers I came across in case the the reader is interested in the subjects.

<details>
  <summary>Click to expand</summary> <br>

<small> <div class="tip" markdown="1">

**Subject**:
- [Name](LINK)

</div> </small>
</details>




