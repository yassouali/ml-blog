---
title: "Deep Video Understanding"
excerpt: "Given the huge amount of video content being created everyday, many tools capable of analyzing and understanding such a content have been developed. In this blog post, we dive into popular deep learning approaches for video understanding."
date: 2021-12-22 00:00:00
published: true
tags: 
  - computer-vision
  - deep-learning
  - video-understanding
---

In terms of the web, video and audiovisual content dominant the internet, accounting for 82% of all of the internet's traffic in 2021, up from 73% in 2016, and growing at a compound annual growth rate of 26% (source: [cisco](https://www.cisco.com/c/dam/m/en_us/solutions/service-provider/vni-forecast-highlights/pdf/Global_2021_Forecast_Highlights.pdf)). Given such a gigantic amount of video content being produced daily, it is important to develope tools capable of analyzing and understanding such content. So that in the end we are able to rank it, reference it, and use it to better organize the web and leverage such data for many real-world applications such as navigation.

In order to understand video data, and given the complex nature of scenes and events depicted in them, video understanding methods would be required to conduct an automatic analysis, in an efficient and practical way, while extracting semantically meaningful information. Fortunately, with the rise of deep learning, significant progress has been made toward this goal, with novel neural network architectures, better training recipes, various video understanding frameworks (see [PySlowFast](https://github.com/facebookresearch/SlowFast), [PyTorchVideo](https://pytorchvideo.org/), [MMAction](https://github.com/open-mmlab/mmaction)/[MMAction2](https://github.com/open-mmlab/mmaction2), and [Gluon-CV](https://cv.gluon.ai/)), and model acceleration techniques.

In this blog post, we will present an overview of the different videos understanding tasks, datasets and methods used, and then go into more details about the main network architectures on which the different methods are built on. For more detailed surveys please refer to [Kong et al.](https://arxiv.org/abs/1806.11230), [Zhu el al.](https://arxiv.org/abs/2012.06567) and [Hutchinson el al.](https://arxiv.org/abs/2010.06647)
{: .notice}













# Video Understanding

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/VID-UN/video-understanding.png' | absolute_url }}" alt="">
  <figcaption>
  Examples of video understanding tasks. (Image source: <a href="https://arxiv.org/abs/2010.06647">Hutchinson et al</a>)
  </figcaption>
</figure>

With the advent of deep learning, video understanding methods has evolved from being based solely on single-modality hand-crafted features such as [Histogram of Oriented Gradients](https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients). To being able to conduct higher-level semantic classification of videos based on events and actions using different modalities (eg. video, audio & text). The most popular video understanding task is action recognition (can also be refered to as human action recognition or activity recognition), which is usually
restricted to assigning some class label (or many labels) to the entire video. However, this overlooks the finer details of a given scene, such as objects and people, their interactions and relationships, and their progression over time. As such, diverse set of video understanding tasks have been introduced to cover the diversity of audiovisual content. An non-exhaustive list of such tasks is:

### Image Understanding Tasks for Videos
The most straight-froward way to analyse video data is to extend image understanding tasks to video inputs, where each frame can be viewed as a 2D image. Popular image models can then be applied in a per-frame basis as it is the case for various computer vision tasks, such as object and pose detection, captioning, face recognition, instance and semantic segmentation, classification and relationship extraction. However, in this case, the temporal information across frames is not utilized to extract more video-specific information, such as the relationships between the different actors at different timestamps or detecting the action being performed at different time horizons.

### Action or Activity Recognition

The task of action or activity recognition can be viewed as an extension of image recognition to videos, where temporal information is necessary to correctly classify the action being performed. Take for instance the image ([source](https://giphy.com/gifs/backflip-memphis-grizzlies-halftime-show-xTiTnLdthPbb6olo9q)) bellow, which is a still image of a person performing a backfilp. By having access to a single frame, the model is not capable of predicting the correct action being performed (is it a backflip or a person failing?). In such a case, without extracting some temporal information across the input frames by analyzing the small variations within the person's appearance, background clutter, viewpoints and action execution, correctly classifying the action is infeasible. 

<p float="center">
  <img style="margin-left:100px; margin-right:50px;" 
    src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/flip1.png' | absolute_url }}" width="200"
  />
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/flip2.gif' | absolute_url }}" width="200" />
</p>

Given the sophisticated nature of the action or activities, the type of information extracted can span different levels of abstractions ([Degardin et al.](https://www.mdpi.com/2076-3417/11/18/8324)), from *action primitives* that describe atomic movements (eg. left leg forward, right arm folding), to *actions* that describe whole body movements as a juncture of action primitives (eg. running, jumping). Such actions can also then be composed into higher level of abstractions, resulting in various *activities* (eg. jumping hurdles, throwing a football).

<figure style="width: 90%" class="align-center">
  <img src="{{ 'images/VID-UN/action-recognition.png' | absolute_url }}" alt="">
  <figcaption>
  Examples of input-output pairs for the task of action recognition. (Image source: <a href="https://arxiv.org/abs/1706.04261v2">Goyal et al</a>)
  </figcaption>
</figure>

### Extensions of Action Recognition
The task of action recognition is a straight forward extension of image classification to video data, where the model takes as input a video clip and outputs a singular global action that is being performed. However, this definition is too limited given the rich nature of video content, as such, various extensions of this task have been proposed:

**1- Temporal action localization:** Instead of simply predicting a single video level class, temporal action localization consists of also predicting the temporal boundaries (the specific start and end timestamps) where the action took place.

<figure style="width: 75%" class="align-center">
  <img src="{{ 'images/VID-UN/localization.png' | absolute_url }}" alt="">
  <figcaption>
  The task of Temporal action localization. (Image source: <a href="https://arxiv.org/abs/1806.02964v3">Lin et al</a>)
  </figcaption>
</figure>

**Video grounding** and **Step localization** are two other closely related tasks. In video grounding, instead of only predicting the temporal boundaries of pre-defined actions, the task consists of predicting the boundaries of any given input text query. As for step localization, it consists of predicting the boundaries of the different sub-tasks (or task primitives) that constitute a complex task. 

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/VID-UN/video-grounding.png' | absolute_url }}" alt="">
  <figcaption>
  The task of Video grounding. (Image source: <a href="https://arxiv.org/abs/1708.01641">Hendricks et al</a>)
  </figcaption>
</figure>

**2- Spatio-temporal action localization:** Spatio-temporal action localization consists of localizing, both spatially (eg. with bounding boxes or masks) and temporally (eg. with start-end times), the different persons  and recognizing their actions (e.g. pose, interactions with objects or interactions with other persons) in a given dynamic scene. The difficulty with such a task is the need to reason jointly about all of the instances within the scene, understanding all of their interactions, both spatially and temporally, in order to correctly recognize the action being performed and localize the actor.

<figure style="width: 100%" class="align-center">
  <img src="{{ 'images/VID-UN/spatio-temporal-localization.png' | absolute_url }}" alt="">
  <figcaption>
  The task of Spatio-temporal action localization. (Image source: <a href="https://arxiv.org/abs/1705.08421">Gu et al</a>)
  </figcaption>
</figure>

**3- Action segmentation:** Instead of producing video level outputs, another way of doing temporal action detection and localization is temporal segmentation on a per-frame base. In this case, the model outputs at each frame the labels and the confidence. The outputs can then be merged to have the start and end times or the bounding boxes in a post-processing step.

**4- Object tracking:** Another important video understating task is object tracking, which consists of analyzing videos in order to identify and track objects temporally belonging to one or more categories, such as pedestrians, cars, animals and inanimate objects. Different from Spatio-Temporal localization, in addition to detecting the objects at different timestamps, object tracking also consists of associating an instance-specific ID to detected object, in order to tracking each instance across time and distinguish among intra-class objects. The task of 
**Trajectory Prediction** extend object tracking by also predicting target's positions in future frames.

### Other tasks
While the majority of video understanding tasks fall into one of the above categories, other self-supervised tasks can also be defined to learn useful representations without the need for any annotations. One of the most popular tasks is **[Video prediction](https://arxiv.org/abs/2004.05214)**. Video prediction draws inspiration from computational models of the [predictive coding](https://en.wikipedia.org/wiki/Predictive_coding) paradigm, and is defined as the task of inferring the subsequent frames in a video, based on a sequence of previous frames used as a context. Another related task is **[Action prediction](https://arxiv.org/pdf/1806.11230.pdf)**, which can be viewed as a special case of video prediction, where the objective is to only predict the future human actions instead of inferring the whole frames.

Given the complexity of the various video understating tasks and the nature of video data, the amount of labels and computational cost required can be quite high. As a result, the majority of solutions are usually not based on end-to-end training. Instead, and similar to image understanding tasks, a large-scale pre-training step is first conducted for the task of action recognition, then a task specific model then trained with the action classification model as the backbone. *As such, for the rest of this blog post, we will focus solely on the task of action recognition, and its different models and methods.*
{: .notice}





















## Action Recognition

https://arxiv.org/abs/1806.11230
https://arxiv.org/abs/2012.06567


https://lyttonhao.github.io/projects/TBN/TBN.html


[A Comprehensive Study of Deep Video Action Recognition]

One of the most important tasks in video understanding is to understand human actions. It has many real-world
applications, including behavior analysis, video retrieval, human-robot interaction, gaming, and entertainment. Human action understanding involves recognizing, localizing, and predicting human behaviors. The task to recognize human actions in a video is called video action recognition. In Figure 1, we visualize several video frames with the associated action labels, which are typical human daily activities such as shaking hands and riding a bike.NetVLAD


. Human action recognition consists of the extraction of89
concise features, from video image data, to achieve a high-level understanding allowing90
computers to recognize human behavior. Over the last decade, significant improvements91
were accomplished through the emerging deep learning models, distinguishing two92
categorizations in terms of feature descriptors, local and global repre
https://www.mdpi.com/2076-3417/11/18/8324


-	Untrimmed video classification: the video has both irrelevant and relevant parts.
Trimmed video classification: the human annotators have already removed all of the irrelevant parts of the video.


- Simple and complex recognition:
https://arxiv.org/abs/1812.01289

Trimmed and untrimmed


In ordinary life, activities of daily living pop up fre-
quently. Our conversations include actions like “cooking
a meal” or “cleaning the house” much more frequently than
actions like “jumping” or “cutting a cucumber”. The lat-
ter, which we call one-actions, exhibit one visual pattern,
possibly repetitive. They are usually short in time, homoge-
neous in motion and coherent in form. In contrast, cooking
a meal or cleaning the house are very different actions. We
refer to them as complex actions, characterized by: i. They
are typically composed of several one-actions, see figure 1.
ii. These one-actions, contained in a complex action, ex-
hibit large variations in their temporal duration and tem-
poral order. iii. As a consequence of the composition, a
complex action takes much longer to unfold. And, by the
in-homogeneity in composition, the complex action needs
to be sampled in full, not to miss crucial parts.

Properties of complex action “Cooking a Meal”:
composition: consists of several one-actions (Cook, ...),
order: weak temporal order of one-actions (Get Î Wash),
extent: one-actions vary in their temporal extents.

In the recent literature, the main focus is the recogni-
tion of short-range actions like in HMDB, UCF and Kinet-
ics [1, 2, 3]. Few attention has been paid to the recogni

The second challenge is tolerating varia-
tions in temporal extent and temporal order of one-actions.
Related methods [11, 15] learn spatio-temporal convolu-
tions with fixed-size kernels, which would be too rigid for
complex actions. To address these challenges, we present
Timeception, a novel convolutional layer dedicated only for
temporal modeling. It learns long-range temporal depen-
dencies with attention to short-range details. Plus, it toler-
ates the differences in temporal extent of one-actions com-
prising the complex action. As a result, we demonstrate
success in recognizing the long and complex actions, and
achieving state-of-the-art-results in Charades [4], Breakfast
Actions [16] and MultiTHUMOS [17]



- Coarse and fine-grained recognition:
https://arxiv.org/abs/2104.09496

Popular action recognition datasets, such as
UCF101 [190] or Kinetics400 [100], mostly comprise
actions happening in various scenes. However, models
learned on these datasets could overfit to contextual infor-
mation irrelevant to the actions [224, 227, 24]. Several
datasets have been proposed to study the problem of
fine-grained action recognition, which could examine the
models’ capacities in modeling action specific information.
These datasets comprise fine-grained actions in human
activities such as cooking [28, 108, 174], working [103]
and sports [181, 124]. For example, FineGym [181] is a
recent large dataset annotated with different moves and
sub-actions in gymnastic videos.


Imagine that you wish to answer particular questions about
a video. These questions could be quite general, e.g., “what
instrument is being played?”, quite specific, e.g., “do people
shake hands?”, or require a composite answer, e.g., “how
many somersaults, if any, are performed in this video, and
where?”. Answering these questions will in general re-
quire attending to the entire video (to ensure that nothing
is missed), and the response is query dependent. Further,
the response may depend on only a very few frames where
a subtle action occurs. With such video understanding ca-
pability, it is possible to effortlessly carry out regular video
metrology such as performance evaluation in sports train-
ing, or issuing reports on video logs.

Coarse vs. fine-grained action recognition.
Top: Object and background cues from only a few frames
can inform classic coarse-grained action recognition in
datasets like Kinetics [30], where visually distinct activities
are to be distinguished. Bottom: However, for finer-grain
classification which depends on subtle differences in pose,
the specific sequence, duration and number of certain sub-
actions, as for the gymnastics sequence above, requires rea-
soning about events at varying temporal scales and attention
to fine details. We develop a novel query-based video net-
work and a training framework for such fine-grained tem-
poral reasoning


- Short and long-term recognition:
https://arxiv.org/abs/2106.11310



Long-Form Video Understanding aims at understand-
ing the “full picture” of a long-form video. Examples include un-
derstanding the storyline of a movie, the relationships among the
characters, the message conveyed by their creators, the aesthetic
styles, etc. It is in contrast to ‘short-form video understanding’,
which models short-term patterns to infer local properties





- Standard and Egocentric recognition:
https://epic-kitchens.github.io/2021
https://ego4d-data.org/



The use of wearable technologies has increased the de-
mand for applications that can efficiently process large
amounts of raw data from motion sensors and videos. Rec-
ognizing an activity is possibly the first step in understand-
ing the context in which a person is embedded. Therefore,
creating robust methods to recognize activities under the
constraints of smart devices becomes one of the main chal-
lenges in the awakening of this technological trend.


The egocentric domain also entails new challenges.
Cameras often produce shaken and blurred shots due to the
natural movements of the wearer. Unintelligible images can
be produced by real life situations such as dark and rainy en-
vironments. Therefore, alternative sources of information
such as motion sensors can be used to increase the predic-
tion performance at a low power consumption cost. In fact,
the use of sensors such as accelerometers [2, 6] have played
an important role in EAR. Traditionally, these devices were
attached to several parts of the body [2, 12, 43], to exter-
nal objects [14, 40] and to the ambient [12, 43] which often
limited their use to controlled environments which can be
quite different to a real-life setting

Recently, large-scale egocentric action recognition [29,
28] has attracted increasing interest with the emerging of
wearable cameras devices. Egocentric action recognition
requires a fine understanding of hand motion and the inter-
acting objects in the complex environment. A few papers
leverage object detection features to offer fine object con-
text to improve egocentric video recognition [136, 223, 229,
180]. Others incorporate spatio-temporal attention [192] or
gaze annotations [131] to localize the interacting object to
facilitate action recognition. Similar to third-person action
recognition, multi-modal inputs (e.g., optical flow and au-
dio) have been demonstrated to be effective in egocentric
action recognition [101].

EPIC-Kitchens is a large scale egocentric video bench-
mark for daily kitchen-centric activity understanding [1]. In
this benchmark, the action classes are defined by combining
verb and noun classes.
By combining all the 352 nouns and 125 verbs, the num-
ber of all possible action classes will reach as large as
44,000. 

This dataset presents a long tail distribution as of-
ten occurred in natural scenarios. Besides, human-object
interaction actions might be very ambiguous. For exam-
ple, in a single video clip, a person might be washing a
dish whilst interacting with a sponge, faucet and/or sink
concurrently, and sometimes the in-interaction active ob-
ject might be completely occluded. These factors all ren-
der action recognition on this dataset extremely challeng


- Multi-modal and uni-modal recognition:

Our experience of the world is multimodal – we see objects, hear sounds, read words, feel textures and
taste flavors. Humans interact with their environment through these multiple sensory streams, combin-
ing information and forming associations between senses [Edelman, 1987; Bahrick & Lickliter, 2000;
L. Smith & Gasser, 2005]. When a baby eats an apple, for instance, taste is not her only experience –
she will also hear the apple crunch, see its shiny skin, and feel its smooth surface on her skin [L. Smith
& Gasser, 2005]. Psychologists suggest that these multiple overlapping and time-locked sensations are
an crucial enabler of human perceptual learning about the world [Bahrick & Lickliter, 2000; L. Smith
& Gasser, 2005].


https://www.robots.ox.ac.uk/~vgg/publications/2020/Nagrani20e/nagrani20e.pdf


Audiovisual activity recognition. Joint modeling of audio
and visual signals has been largely conducted in a “late-
fusion” manner in video recognition literature [41, 50, 24].
For example, all the entries that utilize audio in the 2018 Ac-
tivityNet challenge report [24] have adopted this paradigm –
meaning that there are networks processing visual and audio
inputs separately, and then they either concatenate the out-
put features or average the final class scores across modali-
ties. Recently, an interesting audiovisual fusion approach
has been proposed [41] using flexible binding windows
when fusing audio and visual features. With three similar
network streams, this approach fuses audio features with
the features from RGB and optical flow at the final stage
before classification. In contrast, AVSlowFast is building a
hierarchically integrated audiovisual representation.


Multi-modal video understanding has attracted increas-
ing attention in recent years [55, 3, 129, 167, 154, 2, 105].
There are two main categories for multi-modal video un-
derstanding. The first group of approaches use multi-
modalities such as scene, object, motion, and audio to
enrich the video representations. In the second group,
the goal is to design a model which utilizes modality in-
formation as a supervision signal for pre-training mod-
els [195, 138, 249, 62, 2].


Researchers have long been inter-
ested in developing models that can learn from multiple
modalities (e.g., audio, vision, language). Beyond audio
and visual modalities, extensive research has been con-
ducted in other instantiations of multi-modal learning, in-
cluding vision and language [20, 4, 83], vision and locomo-
tion [87, 22], and learning from physiological data [48].





- Challenges
In terms of dataset, first, defining the label space for training action recognition models is non-trivial.
Second, annotating videos for action recognition are laborious (e.g., need to watch all the video frames) and ambiguous (e.g, hard to determine the exact start and end of an action).
Third, some popular benchmark datasets (e.g., Kinetics family) only release the video links for users to down-
load and not the actual video, which leads to a situation that methods are evaluated on different data

In terms of modeling, first, videos capturing human actions have both strong intra- and inter-class variations. People can perform the same action in different speeds under various viewpoints.

Second, recognizing human actions requires simultaneous understanding of both short-term action-specific motion information and long-range temporal information.
Third, the computational cost is high for both training and inference,




































### Models


A typical action recognition flowchart generally contains two
major components [3], [23], [79], action representation and action
classification. The action representation component basically con-
verts an action video into a feature vector [11], [80], [81], [82] or
a series of vectors [21], [54], [83], and the action classification
component infers an action label from the vector [53], [84],
[85]. Recently, deep networks [14], [18], [27] merge these two
components into a unified end-to-end trainable framework, which
further enhance the classification performance in general. In this
section we will discuss recent work in action representation, action
classification, and deep networks.



Thanks to both the availability of large-scale datasets and the rapid progress in deep learning, there is also a
rapid growth in deep learning based models to recognize video actions.


https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/



- **Snippet-level deep representation:** takes as input a limited number of frames.
	- DeepVideo.
	- two-stream CNNs.
	- 3D CNNs.
- **Long term time-span:** can utilize RNNs or extend the first approaches to take as input a larger number of frames. However, they are still limited in time. To solve this, we can sample only a small number of snippets from a long video and use them for action recognition. This allows us to take into consideration the content of the full video when training.
- **Attention based models.**
- **Graph based models.**



- **2D and 3D networks**


Most existing methods either view a video as a list of 2D images (e.g., [37, 66]) or a width×height×time
pixel volume (e.g., [8, 18, 74]). While these views are convenient, we argue that they are unnatural ways to look at the signals, possibly leading to difficulties in learning. After all, a video frame is simply a projection of (constantly changing) objects and scenes in a 3D world snapshotted at a particular point of time. Modeling videos through modeling the interactions among a list of 2D images likely suffers from model misspecification, because the projected 2D images do not interact with each other — It is the objects in our
3D world that interact with each other. Object Transformers directly model these interactions.

A conceptually easy way to understand a video is as a 3D tensor with two spatial and one time dimension. Hence, this leads to the usage of 3D CNNs as a processing unit to model the temporal information

2D CNNs enjoy the benefit of pre-training brought by the large-scale of image datasets such as ImageNet [30] and
Places205 [270], which cannot be matched even with the largest video datasets available today. On these datasets
numerous efforts have been devoted to the search for 2D CNN architectures that are more accurate and generalize
better. Below we describe the efforts to capitalize on these advances for 3D CNNs.


The seminal work for using 3D CNNs for action recognition is [91]. While inspiring, the network was not deep
enough to show its potential. Tran et al. [202] extended [91] to a deeper 3D network, termed C3D. C3D follows the
modular design of [188], which could be thought of as a 3D version of VGG16 network. Its performance on standard
benchmarks is not satisfactory, but shows strong generalization capability and can be used as a generic feature extractor for various video tasks [250].

To reduce the complexity of 3D network training, P3D [169] and R2+1D [204] explore the idea of 3D factorization.
To be specific, a 3D kernel (e.g., 3×3×3) can be factorized to two separate operations, a 2D spatial convolution (e.g.,1 × 3 × 3) and a 1D temporal convolution (e.g., 3 × 1 × 1). The differences between P3D and R2+1D are how they arrange the two factorized operations and how they formulate each residual block. Trajectory convolution [268] follows this idea but uses deformable convolution for the temporal component to better cope with motion.



- **Two-stream networks**

This method is related to the two-streams hypothesis, according to which the human visual cortex contains two pathways: the ventral stream (which performs object recognition) and the dorsal stream (which recognizes motion). The spatial stream takes raw video frame(s) as input to capture visual appearance information. The temporal stream takes a stack of optical flow images as input to capture motion information between video frames.

Since there are two streams in a two-stream network, there will be a stage that needs to merge the results from both networks to obtain the final prediction. This stage is usually referred to as the spatial-temporal fusion step. The easiest and most straightforward way is late fusion, which performs a weighted average of predictions from both streams. However, in order to have earlier interactions between the two networks could benefit both streams during model learning and this is termed as early fusion, such as skip connections.

Since video understanding intuitively needs motion information, finding an appropriate way to describe the tem-
poral relationship between frames is essential to improving the performance of CNN-based video action recognition.

Optical flow is the motion of objects between consecutive frames of sequence, caused by the relative movement between the object and camera. It is 2D vector field where each vector is a displacement vector showing the movement of points from first frame to second. Given the image intensity (I) as a function of space (x,y) and time (t). In other words, if we take the first image I(x,y,t) and move its pixels by (dx,dy) over t time, we obtain the new image I(x+dx,y+dy,t+dt).

Most video action recognition approaches use raw videos (or decoded video frames) as input. However, there are
several drawbacks of using raw videos, such as the huge amount of data and high temporal redundancy. Video com-
pression methods usually store one frame by reusing contents from another frame (i.e., I-frame) and only store the difference (i.e., P-frames and B-frames) due to the fact that adjacent frames are similar. Here, the I-frame is the original RGB video frame, and P-frames and B-frames include the motion vector and residual, which are used to store the difference. Motivated by the developments in the video compression domain, researchers started to adopt compressed video representations as input to train effective video models.

*Multi-stream networks* 

here are other factors that can
help video action recognition as well, such as pose, object,
audio and depth, etc



- **Clip or video methods**

Segment-based methods

Thanks to optical flow, two-stream networks are able to reason about short-term motion information between frames. However, they still cannot capture long-range temporal information. Motivated by this weakness of two-stream networks , Wang et al. [218] proposed a Temporal Segment Network (TSN) to perform video-level action recognition. Though initially proposed to be used with 2D CNNs, it is simple and generic. Thus recent work using either 2D or 3D CNNs, are still built upon this framework


TSN first divides a whole video into several segments, where the segments distribute uniformly along the temporal dimension randomly selects a single video frame within each segment and forwards them through the network





# 2D Models

## Aggregation / Consensus

https://arxiv.org/pdf/1706.06905.pdf

VLAD3 and ActionVLAD [123, 63]
also appeared concurrently. They extended the NetVLAD
layer [4] to the video domain to perform video-level encod-
ing, instead of using compact bilinear encoding as in [36].
To improve the temporal reasoning ability of TSN, Tempo-
ral Relation Network (TRN) [269] was proposed to learn
and reason about temporal dependencies between video
frames at multiple time scales.


- ActionVlad
https://arxiv.org/abs/1704.02895

- Non-local Action Vlad

- Graph
https://openaccess.thecvf.com/content_ICCVW_2019/html/ADW/Herzig_Spatio-Temporal_Action_Graph_Networks_ICCVW_2019_paper.html

- Relation

- Attention

- Average / Max / Mean

- NetVLAD

Mixture-of-Experts

Learnable pooling

- RNNs

https://arxiv.org/abs/1411.4389


-  Deep Local Video Feature (DVOF) proposed to treat the deep networks that
trained on local inputs as feature extractors and train another
encoding function to map the global features into global la-
bels. 


- Temporal Linear Encoding (TLE) network appeared concurrently with DVOF, but the encoding layer was
embedded in the network so that the whole pipeline could be trained end-to-end

https://lyttonhao.github.io/projects/TBN/TBN.html



















### Datasets and Evaluation

Over the last decade, there has been growing research interest in video action recognition with the emergence of
high-quality large-scale action recognition datasets. We summarize the statistics of popular action recognition

We see that both the number of videos and classes increase rapidly, e.g, from 7K videos over 51 classes in HMDB51 [109] to 8M videos over 3, 862 classes in YouTube8M [1]. Also, the rate at which new datasets are
released is increasing: 3 datasets were released from 2011 to 2015 compared to 13 released from 2016 to 2020.


**Types 1**


Sources of data:
- **Controlled collection:** The data is collected by asking human performers to perform a given action.
	- Weizmann
	- KTH
- **In the wild:** The data is collected from videos that were not originally designed for action recognition.
	- HMDB51 (Movies) 
	- UCF101, ActivityNet, Kinetics (YouTube videos)
	- HiEve (Surveillance footage)



Controlled Action Video Datasets
https://arxiv.org/pdf/1806.11230.pdf

We first describe individual action datasets captured in controlled
settings, and then list datasets with two or more people involved
in actions. We also discuss some of the RGB-D action datasets
captured using a cost-effective Kinect sensor

- Individual Action Datasets
Weizmann dataset
KTH dataset

- Group Action Datasets
UT-Interaction dataset 
BIT-Interaction dataset


Unconstrained Datasets
Although the aforementioned datasets lay a solid foundation for
action recognition research, they were captured in controlled
settings, and may not be able to train approaches that can be
used in real-world scenarios. To address this problem, researchers
collected action videos from Internet, and compiled large-scale
action datasets, which will be discussed in the following.

UCF101 dataset
HMDB51
Sports-1M 
Moments-in-Time 




The datasets also vary in terms of the number of samples per class and the number of classes.
https://lyttonhao.github.io/projects/TBN/TBN.html


Deep learning methods usually improve in accuracy when the volume of the training data grows. In the case
of video action recognition, this means we need large-scale annotated datasets to learn effective models.

For the task of video action recognition, datasets are often built by the following process: (1) Define an action list, by combining labels from previous action recognition datasets and adding new categories depending on the use case. (2) Obtain videos from various sources, such as YouTube and movies, by matching the video title/subtitle to the action list. (3) Provide temporal annotations manually to indicate the start and end position of the action, and (4) finally clean up the dataset by de-duplication and filtering out noisy classes/samples

**Types 2**

https://arxiv.org/pdf/2012.06567.pdf

scene-focused (UCF101, HMDB51 and Kinetics400 in section 4.2), motion-focused (Sth-Sth V1 and V2 in section 4.3)
and multi-label (Charades in section 4.4).

- scene-focused datasets: Here, we compare recent state-of-the-art approaches on
scene-focused datasets: UCF101, HMDB51 and Kinet-
ics400. The reason we call them scene-focused is because
most action videos in these datasets are short, and can be
recognized by static scene appearance alone as shown in
Figure 4.


- Motion-focused datasets: Different from scene-focused datasets, back-
ground scene in Sth-Sth datasets contributes little to the fi-
nal action class prediction. In addition, there are classes
In addition, there are classes
such as “Pushing something from left to right” and “Push-
ing something from right to left”, and which require strong
motion reasoning.

-  Multi-label datasets
roperties of complex action “Cooking a Meal”:
composition: consists of several one-actions (Cook, ...),
order: weak temporal order of one-actions (Get Î Wash),
extent: one-actions vary in their temporal extents.

Breakfast Actions
MultiTHUMOS
Charades




**Types 2**

Unimodal and multimodal datasets

All the datasets described above were captured by RGB video
cameras. Recently, there is an increasing interests in using cost-
effective Kinect sensors to capture human actions due to the
extra depth data channel

Popular RGB-D action datasets are listed in the following (MSR Daily Activity dataset, CAD-120 dataset, UTKinect-Action dataset)




**Evaluation**

During model training, we usually randomly pick a video
frame/clip to form mini-batch samples. However, for eval-
uation, we need a standardized pipeline in order to perform fair comparisons.


For 2D CNNs, a widely adopted evaluation scheme is
to evenly sample 25 frames from each video following
[187, 217]. For each frame, we perform ten-crop data aug-
mentation by cropping the 4 corners and 1 center, flipping
them horizontally and averaging the prediction scores (be-
fore softmax operation) over all crops of the samples, i.e.,
this means we use 250 frames per video for inference.

For 3D CNNs, a widely adopted evaluation scheme
termed 30-view strategy is to evenly sample 10 clips from
each video following [219]. For each video clip, we per-
form three-crop data augmentation. To be specific, we scale
the shorter spatial side to 256 pixels and take three crops of
256 × 256 to cover the spatial dimensions and average the
prediction score

In terms of evaluation metric, we report accuracy for
single-label action recognition, and mAP (mean average
precision) for multi-label action recognition.


















































# Video Backbones

## Two-stream networks

























### Computational Comparison

Source: TSM, Multi-grid, X3D, ECO, SlowFast and Multi-scale Transformer.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/c1.png' | absolute_url }}">
</figure>

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/c2.png' | absolute_url }}">
</figure>

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/c3.png' | absolute_url }}">
</figure>

## C3D

**Paper: [Learning Spatiotemporal Features with 3D Convolutional Network](https://arxiv.org/abs/1412.0767).**

The main component of the model is 3D convolutions. With 2D convolutions, the kernel
is displaced only over the spatial dimensions while the computation is done over the whole depth, either RGB for images, or the temporal dimension in the form of the number of frames for videos. However, with 3D convolutions, the displacement is also done in the temporal dimension. For example, with a filter of size H x W x d, and d is over the temporal dimension of L frames, the kernel is applied (L - d / 2) / strides instead of a single time.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/1.png' | absolute_url }}">
</figure>

*What is the correct depth?* For d x k x k 3D convolutions, we need to find the correct temporal depth. For 5 layers of convolutions, the authors propose two variations:
- Homogeneous: all of the conv layers have the same depth: 1, 3, 5, or 7.
- Variable: either increasing 3-3-5-5-7 or decreasing: 7-5-5-3-3.

By testing these different variations. The authors find that the best choice is a fixed depth with a depth of 3, so 3-3-3-3-3. Given this results, the final C3D architecture has 8 convolutions, 5 max-pooling layers, and 2 fully connected layers, followed by a softmax output layer. All of the 3D convolutions are 3x3x3 with stride 1 in both spatial and temporal dimensions. All pooling kernels are 2x2x2, except for the first pooling layer which is 1×2×2 to avoid removing the temporal information too early.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/2.png' | absolute_url }}">
</figure>


## TLE (Temporal Linear Encoding Networks)

**Paper: [Deep Temporal Linear Encoding Networks](https://arxiv.org/abs/1611.06678).**

The main component of the proposed models is the TLE module, a new spatio-temporal feature encoding module that aggregates multiple video segments over long time ranges. Inspired by the success of [IDTs](https://hal.inria.fr/hal-00873267v2/document), which showed that tracking densely sampled point over the frames with optical flow can give good video representations. TLE consists of efficiently aggregating the frames together on a single feature space to encode all of the clips or videos, instead of using late fusion where each frame or clip is scored separately and the scores are then aggregated.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/3.png' | absolute_url }}">
</figure>

Given K features maps generated by a CNN over K (here K = 3) input segments, where each feature map is of size H x W x C. The paper investigates different linear aggregation methods to encoder all of the K segments into a compact and robust representation:
- Element-wise average.
- Element-wise maximum.
- Element-wise multiplication.
The best is element-wise multiplication which was therefore selected.

After the aggregation, the resulting features are then encoded, producing a linearly encoded feature vector where every channel of the aggregated features interact with the rest of the channels, resulting in better representations. For such an encoding, the paper investigates the following:
- Bilinear Models: compute the outer product between the aggregated features and themselves, resulting in features of size C², then followed by a projection into a lower dimensional space using a linear layer.
- Fully-connected pooling: The aggregated features are directly fed into a linear layer followed by a classification layer.
The best is bilinear aggregation method.

The final network: a two-stream network with spatial and temporal networks. The spatial net operates on RGB frames, and the temporal net operates on a stack of 10 optical flow frames. The model can also be 3D based, this time the input are 3 clips for video recognition, instead of 3 frame for clip recognition. The TLE module is added before the classification layer to aggregate the K feature maps followed by the classification layer. For a 2 stream network, TLE is added on both nets followed by a score averaging. 

## TCN (Temporal Convolutional Networks)

**Paper: [Deep Temporal Linear Encoding Networks](https://arxiv.org/abs/1611.06678).**

Temporal Convolutional Networks (TCNs) use a hierarchy of temporal convolutions to
perform fine-grained action segmentation or detection. Instead of extracting the spatiotemporal features from video frames and then feeding them to a temporal classifier. With TCNs, we have the following: 1) computations are performed layer-wise, meaning every time-step is updated simultaneously instead of updating sequentially per-frame (2) convolutions are computed across time, and (3) predictions at each frame are a function of a fixed-length period of time (temporal receptive field). Based on such properties, the paper proposes two architectures:
- Encoder-Decoder TCN: consisting of L encoding layers, where each layer contains a temporal convolution, a non-linear activation and max pooling across time that halves the number of temporal steps. The decoder is similar to the encoder except that the max pooling layers are replace with upsampling layers that simply duplicate the entries temporally.
- Dilated TCN: follows a similar structure to that of WaveNet, but adapted for action segmentation. It consists of a series of convolutional layer with the same number of filter and with dilated convolutions with increasing rate with higher layers. Each layer consists of dilated convolution, a non-linear activation and a residual connection that combines the layer's input and its convolved version. The convolution consists of W1 x F(t-s) + W2 x F(t). With s as the dilation rate and F as the input features, followed by skip connection.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/4.png' | absolute_url }}">
</figure>

Note that for both models, there is a causal version where the prediction at each frame only depend on the previous ones, here convolution of the encoder-decoder network is restricted to only the d previous frames. Or acausal where we have access to both the forward and backward directions. Here, in the dilated version, the input to each convolution takes the center, left with a given dilation step, and right with a given dilation step.

**Other variations:**

- [MS-TCN](https://arxiv.org/abs/1903.01945) or Multi-Stage TCN: Stacks multiple Dilated TCNs on top of each other for better refinement. In addition to a smoothing loss to reduce the over-segmentation found in some video predictions, which is simply an MSE loss between two class probabilities at time steps t and t-1. The final loss is then the sum of the losses at each stage.
- [MS-TCN++](https://arxiv.org/abs/2006.09220): Here, the architecture is the same as MS-TCN but where the first few stages have a modified version of the dilated convolutions. The new version is called dual dilated convolutions where the output is the concatenation of the output of two dilated convolution with different dilation rates. The dilation rate of first one decreases with higher layers while the second increases.

**Important Note: All of these models take as input 1D vector which are I3D features for each video frame, so they don't take as input the raw frames.**

## Hidden Two-Stream CNNs

**Paper: [Hidden Two-Stream Convolutional Networks for Action Recognition](https://arxiv.org/abs/1704.00389).**

The two-stage pipeline, where we first compute optical flow and then learn the mapping from optical flows to action labels, is sub-optimal for the following reasons:
- The pre-computation of optical flow is time consuming and storage demanding compared to the CNN step.
- Traditional optical flow estimation is completely independent of the final
tasks and can be potentially sub-optimal.
To solve this, the paper proposes an end-to-end CNN approach to learn optical flow so that we can avoid costly computation and storage and obtain task-specific motion representations.

<figure style="width: 65%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/5.png' | absolute_url }}">
</figure>

The model consists of a MotionNet that's trained in an unsupervised manner for optical flow generation, and a temporal stream net that operates over raw frames. Both are concatenated and used to map the estimated optical flow to the target action labels. Given two frames, MotionNet is trained to generate a good flow field so that the first frame can be reconstructed using backward wrapping from the second frame and the predicted flow. The loss in this case is the pixel wise difference between the original first frame and the backward wrapped frame. More precisely, to have good results, they use a fully-connected net with only 3x3 convolutions to avoid over-capturing the dominant camera motion with a multi-scale loss computation, where the loss has three terms: Charbonnier penalty between the first frame and the wrapped image, which is a differentiable variant of the L1 penalty, a smoothness loss over the flow which penalizes the gradients of the flow field, and a structural similarity loss that compares the mean and variance of the 8x8 patches of the first and wrapped frames.

After training MotionNet, we can then combine it with the temporal stream into a one stage and perform end to end training for action recognition. They can either be stacked or used in a two branch network, and the authors found that stacking, while more computationally expensive, is better. Before fusing or feeding the optical flow to the next stage, they need to be normalized first, where the motions larger than 20 pixels are clipped to 20 pixels, and the flows are then normalized and quantized to have a [0, 255] range.

## ActionVLAD

**Paper: [ActionVLAD: Learning spatio-temporal aggregation for action classification](https://arxiv.org/abs/1704.02895).**

One of the main limitation of the two-stream architectures is that they largely disregard the long-term temporal structure of the video and essentially learn a classifier that operates on individual frames or short blocks of few (up to 10) frames, which can force a consensus of classification scores over different segments of the video. This raises the question whether such temporal averaging is capable of modelling the complex spatiotemporal structure of human actions. The desired aggregator needs to operate over the entire video on both the appearance and the motion without requiring every frame to be classified into one of the actions.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/6.png' | absolute_url }}">
</figure>

ActionVLAD: to solve this, the authors propose a centroid based aggregation approach. Given N x T D-dimensional spatiotemporal features. First, we define a vocabulary of K D-dimensional centroids. For each centroid, the distance between this centroid and all of features is computed, and the final aggregation is the weighted sum of all these N x T distances, where the weights is a softmax over the distances.
As such, results of the aggregation is K D-dimensional feature vectors. Such an aggregation is then applied to each stream independently, and then the classification scores are combined.

## TSN (Temporal Segment Networks)

**Paper: [Temporal Segment Networks for Action Recognition in Videos](https://arxiv.org/abs/1705.02953).**

The application of ConvNets to action recognition is constrained by three main obstacles:
- Lack of long-range temporal structure: the previous methods only focused on appearances and short-term motions (up to 10 frames), thus lacking the capacity to incorporate long-range temporal structure.
- Untrimmed videos: to have good real world results, the model needs to be adapted for untrimmed videos where the action may occupy only a small portion of the video.
- Optical flow: the extraction of the optical flow for motion processing becomes a big performance bottleneck with larger datasets which are necessary for video recognition.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/7.png' | absolute_url }}">
</figure>

TSNs tries to solves these issues by providing data and computationally efficient method that can learn video representations across a long range temporal structure. Instead of working on a single frame or a short stack of frames, TSNs operate on a sequence of short snippets sampled from the entire video. To make the sampled snippets represent the whole video, the video is first divided into several segments of equal duration, and then one snippet is randomly sampled from each segment. Each snippet is then passed through the same CNN, and the output of each one is combined with a segmental consensus module that turn segment based features into video level features and then into a final class prediction.

In order to learn parameters dependent on the large skunk of the video corresponding to the desired action and not just one snippet, the gradient of aggregator must flow to all or the majority of the sampled snippets. In the case of max pooling, the gradient will only flow to one snippet and in this case we lack the capacity to model multiple snippets. With average pooling, the gradients will flow to all of the snippets equally, but in this case we might want to only focus on the parts of the video where there is the desired action and ignore the background. To strike a balance, the authors propose different variants like top-k pooling, linear weighting with learned weights, or attention based aggregation.

Action recognition in untrimmed videos: with long video where there is high probability that most of the snippets do not correspond to any actions. The authors propose to sample snippets at a fixed rate, and then windows of different temporal sizes (1, 2, 4, 8 and 16) are used over the sampled snippet. The score of each window computed as the max over all of its snippets. Finally, a top-k aggregation is applied over the class scores of each window to get the final scores.

## I3D

**Paper: [Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset](https://arxiv.org/abs/1705.07750).**

Given how small the previously dominant datasets (UCF-101 and HMDB-51), it can be quite difficult to evaluate the proposed architectures and find the best design choices. To solve this, the paper investigates the dominate design choices using Kinetics dataset, which has two orders of magnitude more data, with 400 human action classes and over 400 clips per class. The comparison will also be based on a new version of 3D convolutions called inflated 3D convolutions, where 2D ImagetNet pertained convolutions are inflated over the time dimension, transforming them into 3D convolution adapted for video understanding tasks.

Inflating 2D convs: each 2D filter can be converted into a 3D ones by simply adding the temporal dimension, going from N x N to N x N x N. Where the N x N original spatial weights are first copied N times, and then divided by N to ensure that the new filter gives the same response as the original one.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/8.png' | absolute_url }}">
</figure>

The authors compare old methods, mainly:
- ConvNet+LSTM where 2D convolutions are applied over K frames then processed temporally with an LSTM.
- 3D ConvNets that processes the input frames both spatially and temporally using 3D convolutions.
- Two-Stream Networks: that uses optical flows in addition to RGB frames in a two stream design for an enhanced motion and a recursive motion understanding, where both inputs are processed using 2D convolutions.
- Two-Stream Inflated 3D ConvNets: where both stream are processed using 3D convolution.
In all of the above, the 3D convolutions can be replaced with the new 3D inflated convolutions for better results.

Network: Given the new inflated 3D, any pre-existing architecture can be converted into a 3D in a straight forward manner. However, in 2D networks, when doing a pooling operation, the pooling kernels and strides are the same in both spatial directions. However, this is not the case with the time domain, where such parameters need to depend on the frame rate, and if the temporal receptive field grows too quickly and in tandem with the spatial receptive field, we might conflate edges from different objects if the it grows too fast, and if it grows too slowly, it may not capture scene dynamics well. To strike a good balance, the proposed I3D, a 3D version of inception, does not have any temporal pooling in the first two max pooling layers, while having symmetric temporal and spatial parameters for the rest, except the last one where the average pooling is applied over 2 frames.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/9.png' | absolute_url }}">
</figure>

## R(2+1)D

**Paper: [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248).**

<figure style="width: 35%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/11.png' | absolute_url }}">
</figure>

Image-based 2D CNNs, while only operating on individual frames, still achieve competitive results on video understanding many benchmarks even if they are unable to model temporal information. However, temporal reasoning still remains an important component for effective action recognition. This paper explores two ways of effectively modeling the temporal information:
- using 3D convolutions only at the start or the end, thus considering the motion information either as a low level or high level one.
- considering (1+2)D convolutions as a middle ground between the simple 2D convs and the expensive 3D convs, which consists of factorizing 3D convolutions into separate spatial and temporal components and use it in a residual learning framework.
This (1+2)D factorization adds additional nonlinear rectification between these two operations, doubling the total number of nonlinearities compared to the 3D version of the model, giving the model better representation capabilities. Additionally, such a decomposition simplifies the training process the results in lower training loss where the convolution is forced into separate spatial and temporal space.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/10.png' | absolute_url }}">
</figure>

The authors then compare different variant of the ResNet architecture adapted for video recognition, either fully 2D model, mixed with 3D convs at the start or at the end, fully 3D conv model, or fully (1+2)D model. The results show that R(2+1)D perform the best while having similar computational cost.

### P3D

This is very similar to [Pseudo-3D Convs](https://arxiv.org/abs/1711.10305), whichs also proposes an adaptation of the bottleneck block of ResNet 2D to video classification. Three different pseudo-3D blocks were introduced: P3D-A, P3D-B, and P3D-C. The blocks implement different orders of convolution: spatial followed by temporal, spatial and temporal in parallel, and spatial followed by temporal with skip connection from spatial convolution to the output of the block, respectively. R(2+1)D differs by using the same convs all the way, while P3D start with 2D convs then uses the proposed version.

<figure style="width: 55%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/12.png' | absolute_url }}">
</figure>


## Non-local Neural Networks

**Paper: [Non-local Neural Networks](https://arxiv.org/abs/1711.07971).**

This paper proposes non-local operations for capturing long rage dependencies with neural nets. This is done directly by computing interactions between any two positions regardless of their distance. This is different from a conv layer that only considers a local neighborhood, or a fully connected layer where the weights are learned across examples, and does not depend on relationship between each input pair.

With the non-local operation, the output at each location (either time, space or spacetime), depends on all of the inputs. Specifically, the output at a location i is the weighted sum over all possible responses between the input i itself and all of the rest of the inputs j, which computed as the function of the similarity between i and j, and a new representation of the input at position j.

<figure style="width: 45%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/13.png' | absolute_url }}">
</figure>

Types of the non-local module: In order to aggregate the information between a given input i with respect to the rest of the inputs (j for all j in the inputs). By first fixing the transformation of j as a linear transformation (1x1 or 1x1x1 convolution), we have many different possible choices on how to compute the pair wise relationships:
- Gaussian: exponentiated dot product similarity between the features of a pair (i, j). Normalization is the sum of the relationships.
- Embedded Gaussian: exponentiated dot product similarity between the embedded (two linear transformations) features of a pair (i, j), this can be seen as a case of the standard self attention. Normalization is the sum of the relationships.
- Dot product: dot product similarity between the embedded features of a pair (i, j). Normalization is the number of pairs.
- Concatenation: ReLu of projected concatenation of the concatenated features of a pair (i, j). Normalization is the number of pairs.

Model: after defining the non-local module, we can then create a non-local block that uses such a module. The block simply consists of a residual connection, where the output is the sum of the input, and the output of the non-local module after being transformed by a linear layer. This linear layer is initialized as zero so that the non-local block is a simple identity block at the start of the training. The block can then be added at different locations of the network, but the authors propose to add it at the end with downsampled spatial and temporal dimensions to avoid having an excessive cost. One of the main variants of the paper is **C2D**, which is a model composed only of 2D convolutions, 2 3D pooling layers at the start, and non-local blocks.

## GCN (Space-Time Region Graphs)

**Paper: [Videos as Space-Time Region Graphs](https://arxiv.org/abs/1806.01810).**

In order to recognize a given action in a video sequence like "opening a book", the paper argues that we need two ingredients for this: (1) temporally linking book regions across time while modeling actions as transformations to track how the shape of the book changes across time, and (2) since the state of the objects might change due to some inter human-object and object-object interactions, we also need to model such interactions.

<figure style="width: 65%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/25.png' | absolute_url }}">
</figure>

To explicitly model such human-object and object-object interactions, the paper proposed to build a space-time graph, where each node in the graph represents the features of a given detected object at a given time step (the features are extracted using ROI Align). The nodes are then connected by two edges, an appearance similarity and spatiotemporal proximity edge. The weights of appearance edges are computed as the similarity to each node to the rest of the nodes (dot product of the transformed features followed by a softmax), while the temporal edges are computed as a weight that reflects the intersection over union between the bounding box of a given object at two different times. The nodes of the graph are then updated using Graph Convolution Networks for both similarity and proximity. The final node representation is computed as a sum of the original features and both the updated similarity and proximity features.

## Spatiotemporal Multiplier & Residual Networks

**Paper: [Spatiotemporal Residual Networks for Video Action Recognition](https://papers.nips.cc/paper/2016/file/3e7e0224018ab3cf51abb96464d518cd-Paper.pdf).**
**Paper: [Spatiotemporal Multiplier Networks for Video Action Recognition](https://openaccess.thecvf.com/content_cvpr_2017/papers/Feichtenhofer_Spatiotemporal_Multiplier_Networks_CVPR_2017_paper.pdf).**

ST-ResNet (Spatiotemporal Residual Nets): The model builds on the two steam approach (temporal with optical flows as inputs, and spatial with RGB images as inputs) with a final score fusion. However,the ST-RestNet model is initialized from ResNet trained on ImageNet, where the 2D convs are transformed from 2D to 3D by inflating them as in I3D, . The model then becomes a 3D convolutional 2-stream net with. Additionally, instead of using a late fusion of the two stream, ST-RestNet adds residual connection between the two streams and jointly fine-tuning them.

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/14.png' | absolute_url }}">
</figure>

STM-ResNet (Spatiotemporal Multiplier Nets): Instead of a simple sum based residual fusion as in ST-ResNet, this paper tries to investigate different fusion method in a more systematic way, in order to find the best fusion method with such residual connections. The residuals can be additive or multiplicative, and one way (motion to appearance or vice-versa) or two way interaction. The authors then found that the best ones are motion to appearance one-way connection, where the multiplicative connection performs a bit better. The final model then consists of 4 residual blocks, each one with three convolutions, where the residual connections are added after the first convs for each one of the 4 residual blocks.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/15.png' | absolute_url }}">
</figure>

## SlowFast

**Paper: [SlowFast Networks for Video Recognition](https://arxiv.org/abs/1812.03982).**

In standard image recognition tasks, the spatial dimensions are treaded symmetrically. However, for video understanding tasks, all spatiotemporal orientations are not equally likely, so there is no reason to treat space and time the same way. This can be understood by seeing how spatial and temporal content vary differently. The spatial semantics evolve slowly (the identity of hands do not change in the span of an action); while the motion being performed evolve much faster than the object's semantics. Based on this intuition, the paper proposes a two-pathway **SlowFast** model, where the slow path operates over low frame rates with a slowly refreshing speed, while the fast pathway operates over high frame rates while having fewer channels and weaker ability to model spatial information. The two pathways are fused by lateral connection along the paths at the end of each block.

<figure style="width: 90%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/19.png' | absolute_url }}">
</figure>

The slow path way takes as input T frames from a clip of T x N frames, so the input to the slow pathway has a temporal sampling stride of N. For example, for a video with 30 fps, N equals 16 in order to roughly sample two frames per second. On the other hand, the fast path way extracts M x T frames from all of N x T frames, if M = N, then all of the input frames are considered. In the paper they consider M = 8, so for a 30fps video, the input is roughly 16 frames, sampling one frame from two input ones. However, in order to reduce the computation, the channel dimension is reduced by a factor of B, which is generally equals 1 / M. This way, when we want to fuse the fast pathway into the slow pathway, we can reshape the inputs from (C/B) x (TxM) x H x W into (C/B x M) x T x H x W; followed by a sum (Time-to-channel). Other possible lateral connections are also considered: Time-strided sampling sampling, where we sample T frames from M x T ones followed by concatenation, or Time-strided convolution with a stride of M and output channels of 2BC. They all perform roughly the same, with Time-strided convolution performing slightly better.

The final results show that by adding the fast pathway, the model is able to better recognize actions with high speed motions like hand clapping.  

## TSM (Temporal Shift Module)

**Paper: [TSM: Temporal Shift Module for Efficient Video Understanding](https://arxiv.org/abs/1811.08383).**

The widely used and standard 2D CNNs are quite efficient and can yield good results on video understanding tasks, however, they lack the temporal information by only operating on individual frames, which decreases their performances. On the other hand, 3D CNNs are capable of jointly learning both spatial and temporal features, making them better at the task of action recognition, but they remain computationally expensive.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/16.png' | absolute_url }}">
</figure>

TSM module tries to solve this shifting some of the channels of the input features in the temporal dimension while still maintaining the usage 2D CNNs, this way, the new activation computed over each temporal dimension separately will contain information from different frames, depending on the size of the shift. However, the shift operation, while having zero computational flop, is quite memory heavy since the activations needs to be moved from one location to the other. Additionally, shifting too many channels can hurt the performances since the spatial modeling will be incoherent. To solve these issue, TSM shits only a small subsets of the channels across the temporal dimension (bidirectionnally for offline videos and unidirectionally for online video), in addition to adding the TSM module inside a residual branch (the output is the original input plus the shifted and convolved input) to maintain the activations of the current frame while inserting some temporal information.

Network: Given a video, it is divided into segments and then a number of frames is sampled equally from each segment, and each frame is processed with a 2D CNN, but with TSM modules at each residual block of the network. This way, at each block, the model will have access to 3 frames, or two additional ones since there is one shit inward in time and one backward in time.

## STM (SpatioTemporal and Motion Encoding)

**Paper: [STM: SpatioTemporal and Motion Encoding for Action Recognition](https://arxiv.org/abs/1908.02486).**

The two dominant approaches for video recognition is 2D CNNs with two stream, an appearance stream operating over individual RGB frame, modeling only spatial information, and a motion stream modeling temporal information by taking as input the optical flows. And the second method is 3D CNNs that model both using 3D convolutions. However, with the first method the optical flows only model recursive motion and lacks long term temporal and spatial interaction, while 3D CNNs still requires a second stream with explicit motion information. To solve this, the paper proposes STM Networks that integrate both spatiotemporal features with motion information.

The STM network consists of adding two modules to the 2D CNNs, a Channel-wise SpatioTemporal Module (CSTM) module to inject temporal information into the spatial features, and Channel-wise Motion Module (CMM) to model the motion explicitly without the need for optical flows as an additional input. 

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/20.png' | absolute_url }}">
</figure>

The CSTM module consists of reshaping the input tenors from N x T x C x H x W into NHW x C x T, and then applying a size 3 1D convolution over the temporal dimension. Since each channel has different spatial features, each channels has a specific filter, and this can be implemented by a groupe wise 1D conv. The final CSTM module then consists of a reshape, a channel-wise temporal conv, a reshape into the original format and a 3x3 2D convolution. The CMM module of a first 1x1 conv to reduce the channel dimension, then for each frame at time t, the two neighboring frame at t-1 and t+1 are extracted, and an element wise subtraction between each two (t-1;t and t;t+1) is computed and then results are concatenated to model motion between the frames. Finally, the two module are applied in parallel in between the 1x1 convs as in the standard resnet block; resulting in an STM block used to design the model.

### TEA ([Temporal Excitation and Aggregation](https://arxiv.org/abs/2004.01398))
A very similar model to CSN is TEA, that also proposes two modules, a motion excitation (ME) module and a multiple temporal aggregation (MTA) module, specifically designed to capture both short- and long-range temporal evolution. The ME module calculates the feature-level temporal differences
from spatiotemporal features. It then utilizes the differences to excite the motion-sensitive channels of the features. The MTA module proposes to deform the local convolution to a group of sub-
convolutions, forming a hierarchical residual architecture. The TEA block using these two modules in series in between two 1x1 2D convs.

### MotionSqueeze ([MotionSqueeze: Neural Motion Feature Learning for Video Understanding](https://arxiv.org/abs/2007.09933))
Another work that tries to model the motion by defining a new module that operated over adjacent temporal features is MotionSqueeze Net. The introduced module first computes the correlation between adjacent frames of the features of two given pixels with some displacement between them, where for each pixel in frame t we consider all possible displacement in a window of size K x K in frame t+1. Then one  displacement is chosen from the K x K possible ones, but to make it differentiable, they use a weight average over all possible K x K values; this is followed by a gaussian smoothing to remove outliers.

## Channel-Separated Convolutional Network (CSN)

**Paper: [Video Classification with Channel-Separated Convolutional Networks](https://arxiv.org/abs/1904.02811).**

This paper studies the effects of different design choices in 3D group convolutional networks for video classification. By testing different designs, they show that factorizing 3D convolutions by separating channel interactions (a 1x1x1 convolution) and spatiotemporal interactions (channel separated 3x3x3 convolution) leads to improved accuracy and lower computational cost. 3D channel-separated convolutions can also provide provide a form of regularization, yielding lower training accuracy but higher test accuracy compared to 3D convolutions.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/17.png' | absolute_url }}">
</figure>

The paper proposes two versions of CSNs:
- Interaction preserved CSN (ipCSN): in this version, the 3x3x3 standard convolutions are replace a 1×1×1 traditional convolution and a 3×3×3 depthwise convolution. This reduced the number of parameters and FLOPs but preserved all channel interactions with the added 1x1x1 convolutions.
- Interaction reduced CSN (irCSN): in this version, the 3x3x3 standard convolutions are replaced with just a 3×3×3 depthwise convolution. In this case there is a reduction in channel interaction. The experiments show that both can perform better than standard ResNet3D, however, with irCSN, the model needs to have more depth to overcome the reduced number of channel interactions.

## Correlation Networks

**Paper: [Video Modeling with Correlation Networks](https://arxiv.org/abs/1906.03349).**

The paper proposes a novel correlation operator to learn a frame-to-frame spatiotemporal matching, where for each pixel in a frame A, we compute its correlation within K x K neighborhood in the corresponding pixel in frame B.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/18.png' | absolute_url }}">
</figure>

More precisely, for two feature maps of two images A and B of size C x H x W, first we define a K x K correlation region; then for each spatial location in A, we compute the correlation of the C-d vector at that location and the K x K region centered at the same location in B. So for each location in A, we compute K x K correlations, and by doing this over the whole input; the final volume is of size (K x K) x H x W, where in this case (K x K) plays the role of the channel dimension. To make this operation learnable, the K x K is first convolved with a filter of the same size before compute the correction. To consider larger K x K regions, the paper proposes a dilated version, where the correlation is compute over a dilated region to larger receptive field. Additionally, to maintain the case number of channels (C = K x K), they propose a groupe separated region, where the inputs and filters are grouped into G groups, and the correlation is computer for each groupe separately where G x K x K = C. Finally, the operation is also applied temporally where the operation is computed for every adjacent pairs in the input sequence. The operation can be integrated into resblocks by replacing the 3x3x3 conv with 7x7 correlation.

## Multigrid Training

**Paper: [A Multigrid Method for Efficiently Training Video Models](https://arxiv.org/abs/1906.03349).**

Instead of training with a fixed temporal and spatial dimensions during the whole training. The paper proposes to scale down these dimensions while scaling up the batch size and learning rate, resulting in both faster training time and even slightly better results. For original scales of T x H x W; the scaling is chosen so that the new scales has a shape t x h x w (iT x jH x kW) and the batch size is scales up with the combine downscaling rage (b = B x (i x j x k)).

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/21.png' | absolute_url }}">
</figure>

The paper proposes 3 types of multi-grid scheduling methods; long cycles with 4 possible choices of scales and low frequency, short cycles with 3 possible choices and high frequency, and long+short cycles that combines both. 

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/22.png' | absolute_url }}">
</figure>

## TPN (Temporal Pyramid Network)

**Paper: [Temporal Pyramid Network](https://arxiv.org/abs/2004.03548).**

Instead of constructing multiple level of features with different temporal scales using multi-branch networks (like slowfast), TPN tries to reduce the computational overhead and reuse the produced features of the backbone at different blocks with a single and temporally fixed input size.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/23.png' | absolute_url }}">
</figure>

TPN first select a set of blocks to extract the features from, such blocks comes from different levels of the network with different temporal receptive fields making them suitable from modeling the input at different temporal scales. First each level is spatially processed to align spatial semantics
of the features using a stack of convolutions. Then a temporal modulation to control the temporal flexibility, with a temporal convolution and a temporal pooling. The two are then fused, either in both direction or from spatial to temporal. For Up to bottom, each time a spatiotemporal downsampling is applied to adjust the T x H x W scales to the lower volume, as for bottom to top, an nearest upsampling is performed at each step. 

## X3D (Expanding 3D)

**Paper: [X3D: Expanding Architectures for Efficient Video Recognition](https://arxiv.org/abs/2004.04730).**

Starting from a very lightweight model refereed to as X2D, which takes as input a single frame with a spatial resolution of 112 x 112, with a conv1 followed by 4 resnet stages with lightweight channels of size [24, 48, 98, 192], and one of each stages contains a minimal number of resnet bottlenecks, ie [1, 2, 5, 3]. Then at each expansion step, one of the 6 possible axes is doubled, the axis that gave the best result is maintained while the rest is reinitialized into their values at the previsous step. This is then repeated until a computational budget is met. If the last doubling step exceeds the budget, the selected axis is then reduced by a factor smaller than 2 until the budget is met.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/24.png' | absolute_url }}">
</figure>

The axes that can be tested at each expansion step are:
- Fast: increasing the frame rate by having a smaller stride when sampling from a given clip/video.
- Temporal: increasing the duration of the sampled clip resulting in larger sampled input frames.
- Spatial: increasing the spatial dimension of the input frames.
- Depth: increasing the number of resnet blocks per stage.
- Width: increasing the number of channels across the model.
- Bottleneck: increasing the channels of the center convolution of the resnet bottleneck block.

By doing such an iterative expansion, starting from the above detailed X2D and choosing the axis with the best top1 accuracy at each step, we have the following results: Bottleneck -> Fast -> Spatial -> Depth -> Temporal -> Fast -> Temporal -> Spatial -> Spatial -> Depth -> Width.

## STAM (Space Time Attention Model)

**Paper: [An Image is Worth 16x16 Words, What is a Video Worth?](https://arxiv.org/abs/2103.13915).**

This paper proposes a simple adaptation of image transformers for video understanding. The proposed method matches the training and inference conditions, and no matter the size of the input video, they only uniformly sample 16 frames from the whole duration. Each input frame is then divided into 16 x 16 patches, each patch is embedded using a linear layer, with a learned positional embedding that is added to maintain to ordering of the patches. After adding the CLS token that will play the role of a representation of each frame, each patched & embedded input image is then passed through a spatial transformer, which is the same as the transformer used for image recognition. The new addition is a temporal transformer added on top of the outputs of the spatial transformer applied to each frame independently. The inputs of the temporal are the CLS tokens of each one, with an added a video level CLS token, so inputs of shape D x (T+1). The temporal transformer temporally mixes the inputs and is followed by a classification head to produce the predictions.

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/27.png' | absolute_url }}">
</figure>

This model is quite efficient since the added temporal transformer operates over a smaller input sequence compared to that of the spatial transformer (for examples 16x16=256 patches compared 16 frames). Additionally, the number of time layer is half that of the spatial ones (12 vs 6).

## VTN (Video Transformer Network)

**Paper: [An Image is Worth 16x16 Words, What is a Video Worth?](https://arxiv.org/abs/2102.00719).**

VTN is very similar to STAM with a two stage approach, and starts by applying the spatial encoder first followed by a temporal transformer. However, in the temporal transformer, they do not use the standard quadratic attention mechanism, they use LongFormer layers with sliding window attention (only fetching information from the neighborhood) and a global one where some pre-selected tokens get access to all of the rest tokens, and such tokens are task dependent. For classification for example, the CLS token gets access to all of the rest since it is the one used at the end. Since both of the window size the number of pre fixed tokens is very small, the complexity is linear instead of quadratic.

<figure style="width: 50%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/28.png' | absolute_url }}">
</figure>

The paper also experiments with different spatial encoders, both CNN based and transformer based. Additionally, by using LongFormer, they can input longer clips (they use 16/32).

## TimeFormer

**Paper: [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095).**

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/29.png' | absolute_url }}">
</figure>

Instead having separated spatial and temporal encoder, TimeFormer tries to have both a spatial and a temporal component at each transformer layer. For this, they proposes different types of attention:
- Space only: each frame only attends to itself.
- Joint space-time: each frame attends to all of the rest.
- Divided space-time: first attend spatially, then temporally, at each transformer layer.
- Sparse: attend both spatially and temporally to a subset of all the patches of all frames.
- Axial: first attend to patches of the same row and column of the same frame, then temporally to all of the patches of the same location across time.

<figure style="width: 70%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/30.png' | absolute_url }}">
</figure>

By testing these types attention, the authors find that divided space-time perform best and on par with the joint attention, while being more memory efficient, and they call the resulting model TimeFormer. Additionally, they show that TimeFormer can be quite robust to the choice of both the number of input frames, and the number of input clips used during testing, since it is able to span the whole video with a single input clip with a large sampling stride.

## ViViT (Video Vision Transformer)

**Paper: [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691).**

Similar to TimeFormer, ViViT tests many different variants of the attention layers in order to consider both the spatial and temporal contents of the inputs. Additionally, they also propose a new method of embedding the inputs, where the embedding is done both spatially and temporally instead of per frame basis with some hand designed regularization and initialization schemes to overcome the problems of data scarcity.

<figure style="width: 75%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/31.png' | absolute_url }}">
</figure>

**Embedding:** the paper proposes two types of embedding, *2D Embedding*; which is the standard per frame basis where each image is converted into N patches, then aggregated across time with T frames, resulting in an input of N x T steps. Or *Tubelet embedding* which consists of applying a 3D convolution over the T x H x W inputs with a stride equaling the size of the kernel to avoid having any overlap, where each input token corresponds to one application of the 3D convolutions across the input.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/32.png' | absolute_url }}">
</figure>

**Models:** In addition to a baseline of spatiotemporal with one attention across all dimensions, temporal and spatial, the paper proposes three of models based on three types of spatiotemporal attention layers.
- *Factorised Encoder*: this is similar to VTN & STAM, where we start by spatial layers and then stack temporal layers on top.
- *Factorised Self-Attention*: this is similar to TimeFormer, where each attention layer has a spatial attention layer followed by a temporal oon.
- *Factorised Dot-Product*: this consists of applying both the spatial and temporal attentions in parallel, and the two outputs are then concatenated and followed by an MLP projection.

**Initialization:** one of the important aspects of ViViT is the initialization process. The initialization is based on an image based pre-trained transformer (on ImageNet 21M for example), first, for the positional embeddings, in the 2D version they simply copy and repeat the single embedding T times. While for tubelet embedding, the embedding layer consits of 3D convolutions instead of 2D, so they either inflated the 2D into a 3D, or simply initialize the center kernel with the 2D and the rest as zero (the second one, called Central frame init, is better). As for the attention layers, the spatial attention are initialized from the image model while the temporal ones are initialized as zeros, so at the start, the model is just a spatial transformer with no temporal extraction.

The results show that Tubelet embedding with Central frame initialization and factorized encoder (first spatial encoder then temporal encoder, as in STAM & VTN) are better.

## MViT (Multiscale Vision Transformers)

**Paper: [Multiscale Vision Transformers](https://arxiv.org/abs/2104.11227).**

The main idea of MViT is using a multi-scale approach across the spatiotemporal dimension (so the sequence length L = THW) and the channel dimension (D). Instead of having them fixed across the whole model, MViT starts with a large sequence length and a small channel dimension, then at scaling stage, the keys, queries and values are pooled across the sequence length to reduce the length of the output, while the channels dimension is slightly increased. The scaling is detailed in the table bellow.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/33.png' | absolute_url }}">
</figure>

The scaling is done in stages, where the spatial dimensions are halved and the channel dimension is doubled at each scale, with a total of 4 scaling stages. Each stage is composed of a given number of transformer layers and the pooling is done only on the first layer of each stage. In this first layer, the scaling is done in both the main branch, but also the skip connection branch by adding the same pooling operation to the residual branch. Since the attention operation consists of (QK)V, the sequence length of the output depends only on the pooling done on the queries, so the paper proposes to have different pooling rates, one for the queries that controls the overall sequence length, and one for (values, keys) that controls the computational cost of each attention operation, but both still vary comparably. While the scaling of the sequence length is done at the attention layer, the scaling of the channel dimension is done on the final MLP layer of the transformer layer. Another important aspect of MViT is the usage of a separate positional embedding in space & time instead of only a space embedding. So each pacth of a given frame is encoded based on its position in the image and its position in the clip, where all patches of the same image will share the same temporal embedding, and all of the patches of the same spatial location will share the same spatial embedding.

## X-ViT

**Paper: [Space-time Mixing Attention for Video Transformer](https://arxiv.org/abs/2106.05968).**

X-ViT instead of factorizing the spatiotemporal attention into two disjoint ones, they try define a better joint attention computation to reduce the quadratic computation (THW x THW) of the spatiotemporal attention operation. Other alternative used by previous models is spatial only (HW x HW), factorized (TxT + HW x HW) or pooled (THW/scale x THW/scale). X-ViT proposes to use a temporal window, where the patch of each images attends only to frames within the predefined temporal window. This way, the complexity of the attention mechanism depends quadratically only on the spatial scales while being linearly dependent on the temporal scale. Additionally, the temporal receptive field grows linearly with the number of layers resulting in larger temporal context.

<figure style="width: 75%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/34.png' | absolute_url }}">
</figure>

For a window of size W, the standard implementation will give a complexity of (W x T x HW x HW). However it can be further reduced, where instead of computing the keys W times at each temporal location, such keys where already computed T times. So instead of recomputing them TW times, we can simply shift (W/2 to the right and left) and duplicate the original T keys resulting in TW keys with only T computations.

Since the model operation directly on all frames, we end-up with T per frame CLS embeddings, and to aggregate them, the paper proposes a final temporal attention over the T tokens (TA). Additionally, another addition the paper proposes is summary tokens, where before computing the keys and values, a 1-D vector summary of the same size the heads computed as the average pooling over all spatial locations of each one of the frames is appended to them before the attention computation.

## Shorter Notes

- [TRN (Temporal Relational Reasoning in Videos)](https://arxiv.org/abs/1711.08496): the paper proposes to add the temporal relation module to learn a temporal similarity between frame. The relation module consists of two MLPs; the first one takes the concatenation of successive frames (the number ranges from 2 to N) and aggregated them into a single feature vector. The outputs of different ranges are then summed and passed through the second MLP to get a video or clip level representation.
- [SST: Single-Stream Temporal Action Proposals](https://openaccess.thecvf.com/content_cvpr_2017/papers/Buch_SST_Single-Stream_Temporal_CVPR_2017_paper.pdf): for action segmentation, instead of first extracting some features using off the shelf methods like ID3 (improved trajectories paper) and then passing them trough a model for segmentation; the paper proposes and end to end approach with a visual encoder (C3D) and a sequential encoder (GRU). For training, the input video is divided into segments, each segment is compressed into a representation using a visual encoder, and then aggregated using a GPU with past information, while outputting proposals at each time step (one time step = one segment). At each time step, the output is K proposals; where each proposal is a different starting point of the action with the same ending point, which is the current time step. 
- [Compressed Video Action Recognition](https://arxiv.org/abs/1712.00636): Instead of passing the original raw frames, this paper proposes to pass the compressed frames into the network. In most modern video codecs, a video is split into I-frames (intra-coded frames), P-frames (predictive frames) and zero or more B-frames (bi-directional frames). I-frames are regular images and compressed as such. P-frames reference the previous frames and encode only the occurring change from one frame to the other, where they contain motion vector that models the moving pixels, and some residuals that the motion vector do not model. B-frame may be viewed as a special P-frame, where motion vectors are computed bi-directionally and may reference a future frame. But how to use such compressed frames. By analyzing the P-frames, we can see that the accumulated motion vectors are similar to optical flows, and that the residuals give us a motion boundary in addition to change of appearance. However, P-frames only model the difference up to the most recent I-frame or P-frame, so the model needs to track all motion back to the most recent I-frame. This is done by using a decoupled model, where the scores at each frame depend on the most I-frame and the current input P-frame.
- [ECO (Efficient Convolutional Network)](https://arxiv.org/abs/1804.09066): The model build on TSN, which capable of aggregating information from the whole video by sampling snippets from different segments. However, instead of a simple aggregation like averaging or top-k max-pooling to merge the representation generated by the 2D CNN applied to each snippet, ECO uses a 3D CNN to merge these representation to learn better long-term features.
- [Long-Term Feature Banks](https://arxiv.org/abs/1812.05038): instead of using the features of few frames, the paper proposes the usage of a feature bank that has features of the whole video. First a pertained person detector like FasterRCNN is applied on the whole video, the bounding boxes are then used on the features of the corresponding frames that were produced by a 3D network run in parallel to the detector using ROIAlign followed by max-pooling to get 1D vector per frame. Then during training, the current output of the model and the whole feature bank in passed to an attention module, where the current features attend to the features of the bank.
- [LGD Networks (Local and Global Diffusion Networks)](https://arxiv.org/abs/1906.05571): instead of only processing local features (which are the spatio-temporal features of shape C x T x H x W); the paper proposed a LGD module that has two parallel branches, the local features in addition to global features, which are C dimensional 1-d features at video/clip level features. After applying a conv to the local features, the LGD modules first upsamples the global features and adds them to the local ones; then pools the local features and uses them to update the global branch with a weighted sum using two linear layers over each branch. Then to create an LGD style net, we replace each conv-relu block with the LGD block, either for 2D TSN style nets or 3D I3D/C3D nets. The training is then done on both branches.








































































# Action Detection and Localization

https://arxiv.org/pdf/1806.01810.pdf

https://scholar.google.co.uk/scholar?cites=1175087663521850723&as_sdt=2005&sciodt=0,5&hl=fr

https://scholar.google.fr/scholar?start=10&hl=fr&as_sdt=2005&sciodt=0,5&as_ylo=2020&cites=10565861317738901762&scipsc=





# Long Term Action Recognition

### Timeception

The paper proposes a new Timeception layer that can be added on top of standard video recognition backbones to process long range temporal information efficiently for long-range action recognition. More specifically, the paper focuses on complex action recognition, which are defined by three main properties:
- composition: a single instance or video consists of several actions (get -> cook -> put -> wash).
- order: a given action can't be recognized by only focusing on the previous one, but it needs larger temporal extent and dependency to detect.
- extent: actions vary in their temporal extents from one example to the other.

First, to design such a model, the paper follows three key design principles:
- *Subspace Modularity:* In the context of deep network cascades, a decomposition should be modular, such that after a cascade of spatial and a temporal convolutions, it must be possible that yet another cascade (of spatial and temporal convolutions) is possible and meaningful.
- *Subspace Balance:* Increasing the number of parameters for modeling a specific subspace should come at the expense of reducing the number of parameters of another subspace. For example, with 2D CNN, spatial subspace (S) is reduced while the semantic channel subspace (C) is expanded.
- *Subspace Efficiency:* the majority of the available parameter budget should be dedicated to subspaces that are directly relevant to the task at hand. For the task of long-range temporal modeling, the temporal subspace is the most important one.

Based on these design choices, the authors propose a **Timeception** layer which, 1) only processes the relevant subspace by solely using depthwise-separable temporal convolutions (called temporal convolutions in the paper) with a kernel T x 1 x 1 x 1 with no spatial kernels. 2) with the usage of successive temporal convolutions, there needs to be some subspace modularity since the semantic subspace is ignored. To solve this, the Timeception module consists of a channel wise dimensionality reduction which is then followed by the temporal convolutions, and to maintain efficiency, both are only applied over groups of channels, the output is then shuffled to mix the channels between groups before feeding the outputs to the Timeception second layer. 3) to take into consideration different temporal extents, the Timeception layer consists of parallel branches where each branch has different kernel or dilation size to have a variable temporal receptive field, the outputs are then concatenated per group, and then all the groups are concatenated and shuffled.

<figure style="width: 80%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/35.png' | absolute_url }}">
</figure>

### Temporal Query Networks

Temporal Query Networks or TQNs propose a novel module for long range fine-grained video understanding. TQN takes as input a set of predefined queries that depend on the classes we want to detect and their hierarchies, where each query corresponds to some concept or a node in the hierarchy, and the attributes of each query to the subnodes or the elements of the concept. The queries are then updated sequentially with a transformer layers taking at each time stamp the features of the current step, and the objective is to have response in the queries that match the current actions taking place.

Given a video and a pretrained 3D backbone, the video features are extracted by sliding the backbone over the video, and taking as input a given clip of N and a stride of S between to frames (for example 8 x 1, where we sample 8 successive frames as one clip). The features of the whole video are then passed to the transformer which updates the queries via a cross attention where the keys and values are generated from the current input features.
The transformer and the queries are then trained with a cross-entropy loss, where the responses of the attributes of each query are used as logits of the corresponding class.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/36.png' | absolute_url }}">
</figure>

Another important aspect of training TQNs is the updating the backbone when training. First the backbone is pre-trained on small snippets, and then used to cache the features. Then at the second stage, the cached features are used as input to the TQN, but where some of the cached features are replaced with features produced by the backbone, and then backbone is then fine tunned.

<figure style="width: 60%" class="align-center">
  <img src="{{ 'http://127.0.0.1:4000/ml-blog/images/VID-UN/37.png' | absolute_url }}">
</figure>
